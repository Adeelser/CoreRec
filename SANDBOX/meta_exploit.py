# import numpy as np
# import vish_graphs as vg
# import pandas as pd
# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import Dataset, DataLoader
# import core_rec as cs
# import matplotlib.pyplot as plt
# from matplotlib.table import Table


# # Load the CSV file into a DataFrame
# adj_matrix = np.loadtxt('SANDBOX/adj.csv', delimiter=",")
# # wgt_matrix = np.loadtxt('SANDBOX/label.csv', delimiter=",")

# wgt_matrix = []
# for i in range(1, 11):
#     weight_matrix1 = np.loadtxt(f'SANDBOX/delete/label_{i}.csv', delimiter=",")
#     wgt_matrix.append(weight_matrix1)


# # Load node labels
# df = pd.read_csv("SANDBOX/labelele.csv")
# col = df.values.flatten()
# node_labels = {i: label for i, label in enumerate(col)}

# # Find top nodes
# top_nodes = vg.find_top_nodes(adj_matrix, 4)

# # ML
# # Convert adjacency matrix to dataset
# graph_dataset = cs.GraphDataset(adj_matrix)
# batch_size=3
# var=1.0

# #picked form here  


# # Predict recommendations for a specific node
# # node_index =4    # target node
# # recommended_nodes = cs.predict(model, adj_matrix, node_index, top_k=1, threshold=0.5)

# # # Convert recommended node indices to labels
# # recommended_labels = [node_labels[node] for node in recommended_nodes]
# # print(f"Recommended nodes for node {node_labels[node_index]}: {recommended_labels}")
# # torch.save(model.state_dict(), 'massive_surya_e10k.pth')

# # print("\n")
# # if Warning:
# #     print(f"Actually Multi-head Attn was not properly being distribuited amongst {num_heads} heads, it was being distribuited amongst {num_heads-delta} heads")
# #     print(f"Hence the embedding dimension was reduced by {delta} to make it divisible by the number of heads")
#         # Define model parameters
# num_layers = 1
# d_model = 128 #embedding dimension
# num_heads=2

#         # decoration for warning 
# delta=0
# Warning = False
# if(d_model % num_heads != 0):
#     Warning = True
#     delta = d_model % num_heads
#     d_model = d_model - delta
# else:
#     pass


# d_feedforward = 512
# input_dim = len(adj_matrix[0])
# num_weights = 10
# # Initialize model, loss function, and optimizer
# model = cs.GraphTransformer(num_layers, d_model, num_heads, d_feedforward, input_dim, num_weights)
# # model = cs.GraphTransformer(num_layers, d_model, num_heads, d_feedforward, input_dim, use_weights=True)
# criterion = nn.MSELoss()
# optimizer = optim.Adam(model.parameters(), lr=0.0001)
# top_nodes = vg.find_top_nodes(adj_matrix, num_nodes=5)

# # Train the model
# num_epochs = 100   
# batch_idx=[1,2,3,5,8,10]
# threshold_idx=[0.5,0.7,0.9,1.0]
# rl=['Vishesh', 'Shrestha', 'Biswajeet', 'Priyanka', 'Poonam', 'Adhiraaj', 'Yash', 'Sachin', 'Vinayak', 'Kranti', 'Sai']
# for m in range(11):
#     print(f"Recommending for : {rl[m]} ")
#     for i in range(len(threshold_idx)):
#         print(f"{threshold_idx[i]} : ")
#         for k in range(len(batch_idx)):
#             data_loader = DataLoader(graph_dataset, batch_size=batch_idx[k], shuffle=True)
#             cs.train_model(model, data_loader, criterion, optimizer, num_epochs)
#             node_index =m   # target node
#             recommended_nodes = cs.predict(model, adj_matrix, node_index, top_k=3, threshold=threshold_idx[i])
#             # Convert recommended node indices to labels
#             recommended_labels = [node_labels[node] for node in recommended_nodes]
#             # print(f"Recommended nodes for node {node_labels[node_index]}: {recommended_labels}")
#             # print(f"Recommended nodes for vishesh of thr {threshold_idx[i]} and batchsize {batch_idx[k]} : {recommended_labels}")
#             print(f" {batch_idx[k]} : {recommended_labels}")


# # Load node labels from the CSV file
# df_labels = pd.read_csv("SANDBOX/labelele.csv")
# node_labels = df_labels["Names"].tolist()


# # Assuming you have the recommended nodes for each node index stored in a list of lists
# recommended_nodes_list = []
# for i in range(11):
#     recommended_nodes = cs.predict(model, adj_matrix, 0, top_k=3, threshold=i)
#     recommended_nodes_list.append(recommended_nodes)





# # # Create a scatter plot with labels for node indexes
# # plt.figure(figsize=(10, 8))
# # for i in range(11):
# #     x = [i] * len(recommended_nodes_list[i])
# #     y = recommended_nodes_list[i]
# #     plt.scatter(x, y, label=f"Node {i}")
# #     for j, txt in enumerate(y):
# #         plt.annotate(node_labels[y[j]], (x[j], y[j]), textcoords="offset points", xytext=(0,10), ha='center')

# # plt.xticks(range(11), node_labels, rotation=45)
# # plt.yticks(range(11), node_labels)
# # plt.xlabel('Node Index / Names')
# # plt.ylabel('Recommended Node Index / Names')
# # plt.title('Recommendations for Node Index')

# # # Create a legend table with batch size and threshold details
# # legend_data = [['Batch Size', 'Threshold'],
# #                [batch_size , var]]
# # table = plt.table(cellText=legend_data, loc='upper left', cellLoc='center', colWidths=[0.1, 0.1])
# # table.auto_set_font_size(False)
# # table.set_fontsize(12)
# # table.scale(1.5, 1.5)
# # 
# # plt.grid(True)
# # plt.show()