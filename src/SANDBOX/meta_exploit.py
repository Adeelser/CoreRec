import numpy as np
import vish_graphs as vg
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import core_rec as cs
import matplotlib.pyplot as plt
from matplotlib.table import Table
import json
# Load the CSV file into a DataFrame
adj_matrix = np.loadtxt('label.csv', delimiter=",")

# Load node labels
df = pd.read_csv("labelele.csv")
col = df.values.flatten()
node_labels = {i: label for i, label in enumerate(col)}
label_to_index = {label: i for i, label in enumerate(col)}  # Create a reverse mapping

# Print available labels for debugging
print("Available labels:", list(label_to_index.keys()))

# Find top nodes
top_nodes = vg.find_top_nodes(adj_matrix, 4)

# ML
# Convert adjacency matrix to dataset
graph_dataset = cs.GraphDataset(adj_matrix)
batch_size = 3
var = 1.0

# Initialize model parameters
num_layers = 1
d_model = 128  # embedding dimension
num_heads = 2
d_feedforward = 512
input_dim = adj_matrix.shape[1]  # Ensure input_dim matches the number of features in adj_matrix
num_weights = 10

# Initialize model, loss function, and optimizer
model = cs.GraphTransformer(num_layers, d_model, num_heads, d_feedforward, input_dim, use_weights=True)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# Create DataLoader
data_loader = DataLoader(graph_dataset, batch_size=batch_size, shuffle=True)

# Train the model
num_epochs = 100
cs.train_model(model, data_loader, criterion, optimizer, num_epochs)

# Recommend nodes for all labels
recommendations_meta_exploit = {}
for label in label_to_index.keys():
    target_node_index = label_to_index[label]
    recommended_nodes = cs.predict(model, adj_matrix, target_node_index, top_k=3, threshold=1)
    recommended_labels = [node_labels[idx] for idx in recommended_nodes]
    recommendations_meta_exploit[label] = recommended_labels

# Save recommendations to a file
with open('recommendations_meta_exploit.json', 'w') as f:
    json.dump(recommendations_meta_exploit, f)
