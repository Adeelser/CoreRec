{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "import operator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd try graphsage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Number of users: 21716\n",
      "Number of items: 3101\n",
      "Total interactions: 195444\n"
     ]
    }
   ],
   "source": [
    "# Graph sage : Training completed. Best HR: 0.6124, Best NDCG: 0.3914\n",
    "\n",
    "\n",
    "class GraphSAGERecommender(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim, hidden_dim, num_layers=2, dropout=0.2):\n",
    "        super(GraphSAGERecommender, self).__init__()\n",
    "        \n",
    "        # Increase embedding dimensions\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Add batch normalization\n",
    "        self.bn_user = nn.BatchNorm1d(embedding_dim)\n",
    "        self.bn_item = nn.BatchNorm1d(embedding_dim)\n",
    "        \n",
    "        # Deeper MLP with residual connections\n",
    "        self.layers = nn.ModuleList()\n",
    "        input_dim = embedding_dim * 2\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ))\n",
    "        \n",
    "        # Prediction layers\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + embedding_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        # Get and normalize embeddings\n",
    "        user_emb = self.bn_user(self.user_embedding(user_ids))\n",
    "        item_emb = self.bn_item(self.item_embedding(item_ids))\n",
    "        \n",
    "        # Initial concatenation\n",
    "        x = torch.cat([user_emb, item_emb], dim=1)\n",
    "        \n",
    "        # Store original input for residual connection\n",
    "        original_input = x\n",
    "        \n",
    "        # Pass through MLP layers with residual connections\n",
    "        for layer in self.layers:\n",
    "            x = layer(x) + x if x.size() == layer(x).size() else layer(x)\n",
    "        \n",
    "        # Concatenate with original embeddings for final prediction\n",
    "        x = torch.cat([x, original_input], dim=1)\n",
    "        return self.predictor(x).squeeze()\n",
    "\n",
    "\n",
    "class BeibeiDataset(Dataset):\n",
    "    def __init__(self, file_path, num_negative=8):  # Increased negative samples\n",
    "        self.data = pd.read_csv(file_path, sep=' ', header=None, names=['user_id', 'item_id', 'label'], skiprows=1)\n",
    "        \n",
    "        # Create mappings\n",
    "        self.user_map = {id_: idx for idx, id_ in enumerate(self.data['user_id'].unique())}\n",
    "        self.item_map = {id_: idx for idx, id_ in enumerate(self.data['item_id'].unique())}\n",
    "        \n",
    "        # Map IDs\n",
    "        self.data['user_id'] = self.data['user_id'].map(self.user_map)\n",
    "        self.data['item_id'] = self.data['item_id'].map(self.item_map)\n",
    "        \n",
    "        self.num_users = len(self.user_map)\n",
    "        self.num_items = len(self.item_map)\n",
    "        self.num_negative = num_negative\n",
    "        \n",
    "        # Create user-items dictionary\n",
    "        self.user_items = self.create_user_items_dict()\n",
    "        \n",
    "        # Add negative samples with improved sampling strategy\n",
    "        self.data = self.add_negative_samples()\n",
    "        \n",
    "    def create_user_items_dict(self):\n",
    "        return {user_id: set(self.data[self.data['user_id'] == user_id]['item_id']) \n",
    "                for user_id in range(self.num_users)}\n",
    "        \n",
    "    def add_negative_samples(self):\n",
    "        negative_samples = []\n",
    "        all_items = set(range(self.num_items))\n",
    "        \n",
    "        # Calculate item popularity\n",
    "        item_counts = self.data['item_id'].value_counts()\n",
    "        item_probs = 1 / (item_counts + 1)  # Add 1 to avoid division by zero\n",
    "        item_probs = item_probs / item_probs.sum()\n",
    "        \n",
    "        for user_id in self.user_items:\n",
    "            pos_items = self.user_items[user_id]\n",
    "            neg_items = list(all_items - pos_items)\n",
    "            \n",
    "            if len(neg_items) > 0:\n",
    "                # Calculate sampling probabilities for negative items\n",
    "                neg_probs = item_probs[neg_items]\n",
    "                neg_probs = neg_probs / neg_probs.sum()\n",
    "                \n",
    "                # Sample negative items based on popularity\n",
    "                num_neg = min(len(neg_items), self.num_negative)\n",
    "                sampled_neg = np.random.choice(\n",
    "                    neg_items, \n",
    "                    size=num_neg, \n",
    "                    replace=False,\n",
    "                    p=neg_probs\n",
    "                )\n",
    "                \n",
    "                for item_id in sampled_neg:\n",
    "                    negative_samples.append([user_id, item_id, 0])\n",
    "        \n",
    "        neg_df = pd.DataFrame(negative_samples, columns=['user_id', 'item_id', 'label'])\n",
    "        return pd.concat([self.data, neg_df], ignore_index=True).sample(frac=1)\n",
    "\n",
    "\n",
    "# First, initialize the dataset\n",
    "print(\"Loading dataset...\")\n",
    "file_path = '/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei.txt'\n",
    "dataset = BeibeiDataset(file_path)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"Number of users: {dataset.num_users}\")\n",
    "print(f\"Number of items: {dataset.num_items}\")\n",
    "print(f\"Total interactions: {len(dataset.data)}\")\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    'embedding_dim': 128,\n",
    "    'hidden_dim': 256,\n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.3,\n",
    "    'batch_size': 1,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'num_epochs': 30\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GraphSAGERecommender(\n",
    "    num_users=dataset.num_users,\n",
    "    num_items=dataset.num_items,\n",
    "    embedding_dim=config['embedding_dim'],\n",
    "    hidden_dim=config['hidden_dim'],\n",
    "    num_layers=config['num_layers'],\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Optimizer and scheduler setup\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config['num_epochs'],\n",
    "    eta_min=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate the model using HR@k and NDCG@k\n",
    "    \"\"\"\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for user_ids, item_ids, labels in test_loader:\n",
    "            user_ids = user_ids.to(device)\n",
    "            item_ids = item_ids.to(device)\n",
    "            \n",
    "            # Get positive items (where label == 1)\n",
    "            positive_mask = labels == 1\n",
    "            test_users = user_ids[positive_mask]\n",
    "            test_items = item_ids[positive_mask]\n",
    "            \n",
    "            for user_id, true_item in zip(test_users, test_items):\n",
    "                # Generate 99 negative items + 1 positive item\n",
    "                neg_items = torch.tensor([i for i in range(dataset.num_items) if i != true_item])\n",
    "                neg_items = neg_items[torch.randperm(len(neg_items))[:99]]\n",
    "                eval_items = torch.cat([neg_items, true_item.unsqueeze(0)])\n",
    "                \n",
    "                # Create user tensor of same length as eval_items\n",
    "                user_tensor = user_id.repeat(100).to(device)\n",
    "                item_tensor = eval_items.to(device)\n",
    "                \n",
    "                # Get predictions\n",
    "                predictions = model(user_tensor, item_tensor)\n",
    "                \n",
    "                # Get ranking of the true item\n",
    "                _, indices = torch.sort(predictions, descending=True)\n",
    "                rank = (indices == 99).nonzero().item()  # 99 is the index of true_item\n",
    "                \n",
    "                # Calculate HR@k and NDCG@k\n",
    "                hits.append(1 if rank < k else 0)\n",
    "                ndcgs.append(1 / np.log2(rank + 2) if rank < k else 0)\n",
    "    \n",
    "    return np.mean(hits), np.mean(ndcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph sage : Training completed. Best HR: 0.6124, Best NDCG: 0.3914\n",
    "\n",
    "class BeibeiDataset(Dataset):\n",
    "    def __init__(self, file_path, num_negative=4):\n",
    "        # Read the data\n",
    "        self.data = pd.read_csv(file_path, sep=' ', header=None, names=['user_id', 'item_id', 'label'], skiprows=1)\n",
    "        \n",
    "        # Create mappings for user and item IDs\n",
    "        self.user_map = {id_: idx for idx, id_ in enumerate(self.data['user_id'].unique())}\n",
    "        self.item_map = {id_: idx for idx, id_ in enumerate(self.data['item_id'].unique())}\n",
    "        \n",
    "        # Map IDs to indices\n",
    "        self.data['user_id'] = self.data['user_id'].map(self.user_map)\n",
    "        self.data['item_id'] = self.data['item_id'].map(self.item_map)\n",
    "        \n",
    "        self.num_users = len(self.user_map)\n",
    "        self.num_items = len(self.item_map)\n",
    "        self.num_negative = num_negative\n",
    "        \n",
    "        # Create user-items dictionary\n",
    "        self.user_items = self.create_user_items_dict()\n",
    "        \n",
    "        # Add negative samples\n",
    "        self.data = self.add_negative_samples()\n",
    "        \n",
    "    def create_user_items_dict(self):\n",
    "        user_items = {}\n",
    "        for user_id in range(self.num_users):\n",
    "            user_items[user_id] = set(self.data[self.data['user_id'] == user_id]['item_id'])\n",
    "        return user_items\n",
    "        \n",
    "    def add_negative_samples(self):\n",
    "        negative_samples = []\n",
    "        all_items = set(range(self.num_items))\n",
    "        \n",
    "        for user_id in self.user_items:\n",
    "            # Get items this user hasn't interacted with\n",
    "            negative_items = list(all_items - self.user_items[user_id])\n",
    "            \n",
    "            # Sample negative items\n",
    "            if len(negative_items) > 0:\n",
    "                num_neg = min(len(negative_items), self.num_negative)\n",
    "                sampled_neg = np.random.choice(negative_items, num_neg, replace=False)\n",
    "                \n",
    "                # Add negative samples\n",
    "                for item_id in sampled_neg:\n",
    "                    negative_samples.append([user_id, item_id, 0])\n",
    "        \n",
    "        # Convert to DataFrame and combine with positive samples\n",
    "        neg_df = pd.DataFrame(negative_samples, columns=['user_id', 'item_id', 'label'])\n",
    "        return pd.concat([self.data, neg_df], ignore_index=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        return (\n",
    "            torch.tensor(row['user_id'], dtype=torch.long),\n",
    "            torch.tensor(row['item_id'], dtype=torch.long),\n",
    "            torch.tensor(row['label'], dtype=torch.float)\n",
    "        )\n",
    "\n",
    "# Load and preprocess data\n",
    "dataset = BeibeiDataset('/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei.txt')\n",
    "\n",
    "# Create data loaders\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model with correct number of users and items\n",
    "model = GraphSAGERecommender(\n",
    "    num_users=dataset.num_users,\n",
    "    num_items=dataset.num_items,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Rest of the training code remains the same\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "# Modified DataLoader with batch_size equal to dataset size\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "# Training loop without batches\n",
    "best_hr = 0\n",
    "best_ndcg = 0\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    \n",
    "    # Get all data at once\n",
    "    user_ids, item_ids, labels = next(iter(train_loader))\n",
    "    user_ids = user_ids.to(device)\n",
    "    item_ids = item_ids.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(user_ids, item_ids)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hr, ndcg = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(f\"HR@10: {hr:.4f} (Best: {best_hr:.4f})\")\n",
    "    print(f\"NDCG@10: {ndcg:.4f} (Best: {best_ndcg:.4f})\")\n",
    "    \n",
    "    # Update best metrics\n",
    "    best_hr = max(best_hr, hr)\n",
    "    best_ndcg = max(best_ndcg, ndcg)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(loss.item())\n",
    "\n",
    "print(f\"Training completed. Best HR: {best_hr:.4f}, Best NDCG: {best_ndcg:.4f}\")\n",
    "\n",
    "# graph sage ++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 15, Loss: 0.1466, HR@10: 0.1048, NDCG@10: 0.0479\n",
    "\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from torch_geometric.data import Data\n",
    "# from torch_geometric.nn import SAGEConv\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# # Load the BeibeiDataset\n",
    "# dataset = BeibeiDataset('/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei.txt')\n",
    "\n",
    "# # Get user and item IDs from the dataset\n",
    "# user_ids = dataset.data['user_id'].values\n",
    "# item_ids = dataset.data['item_id'].values\n",
    "\n",
    "# # Create edges\n",
    "# edge_index = torch.tensor([\n",
    "#     [u for u in user_ids],  # User nodes\n",
    "#     [i + dataset.num_users for i in item_ids]   # Item nodes (offset by num_users)\n",
    "# ], dtype=torch.long)\n",
    "\n",
    "# # Node features\n",
    "# num_users = dataset.num_users\n",
    "# num_items = dataset.num_items\n",
    "# num_nodes = num_users + num_items\n",
    "# x = torch.randn(num_nodes, 64)  # Increased feature dimension to match embedding_dim\n",
    "\n",
    "# # Create PyTorch Geometric data object\n",
    "# graph_data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# class ImprovedGraphSAGE(torch.nn.Module):\n",
    "#     def __init__(self, num_users, num_items, embedding_dim, hidden_dim, dropout=0.3):\n",
    "#         super(ImprovedGraphSAGE, self).__init__()\n",
    "        \n",
    "#         self.user_embedding = torch.nn.Embedding(num_users, embedding_dim)\n",
    "#         self.item_embedding = torch.nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "#         self.conv1 = SAGEConv(embedding_dim, hidden_dim)\n",
    "#         self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "#         self.conv3 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "#         self.bn1 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "#         self.bn2 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "#         self.bn3 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "#         self.predictor = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Dropout(dropout),\n",
    "#             torch.nn.Linear(hidden_dim, 1)\n",
    "#         )\n",
    "        \n",
    "#         self.dropout = dropout\n",
    "#         self._init_weights()\n",
    "        \n",
    "#     def _init_weights(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, torch.nn.Linear):\n",
    "#                 torch.nn.init.xavier_uniform_(m.weight)\n",
    "#                 if m.bias is not None:\n",
    "#                     torch.nn.init.zeros_(m.bias)\n",
    "#             elif isinstance(m, torch.nn.Embedding):\n",
    "#                 torch.nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "#     def forward(self, x, edge_index, user_indices, item_indices):\n",
    "#         # Get initial embeddings\n",
    "#         user_emb = self.user_embedding(user_indices)\n",
    "#         item_emb = self.item_embedding(item_indices)\n",
    "        \n",
    "#         # Update node features with embeddings\n",
    "#         x = torch.cat([\n",
    "#             self.user_embedding.weight,\n",
    "#             self.item_embedding.weight\n",
    "#         ], dim=0)\n",
    "        \n",
    "#         # Process through GraphSAGE layers\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = self.bn1(x)\n",
    "#         x = F.leaky_relu(x)\n",
    "#         x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = self.bn2(x)\n",
    "#         x = F.leaky_relu(x)\n",
    "#         x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "#         x = self.conv3(x, edge_index)\n",
    "#         x = self.bn3(x)\n",
    "#         x = F.leaky_relu(x)\n",
    "        \n",
    "#         # Get final embeddings\n",
    "#         user_final = x[:num_users][user_indices]\n",
    "#         item_final = x[num_users:][item_indices]\n",
    "        \n",
    "#         # Combine embeddings\n",
    "#         combined = torch.cat([user_final, item_final], dim=1)\n",
    "#         return self.predictor(combined).squeeze()\n",
    "\n",
    "# def calculate_metrics(model, test_data, k=10):\n",
    "#     hits = []\n",
    "#     ndcgs = []\n",
    "    \n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         pos_samples = test_data[test_data['label'] == 1]\n",
    "        \n",
    "#         for _, row in pos_samples.iterrows():\n",
    "#             user_idx = row['user_id']\n",
    "#             true_item = row['item_id']\n",
    "            \n",
    "#             # Calculate scores for all items\n",
    "#             user_tensor = torch.LongTensor([user_idx]).repeat(dataset.num_items).to(device)\n",
    "#             item_tensor = torch.LongTensor(range(dataset.num_items)).to(device)\n",
    "            \n",
    "#             scores = model(graph_data.x, graph_data.edge_index, user_tensor, item_tensor)\n",
    "            \n",
    "#             # Get top-k items\n",
    "#             _, top_items = torch.topk(scores, k)\n",
    "#             top_items = top_items.cpu().numpy()\n",
    "            \n",
    "#             # Calculate metrics\n",
    "#             hit = 1 if true_item in top_items else 0\n",
    "#             hits.append(hit)\n",
    "            \n",
    "#             if hit:\n",
    "#                 rank = np.where(top_items == true_item)[0][0]\n",
    "#                 ndcg = 1 / np.log2(rank + 2)\n",
    "#             else:\n",
    "#                 ndcg = 0\n",
    "#             ndcgs.append(ndcg)\n",
    "    \n",
    "#     return np.mean(hits), np.mean(ndcgs)\n",
    "\n",
    "# # Setup device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# graph_data = graph_data.to(device)\n",
    "\n",
    "# # Data splitting\n",
    "# pos_data = dataset.data[dataset.data['label'] == 1]\n",
    "# neg_data = dataset.data[dataset.data['label'] == 0]\n",
    "\n",
    "# test_size_pos = int(0.2 * len(pos_data))\n",
    "# train_pos = pos_data.iloc[:-test_size_pos]\n",
    "# test_pos = pos_data.iloc[-test_size_pos:]\n",
    "\n",
    "# test_size_neg = int(0.2 * len(neg_data))\n",
    "# train_neg = neg_data.iloc[:-test_size_neg]\n",
    "# test_neg = neg_data.iloc[-test_size_neg:]\n",
    "\n",
    "# train_data = pd.concat([train_pos, train_neg]).sample(frac=1, random_state=42)\n",
    "# test_data = pd.concat([test_pos, test_neg]).sample(frac=1, random_state=42)\n",
    "\n",
    "# print(f\"Train set size: {len(train_data)}, positive samples: {len(train_pos)}\")\n",
    "# print(f\"Test set size: {len(test_data)}, positive samples: {len(test_pos)}\")\n",
    "\n",
    "# # Initialize model\n",
    "# model = ImprovedGraphSAGE(\n",
    "#     num_users=dataset.num_users,\n",
    "#     num_items=dataset.num_items,\n",
    "#     embedding_dim=64,\n",
    "#     hidden_dim=128\n",
    "# ).to(device)\n",
    "\n",
    "# # Training setup\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "# # scheduler = torch.optim.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "# # Training loop\n",
    "# best_hr = 0\n",
    "# best_ndcg = 0\n",
    "\n",
    "# for epoch in range(50):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     # Process in batches\n",
    "#     batch_size = 1024\n",
    "#     for start_idx in range(0, len(train_data), batch_size):\n",
    "#         end_idx = min(start_idx + batch_size, len(train_data))\n",
    "#         batch_data = train_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "#         user_ids = torch.LongTensor(batch_data['user_id'].values).to(device)\n",
    "#         item_ids = torch.LongTensor(batch_data['item_id'].values).to(device)\n",
    "#         labels = torch.FloatTensor(batch_data['label'].values).to(device)\n",
    "        \n",
    "#         # Forward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(graph_data.x, graph_data.edge_index, user_ids, item_ids)\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     avg_loss = total_loss / (len(train_data) / batch_size)\n",
    "    \n",
    "#     # Evaluation\n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "#         hr, ndcg = calculate_metrics(model, test_data)\n",
    "#         print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}, HR@10: {hr:.4f}, NDCG@10: {ndcg:.4f}')\n",
    "        \n",
    "#         best_hr = max(best_hr, hr)\n",
    "#         best_ndcg = max(best_ndcg, ndcg)\n",
    "#         scheduler.step(avg_loss)\n",
    "\n",
    "# print(f\"Training completed. Best HR@10: {best_hr:.4f}, Best NDCG@10: {best_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rough biasMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 86865, positive samples: 17373\n",
      "Test set size: 21715, positive samples: 4343\n",
      "Epoch 10, Loss: 0.0613\n",
      "Epoch 20, Loss: 0.0121\n",
      "Epoch 30, Loss: 0.0046\n",
      "Epoch 40, Loss: 0.0022\n",
      "Epoch 50, Loss: 0.0012\n",
      "Epoch 60, Loss: 0.0006\n",
      "Epoch 70, Loss: 0.0004\n",
      "Epoch 80, Loss: 0.0002\n",
      "Epoch 90, Loss: 0.0001\n",
      "Epoch 100, Loss: 0.0001\n",
      "Evaluating 4343 positive samples...\n",
      "HR@10: 0.0258, NDCG@10: 0.0130\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load dataset\n",
    "dataset = BeibeiDataset('/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei.txt')\n",
    "\n",
    "# Calculate global mean of labels\n",
    "global_mean = dataset.data['label'].mean()\n",
    "\n",
    "# Modified BiasMF class\n",
    "class BiasMF(torch.nn.Module):\n",
    "    def __init__(self, num_users, num_items, latent_dim, global_mean):\n",
    "        super(BiasMF, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.latent_dim = latent_dim\n",
    "        self.mu = global_mean\n",
    "\n",
    "        self.user_embedding = torch.nn.Embedding(self.num_users, self.latent_dim)\n",
    "        self.item_embedding = torch.nn.Embedding(self.num_items, self.latent_dim)\n",
    "\n",
    "        self.user_bias = torch.nn.Embedding(self.num_users, 1)\n",
    "        self.user_bias.weight.data = torch.zeros(self.num_users, 1).float()\n",
    "        self.item_bias = torch.nn.Embedding(self.num_items, 1)\n",
    "        self.item_bias.weight.data = torch.zeros(self.num_items, 1).float()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_vec = self.user_embedding(user_indices)\n",
    "        item_vec = self.item_embedding(item_indices)\n",
    "        dot = torch.mul(user_vec, item_vec).sum(dim=1)\n",
    "\n",
    "        rating = dot + self.mu + self.user_bias(user_indices).view(-1) + self.item_bias(item_indices).view(-1)\n",
    "\n",
    "        return rating\n",
    "\n",
    "# Initialize model with individual parameters\n",
    "model = BiasMF(\n",
    "    num_users=dataset.num_users,\n",
    "    num_items=dataset.num_items,\n",
    "    latent_dim=16,\n",
    "    global_mean=global_mean\n",
    ").to(device)\n",
    "\n",
    "# Rest of the code remains the same\n",
    "# Split data\n",
    "pos_data = dataset.data[dataset.data['label'] == 1]\n",
    "neg_data = dataset.data[dataset.data['label'] == 0]\n",
    "\n",
    "# Split positive samples\n",
    "test_size_pos = int(0.2 * len(pos_data))\n",
    "train_pos = pos_data.iloc[:-test_size_pos]\n",
    "test_pos = pos_data.iloc[-test_size_pos:]\n",
    "\n",
    "# Split negative samples\n",
    "test_size_neg = int(0.2 * len(neg_data))\n",
    "train_neg = neg_data.iloc[:-test_size_neg]\n",
    "test_neg = neg_data.iloc[-test_size_neg:]\n",
    "\n",
    "# Combine splits\n",
    "train_data = pd.concat([train_pos, train_neg]).sample(frac=1, random_state=42)\n",
    "test_data = pd.concat([test_pos, test_neg]).sample(frac=1, random_state=42)\n",
    "\n",
    "print(f\"Train set size: {len(train_data)}, positive samples: {len(train_pos)}\")\n",
    "print(f\"Test set size: {len(test_data)}, positive samples: {len(test_pos)}\")\n",
    "\n",
    "# Training setup\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Convert data to tensors\n",
    "train_users = torch.LongTensor(train_data['user_id'].values).to(device)\n",
    "train_items = torch.LongTensor(train_data['item_id'].values).to(device)\n",
    "train_labels = torch.FloatTensor(train_data['label'].values).to(device)\n",
    "\n",
    "# Training loop\n",
    "batch_size = 1024\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Process in batches\n",
    "    for start_idx in range(0, len(train_users), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(train_users))\n",
    "        \n",
    "        user_batch = train_users[start_idx:end_idx]\n",
    "        item_batch = train_items[start_idx:end_idx]\n",
    "        label_batch = train_labels[start_idx:end_idx]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(user_batch, item_batch)\n",
    "        loss = criterion(predictions, label_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        avg_loss = total_loss / (len(train_users) / batch_size)\n",
    "        print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "\n",
    "def calculate_metrics(model, test_data, k=10):\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get positive samples from test data\n",
    "        pos_samples = test_data[test_data['label'] == 1]\n",
    "        \n",
    "        if len(pos_samples) == 0:\n",
    "            print(\"No positive samples found in test data!\")\n",
    "            return 0.0, 0.0\n",
    "            \n",
    "        print(f\"Evaluating {len(pos_samples)} positive samples...\")\n",
    "        \n",
    "        for _, row in pos_samples.iterrows():\n",
    "            user_idx = row['user_id']\n",
    "            true_item = row['item_id']\n",
    "            \n",
    "            try:\n",
    "                # Calculate scores for all items\n",
    "                user_tensor = torch.LongTensor([user_idx]).repeat(dataset.num_items).to(device)\n",
    "                item_tensor = torch.LongTensor(range(dataset.num_items)).to(device)\n",
    "                \n",
    "                scores = model(user_tensor, item_tensor)\n",
    "                \n",
    "                # Get top-k items\n",
    "                _, top_items = torch.topk(scores, k)\n",
    "                top_items = top_items.cpu().numpy()\n",
    "                \n",
    "                # Calculate metrics\n",
    "                hit = 1 if true_item in top_items else 0\n",
    "                hits.append(hit)\n",
    "                \n",
    "                if hit:\n",
    "                    rank = np.where(top_items == true_item)[0][0]\n",
    "                    ndcg = 1 / np.log2(rank + 2)\n",
    "                else:\n",
    "                    ndcg = 0\n",
    "                ndcgs.append(ndcg)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing user {user_idx}, item {true_item}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    if not hits:\n",
    "        print(\"No valid predictions were made!\")\n",
    "        return 0.0, 0.0\n",
    "        \n",
    "    return np.mean(hits), np.mean(ndcgs)\n",
    "\n",
    "# Calculate metrics\n",
    "hr, ndcg = calculate_metrics(model, test_data, k=10)\n",
    "print(f'HR@10: {hr:.4f}, NDCG@10: {ndcg:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.0731\n",
      "Evaluating 4343 positive samples...\n",
      "HR@10: 0.1025, NDCG@10: 0.0542\n",
      "Epoch 20, Loss: 0.0123\n",
      "Evaluating 4343 positive samples...\n",
      "HR@10: 0.1073, NDCG@10: 0.0550\n",
      "Epoch 30, Loss: 0.0051\n",
      "Evaluating 4343 positive samples...\n",
      "HR@10: 0.1078, NDCG@10: 0.0553\n",
      "Epoch 40, Loss: 0.0035\n",
      "Evaluating 4343 positive samples...\n",
      "HR@10: 0.1068, NDCG@10: 0.0544\n",
      "Epoch 50, Loss: 0.0025\n",
      "Evaluating 4343 positive samples...\n",
      "HR@10: 0.1082, NDCG@10: 0.0546\n",
      "Epoch 60, Loss: 0.0022\n",
      "Evaluating 4343 positive samples...\n",
      "HR@10: 0.1091, NDCG@10: 0.0558\n",
      "Epoch 70, Loss: 0.0020\n",
      "Evaluating 4343 positive samples...\n",
      "HR@10: 0.1073, NDCG@10: 0.0520\n",
      "Epoch 80, Loss: 0.0015\n",
      "Evaluating 4343 positive samples...\n",
      "HR@10: 0.1059, NDCG@10: 0.0548\n",
      "Epoch 90, Loss: 0.0014\n",
      "Evaluating 4343 positive samples...\n",
      "HR@10: 0.1050, NDCG@10: 0.0529\n",
      "Epoch 100, Loss: 0.0012\n",
      "Evaluating 4343 positive samples...\n",
      "HR@10: 0.1052, NDCG@10: 0.0513\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load dataset\n",
    "dataset = BeibeiDataset('/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei.txt')\n",
    "\n",
    "# Calculate global mean of labels\n",
    "global_mean = dataset.data['label'].mean()\n",
    "# Remove the second model initialization and use only EnhancedBiasMF\n",
    "class EnhancedBiasMF(torch.nn.Module):\n",
    "    def __init__(self, num_users, num_items, latent_dim, global_mean, dropout=0.2):\n",
    "        super(EnhancedBiasMF, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.latent_dim = latent_dim\n",
    "        self.mu = global_mean\n",
    "        \n",
    "        # Embeddings\n",
    "        self.user_embedding = torch.nn.Embedding(num_users, latent_dim)\n",
    "        self.item_embedding = torch.nn.Embedding(num_items, latent_dim)\n",
    "        torch.nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "        \n",
    "        # Bias terms\n",
    "        self.user_bias = torch.nn.Embedding(num_users, 1)\n",
    "        self.item_bias = torch.nn.Embedding(num_items, 1)\n",
    "        torch.nn.init.zeros_(self.user_bias.weight)\n",
    "        torch.nn.init.zeros_(self.item_bias.weight)\n",
    "        \n",
    "        # MLP layers\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim * 2, latent_dim),\n",
    "            torch.nn.BatchNorm1d(latent_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(latent_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        # Get embeddings\n",
    "        user_vec = self.user_embedding(user_indices)\n",
    "        item_vec = self.item_embedding(item_indices)\n",
    "        \n",
    "        # Get biases\n",
    "        user_bias = self.user_bias(user_indices).squeeze()\n",
    "        item_bias = self.item_bias(item_indices).squeeze()\n",
    "        \n",
    "        # Combine embeddings\n",
    "        concat = torch.cat([user_vec, item_vec], dim=1)\n",
    "        \n",
    "        # MLP prediction\n",
    "        mlp_output = self.mlp(concat).squeeze()\n",
    "        \n",
    "        # Final prediction\n",
    "        rating = mlp_output + self.mu + user_bias + item_bias\n",
    "        \n",
    "        return rating\n",
    "\n",
    "# Model and training configuration\n",
    "config = {\n",
    "    'latent_dim': 64,\n",
    "    'batch_size': 2048,\n",
    "    'num_epochs': 100,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-5,\n",
    "    'dropout': 0.2\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = EnhancedBiasMF(\n",
    "    num_users=dataset.num_users,\n",
    "    num_items=dataset.num_items,\n",
    "    latent_dim=config['latent_dim'],\n",
    "    global_mean=global_mean,\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Training setup\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config['num_epochs']):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for start_idx in range(0, len(train_users), config['batch_size']):\n",
    "        end_idx = min(start_idx + config['batch_size'], len(train_users))\n",
    "        \n",
    "        user_batch = train_users[start_idx:end_idx]\n",
    "        item_batch = train_items[start_idx:end_idx]\n",
    "        label_batch = train_labels[start_idx:end_idx]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(user_batch, item_batch)\n",
    "        loss = criterion(predictions, label_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / (len(train_users) / config['batch_size'])\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}')\n",
    "        # Evaluate on validation set\n",
    "        hr, ndcg = calculate_metrics(model, test_data, k=10)\n",
    "        print(f'HR@10: {hr:.4f}, NDCG@10: {ndcg:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 4343 positive samples...\n",
      "Epoch 1:\n",
      "Loss: 3.0106\n",
      "HR@10: 0.0035, NDCG@10: 0.0014\n",
      "Evaluating 4343 positive samples...\n",
      "Epoch 2:\n",
      "Loss: 0.1343\n",
      "HR@10: 0.0037, NDCG@10: 0.0016\n",
      "Evaluating 4343 positive samples...\n",
      "Epoch 3:\n",
      "Loss: 0.0417\n",
      "HR@10: 0.0037, NDCG@10: 0.0016\n",
      "Evaluating 4343 positive samples...\n",
      "Epoch 4:\n",
      "Loss: 0.0225\n",
      "HR@10: 0.0037, NDCG@10: 0.0015\n",
      "Evaluating 4343 positive samples...\n",
      "Epoch 5:\n",
      "Loss: 0.0141\n",
      "HR@10: 0.0035, NDCG@10: 0.0014\n",
      "Evaluating 4343 positive samples...\n",
      "Epoch 6:\n",
      "Loss: 0.0109\n",
      "HR@10: 0.0035, NDCG@10: 0.0014\n",
      "Evaluating 4343 positive samples...\n",
      "Epoch 7:\n",
      "Loss: 0.0093\n",
      "HR@10: 0.0035, NDCG@10: 0.0014\n",
      "Early stopping triggered\n",
      "Training completed. Best HR@10: 0.0037\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load dataset\n",
    "dataset = BeibeiDataset('/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei.txt')\n",
    "\n",
    "# Calculate global mean of labels\n",
    "global_mean = dataset.data['label'].mean()\n",
    "# Remove the second model initialization and use only EnhancedBiasMF\n",
    "class BiasMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, latent_dim, global_mean, dropout=0.1):\n",
    "        super(BiasMF, self).__init__()\n",
    "        \n",
    "        # Basic parameters\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.latent_dim = latent_dim\n",
    "        self.global_mean = global_mean\n",
    "        \n",
    "        # Embeddings initialization with Xavier/Glorot\n",
    "        self.user_embedding = nn.Embedding(num_users, latent_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, latent_dim)\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "        \n",
    "        # Bias terms\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        \n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Layer normalization for better stability\n",
    "        self.layer_norm = nn.LayerNorm(latent_dim)\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        # Get embeddings\n",
    "        user_vec = self.dropout(self.user_embedding(user_indices))\n",
    "        item_vec = self.dropout(self.item_embedding(item_indices))\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        user_vec = self.layer_norm(user_vec)\n",
    "        item_vec = self.layer_norm(item_vec)\n",
    "        \n",
    "        # Get biases\n",
    "        user_b = self.user_bias(user_indices).squeeze()\n",
    "        item_b = self.item_bias(item_indices).squeeze()\n",
    "        \n",
    "        # Compute dot product\n",
    "        dot = torch.sum(user_vec * item_vec, dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        pred = self.global_mean + dot + user_b + item_b\n",
    "        return pred\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    'latent_dim': 64,\n",
    "    'batch_size': 1024,\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-5,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = BiasMF(\n",
    "    num_users=dataset.num_users,\n",
    "    num_items=dataset.num_items,\n",
    "    latent_dim=config['latent_dim'],\n",
    "    global_mean=global_mean,\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_hr = 0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Training\n",
    "    for start_idx in range(0, len(train_users), config['batch_size']):\n",
    "        end_idx = min(start_idx + config['batch_size'], len(train_users))\n",
    "        \n",
    "        user_batch = train_users[start_idx:end_idx]\n",
    "        item_batch = train_items[start_idx:end_idx]\n",
    "        label_batch = train_labels[start_idx:end_idx]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(user_batch, item_batch)\n",
    "        loss = criterion(predictions, label_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    # Validation\n",
    "    avg_loss = total_loss / num_batches\n",
    "    hr, ndcg = calculate_metrics(model, test_data)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}:')\n",
    "    print(f'Loss: {avg_loss:.4f}')\n",
    "    print(f'HR@10: {hr:.4f}, NDCG@10: {ndcg:.4f}')\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if hr > best_hr:\n",
    "        best_hr = hr\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "print(f\"Training completed. Best HR@10: {best_hr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Initializing dataset...\n",
      "Loading training data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/\n",
      "Loading behavior: pv from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_pv\n",
      "Loaded pv matrix with shape: (21716, 7977)\n",
      "Loading behavior: cart from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_cart\n",
      "Loaded cart matrix with shape: (21716, 7977)\n",
      "Loading behavior: buy from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_buy\n",
      "Loaded buy matrix with shape: (21716, 7977)\n",
      "Dataset dimensions: 21716 users, 7977 items\n",
      "Loading test data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/tst_int\n",
      "Number of test users: 10000\n",
      "Preparing training instances...\n",
      "Processing user 0/21716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/20z1hn994jd04q4kl0gpgh740000gn/T/ipykernel_1586/3372765655.py:96: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  mat = pickle.load(fs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing user 1000/21716\n",
      "Processing user 2000/21716\n",
      "Processing user 3000/21716\n",
      "Processing user 4000/21716\n",
      "Processing user 5000/21716\n",
      "Processing user 6000/21716\n",
      "Processing user 7000/21716\n",
      "Processing user 8000/21716\n",
      "Processing user 9000/21716\n",
      "Processing user 10000/21716\n",
      "Processing user 11000/21716\n",
      "Processing user 12000/21716\n",
      "Processing user 13000/21716\n",
      "Processing user 14000/21716\n",
      "Processing user 15000/21716\n",
      "Processing user 16000/21716\n",
      "Processing user 17000/21716\n",
      "Processing user 18000/21716\n",
      "Processing user 19000/21716\n",
      "Processing user 20000/21716\n",
      "Processing user 21000/21716\n",
      "Generated 1414300 training instances\n",
      "Initializing model...\n",
      "Preparing training tensors...\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0067\n",
      "Batch 200: Loss = 0.0031\n",
      "Batch 300: Loss = 0.0010\n",
      "Batch 400: Loss = 0.0004\n",
      "Batch 500: Loss = 0.0003\n",
      "Batch 600: Loss = 0.0002\n",
      "Batch 700: Loss = 0.0001\n",
      "Batch 800: Loss = 0.0001\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0001\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 1 completed. Average loss: 0.0124\n",
      "\n",
      "Epoch 2/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 2 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 3/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 3 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 4/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 4 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 5/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 5 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.0541, NDCG@10: 0.0246\n",
      "New best HR achieved!\n",
      "\n",
      "Epoch 6/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 6 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 7/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 7 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 8/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 8 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 9/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 9 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 10/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 10 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.0498, NDCG@10: 0.0228\n",
      "\n",
      "Epoch 11/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 11 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 12/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 12 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 13/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 13 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 14/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 14 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 15/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 15 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.0488, NDCG@10: 0.0222\n",
      "\n",
      "Epoch 16/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 16 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 17/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 17 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 18/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 18 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 19/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 19 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 20/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 20 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.0503, NDCG@10: 0.0223\n",
      "\n",
      "Epoch 21/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 21 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 22/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 22 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 23/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 23 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 24/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 24 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 25/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 25 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.0484, NDCG@10: 0.0223\n",
      "\n",
      "Epoch 26/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 26 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 27/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 27 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 28/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 28 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 29/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 29 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 30/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 30 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.0480, NDCG@10: 0.0221\n",
      "\n",
      "Epoch 31/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 31 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 32/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 32 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 33/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 33 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 34/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 34 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 35/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 35 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.0483, NDCG@10: 0.0216\n",
      "\n",
      "Epoch 36/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 36 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 37/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 37 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 38/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 38 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 39/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 39 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 40/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 40 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.0471, NDCG@10: 0.0216\n",
      "\n",
      "Epoch 41/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 41 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 42/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 42 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 43/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 43 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 44/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 44 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 45/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 45 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.0473, NDCG@10: 0.0216\n",
      "\n",
      "Epoch 46/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 46 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 47/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 47 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 48/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 48 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 49/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 49 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 50/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Batch 1300: Loss = 0.0000\n",
      "Epoch 50 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.0471, NDCG@10: 0.0211\n",
      "\n",
      "Training completed.\n",
      "Best HR@10: 0.0541\n",
      "Best NDCG@10: 0.0246\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class MultiBehaviorBiasMF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_behaviors, latent_dim=64, dropout=0.1):\n",
    "        super(MultiBehaviorBiasMF, self).__init__()\n",
    "        \n",
    "        self.n_behaviors = n_behaviors\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # User and item embeddings\n",
    "        self.user_embedding = nn.Embedding(n_users, latent_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, latent_dim)\n",
    "        \n",
    "        # Behavior-specific embeddings\n",
    "        self.behavior_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(2, latent_dim) for _ in range(n_behaviors)\n",
    "        ])\n",
    "        \n",
    "        # Bias terms\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        \n",
    "        # MLP for combining behavior signals\n",
    "        self.behavior_mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim * (2 + n_behaviors), latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(latent_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "        for beh_emb in self.behavior_embeddings:\n",
    "            nn.init.xavier_uniform_(beh_emb.weight)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        \n",
    "    def forward(self, user_indices, item_indices, behavior_data):\n",
    "        # Get basic embeddings\n",
    "        user_emb = self.user_embedding(user_indices)\n",
    "        item_emb = self.item_embedding(item_indices)\n",
    "        \n",
    "        # Get behavior embeddings\n",
    "        behavior_embs = []\n",
    "        for i in range(self.n_behaviors):\n",
    "            beh_data = behavior_data[:, i].long()\n",
    "            beh_emb = self.behavior_embeddings[i](beh_data)\n",
    "            behavior_embs.append(beh_emb)\n",
    "        \n",
    "        # Combine all embeddings\n",
    "        combined = torch.cat([user_emb, item_emb] + behavior_embs, dim=1)\n",
    "        \n",
    "        # Get prediction\n",
    "        pred = self.behavior_mlp(combined).squeeze()\n",
    "        \n",
    "        # Add biases\n",
    "        pred = pred + self.user_bias(user_indices).squeeze()\n",
    "        pred = pred + self.item_bias(item_indices).squeeze()\n",
    "        \n",
    "        return pred\n",
    "\n",
    "class MultiBehaviorDataset:\n",
    "    def __init__(self, data_path='/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/'):\n",
    "        self.data_path = data_path\n",
    "        self.behaviors = ['pv', 'cart', 'buy']\n",
    "        self.trn_file = data_path + 'trn_'\n",
    "        self.tst_file = data_path + 'tst_'\n",
    "        \n",
    "        # Load the data during initialization\n",
    "        self.load_training_data()\n",
    "        self.load_test_data()\n",
    "        \n",
    "    def load_training_data(self):\n",
    "        print(\"Loading training data from:\", self.data_path)\n",
    "        self.trn_mats = []\n",
    "        for beh in self.behaviors:\n",
    "            path = self.trn_file + beh\n",
    "            print(f\"Loading behavior: {beh} from {path}\")\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"File not found: {path}\")\n",
    "            with open(path, 'rb') as fs:\n",
    "                mat = pickle.load(fs)\n",
    "                if not isinstance(mat, csr_matrix):\n",
    "                    mat = csr_matrix(mat)\n",
    "                mat = (mat != 0).astype(np.float32)\n",
    "                self.trn_mats.append(mat)\n",
    "                print(f\"Loaded {beh} matrix with shape: {mat.shape}\")\n",
    "        \n",
    "        self.trn_label = 1 * (self.trn_mats[-1] != 0)\n",
    "        self.n_users, self.n_items = self.trn_mats[0].shape\n",
    "        self.n_behaviors = len(self.behaviors)\n",
    "        print(f\"Dataset dimensions: {self.n_users} users, {self.n_items} items\")\n",
    "        \n",
    "    def load_test_data(self):\n",
    "        test_path = self.tst_file + 'int'\n",
    "        print(f\"Loading test data from: {test_path}\")\n",
    "        if not os.path.exists(test_path):\n",
    "            raise FileNotFoundError(f\"File not found: {test_path}\")\n",
    "        with open(test_path, 'rb') as fs:\n",
    "            self.tst_int = np.array(pickle.load(fs))\n",
    "        \n",
    "        self.tst_users = np.reshape(np.argwhere(self.tst_int != None), [-1])\n",
    "        print(f\"Number of test users: {len(self.tst_users)}\")\n",
    "        \n",
    "    def prepare_train_instances(self):\n",
    "        print(\"Preparing training instances...\")\n",
    "        train_data = []\n",
    "        \n",
    "        for user in range(self.n_users):\n",
    "            if user % 1000 == 0:\n",
    "                print(f\"Processing user {user}/{self.n_users}\")\n",
    "                \n",
    "            pos_items = self.trn_label[user].indices\n",
    "            \n",
    "            if len(pos_items) > 0:\n",
    "                for item in pos_items:\n",
    "                    behaviors = [float(mat[user, item]) for mat in self.trn_mats]\n",
    "                    train_data.append([user, item, 1.0] + behaviors)\n",
    "                    \n",
    "                    all_items = set(range(self.n_items))\n",
    "                    pos_items_set = set(pos_items)\n",
    "                    neg_items_pool = list(all_items - pos_items_set)\n",
    "                    \n",
    "                    if len(neg_items_pool) >= 4:\n",
    "                        neg_items = np.random.choice(neg_items_pool, size=4, replace=False)\n",
    "                        for neg_item in neg_items:\n",
    "                            behaviors = [float(mat[user, neg_item]) for mat in self.trn_mats]\n",
    "                            train_data.append([user, neg_item, 0.0] + behaviors)\n",
    "        \n",
    "        if len(train_data) == 0:\n",
    "            raise ValueError(\"No training instances were generated!\")\n",
    "            \n",
    "        print(f\"Generated {len(train_data)} training instances\")\n",
    "        return np.array(train_data)\n",
    "\n",
    "    def get_test_instances(self):\n",
    "        print(\"Preparing test instances...\")\n",
    "        test_instances = []\n",
    "        for user in self.tst_users:\n",
    "            pos_item = self.tst_int[user]\n",
    "            if pos_item is not None:\n",
    "                test_instances.append([user, pos_item, 1.0])\n",
    "                \n",
    "                try:\n",
    "                    all_items = set(range(self.n_items))\n",
    "                    pos_items_train = set(self.trn_label[user].indices)\n",
    "                    pos_items_train.add(pos_item)\n",
    "                    neg_items_pool = list(all_items - pos_items_train)\n",
    "                    \n",
    "                    n_neg_samples = min(99, len(neg_items_pool))\n",
    "                    if n_neg_samples > 0:\n",
    "                        neg_items = np.random.choice(neg_items_pool, size=n_neg_samples, replace=False)\n",
    "                        for neg_item in neg_items:\n",
    "                            test_instances.append([user, neg_item, 0.0])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing user {user}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        if len(test_instances) == 0:\n",
    "            raise ValueError(\"No test instances were generated!\")\n",
    "            \n",
    "        print(f\"Generated {len(test_instances)} test instances\")\n",
    "        return np.array(test_instances)\n",
    "\n",
    "def evaluate_model(model, dataset, test_instances, k=10):\n",
    "    model.eval()\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    test_users = {}\n",
    "    for instance in test_instances:\n",
    "        user, item, label = instance\n",
    "        if user not in test_users:\n",
    "            test_users[user] = {'pos': [], 'neg': []}\n",
    "        if label == 1.0:\n",
    "            test_users[user]['pos'].append(item)\n",
    "        else:\n",
    "            test_users[user]['neg'].append(item)\n",
    "    \n",
    "    print(f\"Evaluating {len(test_users)} users...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, user in enumerate(test_users):\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Evaluating user {i}/{len(test_users)}\")\n",
    "                \n",
    "            items = test_users[user]['pos'] + test_users[user]['neg']\n",
    "            if not items:\n",
    "                continue\n",
    "                \n",
    "            user_tensor = torch.LongTensor([user] * len(items)).to(device)\n",
    "            item_tensor = torch.LongTensor(items).to(device)\n",
    "            behavior_tensor = torch.zeros(len(items), dataset.n_behaviors).to(device)\n",
    "            \n",
    "            try:\n",
    "                predictions = model(user_tensor, item_tensor, behavior_tensor)\n",
    "                predictions = predictions.cpu().numpy()\n",
    "                \n",
    "                pos_items = set(test_users[user]['pos'])\n",
    "                item_score_dict = {items[i]: predictions[i] for i in range(len(items))}\n",
    "                item_score_pair_sorted = sorted(item_score_dict.items(), key=lambda x: x[1], reverse=True)[0:k]\n",
    "                items_sorted = [x[0] for x in item_score_pair_sorted]\n",
    "                \n",
    "                hit = len(set(items_sorted) & pos_items) > 0\n",
    "                hits.append(hit)\n",
    "                \n",
    "                ndcg = 0\n",
    "                for i, item in enumerate(items_sorted):\n",
    "                    if item in pos_items:\n",
    "                        ndcg = 1 / np.log2(i + 2)\n",
    "                        break\n",
    "                ndcgs.append(ndcg)\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating user {user}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    if not hits:\n",
    "        return 0.0, 0.0\n",
    "        \n",
    "    return np.mean(hits), np.mean(ndcgs)\n",
    "\n",
    "def main():\n",
    "    print(\"Initializing dataset...\")\n",
    "    dataset = MultiBehaviorDataset()\n",
    "    train_data = dataset.prepare_train_instances()\n",
    "\n",
    "    config = {\n",
    "        'latent_dim': 64,\n",
    "        'batch_size': 1024,\n",
    "        'num_epochs': 50,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-5,\n",
    "        'dropout': 0.1\n",
    "    }\n",
    "\n",
    "    print(\"Initializing model...\")\n",
    "    model = MultiBehaviorBiasMF(\n",
    "        n_users=dataset.n_users,\n",
    "        n_items=dataset.n_items,\n",
    "        n_behaviors=dataset.n_behaviors,\n",
    "        latent_dim=config['latent_dim'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=config['learning_rate'], \n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "\n",
    "    print(\"Preparing training tensors...\")\n",
    "    train_users = torch.LongTensor(train_data[:, 0]).to(device)\n",
    "    train_items = torch.LongTensor(train_data[:, 1]).to(device)\n",
    "    train_labels = torch.FloatTensor(train_data[:, 2]).to(device)\n",
    "    train_behaviors = torch.FloatTensor(train_data[:, 3:]).to(device)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    best_hr = 0\n",
    "    best_ndcg = 0\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{config['num_epochs']}\")\n",
    "        print(\"Training...\")\n",
    "        \n",
    "        for start_idx in range(0, len(train_users), config['batch_size']):\n",
    "            end_idx = min(start_idx + config['batch_size'], len(train_users))\n",
    "            \n",
    "            user_batch = train_users[start_idx:end_idx]\n",
    "            item_batch = train_items[start_idx:end_idx]\n",
    "            label_batch = train_labels[start_idx:end_idx]\n",
    "            behavior_batch = train_behaviors[start_idx:end_idx]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(user_batch, item_batch, behavior_batch)\n",
    "            loss = criterion(predictions, label_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            if num_batches % 100 == 0:\n",
    "                print(f\"Batch {num_batches}: Loss = {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1} completed. Average loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(\"\\nEvaluating...\")\n",
    "            test_instances = dataset.get_test_instances()\n",
    "            hr, ndcg = evaluate_model(model, dataset, test_instances)\n",
    "            print(f'HR@10: {hr:.4f}, NDCG@10: {ndcg:.4f}')\n",
    "            \n",
    "            if hr > best_hr:\n",
    "                best_hr = hr\n",
    "                best_ndcg = ndcg\n",
    "                print(\"New best HR achieved!\")\n",
    "                \n",
    "    print(f\"\\nTraining completed.\")\n",
    "    print(f\"Best HR@10: {best_hr:.4f}\")\n",
    "    print(f\"Best NDCG@10: {best_ndcg:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Initializing dataset...\n",
      "Loading training data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/\n",
      "Loading behavior: pv from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_pv\n",
      "Loaded pv matrix with shape: (21716, 7977)\n",
      "Loading behavior: cart from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_cart\n",
      "Loaded cart matrix with shape: (21716, 7977)\n",
      "Loading behavior: buy from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_buy\n",
      "Loaded buy matrix with shape: (21716, 7977)\n",
      "Dataset dimensions: 21716 users, 7977 items\n",
      "Loading test data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/tst_int\n",
      "Number of test users: 10000\n",
      "Preparing training instances...\n",
      "Processing user 0/21716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/20z1hn994jd04q4kl0gpgh740000gn/T/ipykernel_1586/2901519435.py:107: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  mat = pickle.load(fs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing user 1000/21716\n",
      "Processing user 2000/21716\n",
      "Processing user 3000/21716\n",
      "Processing user 4000/21716\n",
      "Processing user 5000/21716\n",
      "Processing user 6000/21716\n",
      "Processing user 7000/21716\n",
      "Processing user 8000/21716\n",
      "Processing user 9000/21716\n",
      "Processing user 10000/21716\n",
      "Processing user 11000/21716\n",
      "Processing user 12000/21716\n",
      "Processing user 13000/21716\n",
      "Processing user 14000/21716\n",
      "Processing user 15000/21716\n",
      "Processing user 16000/21716\n",
      "Processing user 17000/21716\n",
      "Processing user 18000/21716\n",
      "Processing user 19000/21716\n",
      "Processing user 20000/21716\n",
      "Processing user 21000/21716\n",
      "Generated 1414300 training instances\n",
      "Train size: 1272870, Validation size: 141430\n",
      "Initializing model...\n",
      "Preparing training tensors...\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0062\n",
      "Batch 200: Loss = 0.0014\n",
      "Batch 300: Loss = 0.0008\n",
      "Batch 400: Loss = 0.0003\n",
      "Batch 500: Loss = 0.0002\n",
      "Batch 600: Loss = 0.0005\n",
      "Batch 700: Loss = 0.0001\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 1 completed. Average loss: 0.0120\n",
      "\n",
      "Epoch 2/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 2 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 3/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 3 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 4/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 4 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 5/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 5 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.3937, NDCG@10: 0.2160\n",
      "Model saved to saved_models/model_20241228_203851.pth\n",
      "New best HR achieved!\n",
      "\n",
      "Epoch 6/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 6 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 7/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 7 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 8/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 8 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 9/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 9 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 10/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 10 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.3674, NDCG@10: 0.2011\n",
      "\n",
      "Epoch 11/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 11 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 12/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 12 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 13/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 13 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 14/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 14 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 15/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 15 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.3430, NDCG@10: 0.1925\n",
      "\n",
      "Epoch 16/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 16 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 17/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 17 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 18/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 18 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 19/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 19 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 20/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 20 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.3431, NDCG@10: 0.1936\n",
      "\n",
      "Epoch 21/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 21 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 22/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 22 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 23/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 23 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 24/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 24 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 25/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 25 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.3449, NDCG@10: 0.1934\n",
      "Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n",
      "\n",
      "Epoch 26/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 26 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 27/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 27 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 28/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 28 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 29/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 29 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 30/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 30 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.3429, NDCG@10: 0.1911\n",
      "\n",
      "Epoch 31/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 31 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 32/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 32 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 33/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 33 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 34/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 34 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 35/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 35 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.3412, NDCG@10: 0.1917\n",
      "\n",
      "Epoch 36/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 36 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 37/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 37 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 38/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 38 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 39/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 39 completed. Average loss: 0.0000\n",
      "\n",
      "Epoch 40/50\n",
      "Training...\n",
      "Batch 100: Loss = 0.0000\n",
      "Batch 200: Loss = 0.0000\n",
      "Batch 300: Loss = 0.0000\n",
      "Batch 400: Loss = 0.0000\n",
      "Batch 500: Loss = 0.0000\n",
      "Batch 600: Loss = 0.0000\n",
      "Batch 700: Loss = 0.0000\n",
      "Batch 800: Loss = 0.0000\n",
      "Batch 900: Loss = 0.0000\n",
      "Batch 1000: Loss = 0.0000\n",
      "Batch 1100: Loss = 0.0000\n",
      "Batch 1200: Loss = 0.0000\n",
      "Epoch 40 completed. Average loss: 0.0000\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 10000 users...\n",
      "Evaluating user 0/10000\n",
      "Evaluating user 1000/10000\n",
      "Evaluating user 2000/10000\n",
      "Evaluating user 3000/10000\n",
      "Evaluating user 4000/10000\n",
      "Evaluating user 5000/10000\n",
      "Evaluating user 6000/10000\n",
      "Evaluating user 7000/10000\n",
      "Evaluating user 8000/10000\n",
      "Evaluating user 9000/10000\n",
      "HR@10: 0.3437, NDCG@10: 0.1909\n",
      "Early stopping triggered\n",
      "\n",
      "Training completed.\n",
      "Best HR@10: 0.3937\n",
      "Best NDCG@10: 0.2160\n",
      "Model saved to: saved_models/model_20241228_203851.pth\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_hr = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, hr):\n",
    "        if self.best_hr is None:\n",
    "            self.best_hr = hr\n",
    "        elif hr < self.best_hr + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_hr = hr\n",
    "            self.counter = 0\n",
    "\n",
    "class MultiBehaviorBiasMF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_behaviors, latent_dim=64, dropout=0.1):\n",
    "        super(MultiBehaviorBiasMF, self).__init__()\n",
    "        \n",
    "        self.n_behaviors = n_behaviors\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # User and item embeddings\n",
    "        self.user_embedding = nn.Embedding(n_users, latent_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, latent_dim)\n",
    "        \n",
    "        # Behavior-specific embeddings\n",
    "        self.behavior_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(2, latent_dim) for _ in range(n_behaviors)\n",
    "        ])\n",
    "        \n",
    "        # Bias terms\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        \n",
    "        # MLP for combining behavior signals\n",
    "        self.behavior_mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim * (2 + n_behaviors), latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(latent_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "        for beh_emb in self.behavior_embeddings:\n",
    "            nn.init.xavier_uniform_(beh_emb.weight)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        \n",
    "    def forward(self, user_indices, item_indices, behavior_data):\n",
    "        user_emb = self.user_embedding(user_indices)\n",
    "        item_emb = self.item_embedding(item_indices)\n",
    "        \n",
    "        behavior_embs = []\n",
    "        for i in range(self.n_behaviors):\n",
    "            beh_data = behavior_data[:, i].long()\n",
    "            beh_emb = self.behavior_embeddings[i](beh_data)\n",
    "            behavior_embs.append(beh_emb)\n",
    "        \n",
    "        combined = torch.cat([user_emb, item_emb] + behavior_embs, dim=1)\n",
    "        pred = self.behavior_mlp(combined).squeeze()\n",
    "        pred = pred + self.user_bias(user_indices).squeeze()\n",
    "        pred = pred + self.item_bias(item_indices).squeeze()\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "class MultiBehaviorDataset:\n",
    "    def __init__(self, data_path='/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/'):\n",
    "        self.data_path = data_path\n",
    "        self.behaviors = ['pv', 'cart', 'buy']\n",
    "        self.trn_file = data_path + 'trn_'\n",
    "        self.tst_file = data_path + 'tst_'\n",
    "        \n",
    "        self.load_training_data()\n",
    "        self.load_test_data()\n",
    "        \n",
    "    def load_training_data(self):\n",
    "        print(\"Loading training data from:\", self.data_path)\n",
    "        self.trn_mats = []\n",
    "        for beh in self.behaviors:\n",
    "            path = self.trn_file + beh\n",
    "            print(f\"Loading behavior: {beh} from {path}\")\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"File not found: {path}\")\n",
    "            with open(path, 'rb') as fs:\n",
    "                mat = pickle.load(fs)\n",
    "                if not isinstance(mat, csr_matrix):\n",
    "                    mat = csr_matrix(mat)\n",
    "                mat = (mat != 0).astype(np.float32)\n",
    "                self.trn_mats.append(mat)\n",
    "                print(f\"Loaded {beh} matrix with shape: {mat.shape}\")\n",
    "        \n",
    "        self.trn_label = 1 * (self.trn_mats[-1] != 0)\n",
    "        self.n_users, self.n_items = self.trn_mats[0].shape\n",
    "        self.n_behaviors = len(self.behaviors)\n",
    "        print(f\"Dataset dimensions: {self.n_users} users, {self.n_items} items\")\n",
    "        \n",
    "    def load_test_data(self):\n",
    "        test_path = self.tst_file + 'int'\n",
    "        print(f\"Loading test data from: {test_path}\")\n",
    "        if not os.path.exists(test_path):\n",
    "            raise FileNotFoundError(f\"File not found: {test_path}\")\n",
    "        with open(test_path, 'rb') as fs:\n",
    "            self.tst_int = np.array(pickle.load(fs))\n",
    "        \n",
    "        self.tst_users = np.reshape(np.argwhere(self.tst_int != None), [-1])\n",
    "        print(f\"Number of test users: {len(self.tst_users)}\")\n",
    "\n",
    "    def prepare_train_instances(self):\n",
    "        print(\"Preparing training instances...\")\n",
    "        train_data = []\n",
    "        \n",
    "        for user in range(self.n_users):\n",
    "            if user % 1000 == 0:\n",
    "                print(f\"Processing user {user}/{self.n_users}\")\n",
    "                \n",
    "            pos_items = self.trn_label[user].indices\n",
    "            \n",
    "            if len(pos_items) > 0:\n",
    "                for item in pos_items:\n",
    "                    behaviors = [float(mat[user, item]) for mat in self.trn_mats]\n",
    "                    train_data.append([user, item, 1.0] + behaviors)\n",
    "                    \n",
    "                    all_items = set(range(self.n_items))\n",
    "                    pos_items_set = set(pos_items)\n",
    "                    neg_items_pool = list(all_items - pos_items_set)\n",
    "                    \n",
    "                    if len(neg_items_pool) >= 4:\n",
    "                        neg_items = np.random.choice(neg_items_pool, size=4, replace=False)\n",
    "                        for neg_item in neg_items:\n",
    "                            behaviors = [float(mat[user, neg_item]) for mat in self.trn_mats]\n",
    "                            train_data.append([user, neg_item, 0.0] + behaviors)\n",
    "        \n",
    "        if len(train_data) == 0:\n",
    "            raise ValueError(\"No training instances were generated!\")\n",
    "            \n",
    "        print(f\"Generated {len(train_data)} training instances\")\n",
    "        return np.array(train_data)\n",
    "\n",
    "    def get_test_instances(self):\n",
    "        print(\"Preparing test instances...\")\n",
    "        test_instances = []\n",
    "        for user in self.tst_users:\n",
    "            pos_item = self.tst_int[user]\n",
    "            if pos_item is not None:\n",
    "                test_instances.append([user, pos_item, 1.0])\n",
    "                \n",
    "                try:\n",
    "                    all_items = set(range(self.n_items))\n",
    "                    pos_items_train = set(self.trn_label[user].indices)\n",
    "                    pos_items_train.add(pos_item)\n",
    "                    neg_items_pool = list(all_items - pos_items_train)\n",
    "                    \n",
    "                    n_neg_samples = min(99, len(neg_items_pool))\n",
    "                    if n_neg_samples > 0:\n",
    "                        neg_items = np.random.choice(neg_items_pool, size=n_neg_samples, replace=False)\n",
    "                        for neg_item in neg_items:\n",
    "                            test_instances.append([user, neg_item, 0.0])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing user {user}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        if len(test_instances) == 0:\n",
    "            raise ValueError(\"No test instances were generated!\")\n",
    "            \n",
    "        print(f\"Generated {len(test_instances)} test instances\")\n",
    "        return np.array(test_instances)\n",
    "    \n",
    "def save_model(model, optimizer, epoch, hr, ndcg, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'hr': hr,\n",
    "        'ndcg': ndcg,\n",
    "    }, path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(model, optimizer, path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['hr'], checkpoint['ndcg']\n",
    "\n",
    "def split_train_val(train_data, val_ratio=0.1):\n",
    "    indices = np.random.permutation(len(train_data))\n",
    "    val_size = int(len(train_data) * val_ratio)\n",
    "    val_indices = indices[:val_size]\n",
    "    train_indices = indices[val_size:]\n",
    "    return train_data[train_indices], train_data[val_indices]\n",
    "\n",
    "def evaluate_model(model, dataset, test_instances, k=10):\n",
    "    model.eval()\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    test_users = {}\n",
    "    for instance in test_instances:\n",
    "        user, item, label = instance\n",
    "        if user not in test_users:\n",
    "            test_users[user] = {'pos': [], 'neg': []}\n",
    "        if label == 1.0:\n",
    "            test_users[user]['pos'].append(item)\n",
    "        else:\n",
    "            test_users[user]['neg'].append(item)\n",
    "    \n",
    "    print(f\"Evaluating {len(test_users)} users...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, user in enumerate(test_users):\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Evaluating user {i}/{len(test_users)}\")\n",
    "                \n",
    "            items = test_users[user]['pos'] + test_users[user]['neg']\n",
    "            if not items:\n",
    "                continue\n",
    "                \n",
    "            user_tensor = torch.LongTensor([user] * len(items)).to(device)\n",
    "            item_tensor = torch.LongTensor(items).to(device)\n",
    "            behavior_tensor = torch.zeros(len(items), dataset.n_behaviors).to(device)\n",
    "            \n",
    "            try:\n",
    "                predictions = model(user_tensor, item_tensor, behavior_tensor)\n",
    "                predictions = predictions.cpu().numpy()\n",
    "                \n",
    "                pos_items = set(test_users[user]['pos'])\n",
    "                item_score_dict = {items[i]: predictions[i] for i in range(len(items))}\n",
    "                item_score_pair_sorted = sorted(item_score_dict.items(), key=lambda x: x[1], reverse=True)[0:k]\n",
    "                items_sorted = [x[0] for x in item_score_pair_sorted]\n",
    "                \n",
    "                hit = len(set(items_sorted) & pos_items) > 0\n",
    "                hits.append(hit)\n",
    "                \n",
    "                ndcg = 0\n",
    "                for i, item in enumerate(items_sorted):\n",
    "                    if item in pos_items:\n",
    "                        ndcg = 1 / np.log2(i + 2)\n",
    "                        break\n",
    "                ndcgs.append(ndcg)\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating user {user}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    if not hits:\n",
    "        return 0.0, 0.0\n",
    "        \n",
    "    return np.mean(hits), np.mean(ndcgs)\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'latent_dim': 64,\n",
    "        'batch_size': 1024,\n",
    "        'num_epochs': 50,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-5,\n",
    "        'dropout': 0.1,\n",
    "        'patience': 7,\n",
    "        'val_ratio': 0.1,\n",
    "        'eval_every': 5\n",
    "    }\n",
    "    \n",
    "    # Create timestamp for model saving\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    model_dir = 'saved_models'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_dir, f'model_{timestamp}.pth')\n",
    "\n",
    "    print(\"Initializing dataset...\")\n",
    "    dataset = MultiBehaviorDataset()\n",
    "    train_data = dataset.prepare_train_instances()\n",
    "    \n",
    "    # Split into train and validation\n",
    "    train_data, val_data = split_train_val(train_data, config['val_ratio'])\n",
    "    print(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}\")\n",
    "\n",
    "    print(\"Initializing model...\")\n",
    "    model = MultiBehaviorBiasMF(\n",
    "        n_users=dataset.n_users,\n",
    "        n_items=dataset.n_items,\n",
    "        n_behaviors=dataset.n_behaviors,\n",
    "        latent_dim=config['latent_dim'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=config['learning_rate'], \n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "\n",
    "    print(\"Preparing training tensors...\")\n",
    "    train_users = torch.LongTensor(train_data[:, 0]).to(device)\n",
    "    train_items = torch.LongTensor(train_data[:, 1]).to(device)\n",
    "    train_labels = torch.FloatTensor(train_data[:, 2]).to(device)\n",
    "    train_behaviors = torch.FloatTensor(train_data[:, 3:]).to(device)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    early_stopping = EarlyStopping(patience=config['patience'])\n",
    "    best_hr = 0\n",
    "    best_ndcg = 0\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{config['num_epochs']}\")\n",
    "        print(\"Training...\")\n",
    "        \n",
    "        for start_idx in range(0, len(train_users), config['batch_size']):\n",
    "            end_idx = min(start_idx + config['batch_size'], len(train_users))\n",
    "            \n",
    "            user_batch = train_users[start_idx:end_idx]\n",
    "            item_batch = train_items[start_idx:end_idx]\n",
    "            label_batch = train_labels[start_idx:end_idx]\n",
    "            behavior_batch = train_behaviors[start_idx:end_idx]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(user_batch, item_batch, behavior_batch)\n",
    "            loss = criterion(predictions, label_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            if num_batches % 100 == 0:\n",
    "                print(f\"Batch {num_batches}: Loss = {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1} completed. Average loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        if (epoch + 1) % config['eval_every'] == 0:\n",
    "            print(\"\\nEvaluating...\")\n",
    "            test_instances = dataset.get_test_instances()\n",
    "            hr, ndcg = evaluate_model(model, dataset, test_instances)\n",
    "            print(f'HR@10: {hr:.4f}, NDCG@10: {ndcg:.4f}')\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(hr)\n",
    "            \n",
    "            if hr > best_hr:\n",
    "                best_hr = hr\n",
    "                best_ndcg = ndcg\n",
    "                save_model(model, optimizer, epoch, hr, ndcg, model_path)\n",
    "                print(\"New best HR achieved!\")\n",
    "            \n",
    "            early_stopping(hr)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "                \n",
    "    print(f\"\\nTraining completed.\")\n",
    "    print(f\"Best HR@10: {best_hr:.4f}\")\n",
    "    print(f\"Best NDCG@10: {best_ndcg:.4f}\")\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from here it started regressing to true results and balance neg sample s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "Loading training data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/\n",
      "Loading behavior: pv from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_pv\n",
      "Loading behavior: cart from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_cart\n",
      "Loading behavior: buy from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_buy\n",
      "Dataset dimensions: 21716 users, 7977 items\n",
      "Loading test data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/tst_int\n",
      "Using 10000 test users\n",
      "Preparing training instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/20z1hn994jd04q4kl0gpgh740000gn/T/ipykernel_37763/525910023.py:147: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  mat = pickle.load(fs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6000 training instances\n",
      "Using 6000 training instances\n",
      "Preparing training tensors...\n",
      "Starting training...\n",
      "Training completed in 139.57s\n",
      "Training loss: 36.5297\n",
      "\n",
      "Starting evaluation...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 1000000 instances in 10000 chunks...\n",
      "Processing chunk 1/10000\n",
      "Processing chunk 51/10000\n",
      "Processing chunk 101/10000\n",
      "Processing chunk 151/10000\n",
      "Processing chunk 201/10000\n",
      "Processing chunk 251/10000\n",
      "Processing chunk 301/10000\n",
      "Processing chunk 351/10000\n",
      "Processing chunk 401/10000\n",
      "Processing chunk 451/10000\n",
      "Processing chunk 501/10000\n",
      "Processing chunk 551/10000\n",
      "Processing chunk 601/10000\n",
      "Processing chunk 651/10000\n",
      "Processing chunk 701/10000\n",
      "Processing chunk 751/10000\n",
      "Processing chunk 801/10000\n",
      "Processing chunk 851/10000\n",
      "Processing chunk 901/10000\n",
      "Processing chunk 951/10000\n",
      "Processing chunk 1001/10000\n",
      "Processing chunk 1051/10000\n",
      "Processing chunk 1101/10000\n",
      "Processing chunk 1151/10000\n",
      "Processing chunk 1201/10000\n",
      "Processing chunk 1251/10000\n",
      "Processing chunk 1301/10000\n",
      "Processing chunk 1351/10000\n",
      "Processing chunk 1401/10000\n",
      "Processing chunk 1451/10000\n",
      "Processing chunk 1501/10000\n",
      "Processing chunk 1551/10000\n",
      "Processing chunk 1601/10000\n",
      "Processing chunk 1651/10000\n",
      "Processing chunk 1701/10000\n",
      "Processing chunk 1751/10000\n",
      "Processing chunk 1801/10000\n",
      "Processing chunk 1851/10000\n",
      "Processing chunk 1901/10000\n",
      "Processing chunk 1951/10000\n",
      "Processing chunk 2001/10000\n",
      "Processing chunk 2051/10000\n",
      "Processing chunk 2101/10000\n",
      "Processing chunk 2151/10000\n",
      "Processing chunk 2201/10000\n",
      "Processing chunk 2251/10000\n",
      "Processing chunk 2301/10000\n",
      "Processing chunk 2351/10000\n",
      "Processing chunk 2401/10000\n",
      "Processing chunk 2451/10000\n",
      "Processing chunk 2501/10000\n",
      "Processing chunk 2551/10000\n",
      "Processing chunk 2601/10000\n",
      "Processing chunk 2651/10000\n",
      "Processing chunk 2701/10000\n",
      "Processing chunk 2751/10000\n",
      "Processing chunk 2801/10000\n",
      "Processing chunk 2851/10000\n",
      "Processing chunk 2901/10000\n",
      "Processing chunk 2951/10000\n",
      "Processing chunk 3001/10000\n",
      "Processing chunk 3051/10000\n",
      "Processing chunk 3101/10000\n",
      "Processing chunk 3151/10000\n",
      "Processing chunk 3201/10000\n",
      "Processing chunk 3251/10000\n",
      "Processing chunk 3301/10000\n",
      "Processing chunk 3351/10000\n",
      "Processing chunk 3401/10000\n",
      "Processing chunk 3451/10000\n",
      "Processing chunk 3501/10000\n",
      "Processing chunk 3551/10000\n",
      "Processing chunk 3601/10000\n",
      "Processing chunk 3651/10000\n",
      "Processing chunk 3701/10000\n",
      "Processing chunk 3751/10000\n",
      "Processing chunk 3801/10000\n",
      "Processing chunk 3851/10000\n",
      "Processing chunk 3901/10000\n",
      "Processing chunk 3951/10000\n",
      "Processing chunk 4001/10000\n",
      "Processing chunk 4051/10000\n",
      "Processing chunk 4101/10000\n",
      "Processing chunk 4151/10000\n",
      "Processing chunk 4201/10000\n",
      "Processing chunk 4251/10000\n",
      "Processing chunk 4301/10000\n",
      "Processing chunk 4351/10000\n",
      "Processing chunk 4401/10000\n",
      "Processing chunk 4451/10000\n",
      "Processing chunk 4501/10000\n",
      "Processing chunk 4551/10000\n",
      "Processing chunk 4601/10000\n",
      "Processing chunk 4651/10000\n",
      "Processing chunk 4701/10000\n",
      "Processing chunk 4751/10000\n",
      "Processing chunk 4801/10000\n",
      "Processing chunk 4851/10000\n",
      "Processing chunk 4901/10000\n",
      "Processing chunk 4951/10000\n",
      "Processing chunk 5001/10000\n",
      "Processing chunk 5051/10000\n",
      "Processing chunk 5101/10000\n",
      "Processing chunk 5151/10000\n",
      "Processing chunk 5201/10000\n",
      "Processing chunk 5251/10000\n",
      "Processing chunk 5301/10000\n",
      "Processing chunk 5351/10000\n",
      "Processing chunk 5401/10000\n",
      "Processing chunk 5451/10000\n",
      "Processing chunk 5501/10000\n",
      "Processing chunk 5551/10000\n",
      "Processing chunk 5601/10000\n",
      "Processing chunk 5651/10000\n",
      "Processing chunk 5701/10000\n",
      "Processing chunk 5751/10000\n",
      "Processing chunk 5801/10000\n",
      "Processing chunk 5851/10000\n",
      "Processing chunk 5901/10000\n",
      "Processing chunk 5951/10000\n",
      "Processing chunk 6001/10000\n",
      "Processing chunk 6051/10000\n",
      "Processing chunk 6101/10000\n",
      "Processing chunk 6151/10000\n",
      "Processing chunk 6201/10000\n",
      "Processing chunk 6251/10000\n",
      "Processing chunk 6301/10000\n",
      "Processing chunk 6351/10000\n",
      "Processing chunk 6401/10000\n",
      "Processing chunk 6451/10000\n",
      "Processing chunk 6501/10000\n",
      "Processing chunk 6551/10000\n",
      "Processing chunk 6601/10000\n",
      "Processing chunk 6651/10000\n",
      "Processing chunk 6701/10000\n",
      "Processing chunk 6751/10000\n",
      "Processing chunk 6801/10000\n",
      "Processing chunk 6851/10000\n",
      "Processing chunk 6901/10000\n",
      "Processing chunk 6951/10000\n",
      "Processing chunk 7001/10000\n",
      "Processing chunk 7051/10000\n",
      "Processing chunk 7101/10000\n",
      "Processing chunk 7151/10000\n",
      "Processing chunk 7201/10000\n",
      "Processing chunk 7251/10000\n",
      "Processing chunk 7301/10000\n",
      "Processing chunk 7351/10000\n",
      "Processing chunk 7401/10000\n",
      "Processing chunk 7451/10000\n",
      "Processing chunk 7501/10000\n",
      "Processing chunk 7551/10000\n",
      "Processing chunk 7601/10000\n",
      "Processing chunk 7651/10000\n",
      "Processing chunk 7701/10000\n",
      "Processing chunk 7751/10000\n",
      "Processing chunk 7801/10000\n",
      "Processing chunk 7851/10000\n",
      "Processing chunk 7901/10000\n",
      "Processing chunk 7951/10000\n",
      "Processing chunk 8001/10000\n",
      "Processing chunk 8051/10000\n",
      "Processing chunk 8101/10000\n",
      "Processing chunk 8151/10000\n",
      "Processing chunk 8201/10000\n",
      "Processing chunk 8251/10000\n",
      "Processing chunk 8301/10000\n",
      "Processing chunk 8351/10000\n",
      "Processing chunk 8401/10000\n",
      "Processing chunk 8451/10000\n",
      "Processing chunk 8501/10000\n",
      "Processing chunk 8551/10000\n",
      "Processing chunk 8601/10000\n",
      "Processing chunk 8651/10000\n",
      "Processing chunk 8701/10000\n",
      "Processing chunk 8751/10000\n",
      "Processing chunk 8801/10000\n",
      "Processing chunk 8851/10000\n",
      "Processing chunk 8901/10000\n",
      "Processing chunk 8951/10000\n",
      "Processing chunk 9001/10000\n",
      "Processing chunk 9051/10000\n",
      "Processing chunk 9101/10000\n",
      "Processing chunk 9151/10000\n",
      "Processing chunk 9201/10000\n",
      "Processing chunk 9251/10000\n",
      "Processing chunk 9301/10000\n",
      "Processing chunk 9351/10000\n",
      "Processing chunk 9401/10000\n",
      "Processing chunk 9451/10000\n",
      "Processing chunk 9501/10000\n",
      "Processing chunk 9551/10000\n",
      "Processing chunk 9601/10000\n",
      "Processing chunk 9651/10000\n",
      "Processing chunk 9701/10000\n",
      "Processing chunk 9751/10000\n",
      "Processing chunk 9801/10000\n",
      "Processing chunk 9851/10000\n",
      "Processing chunk 9901/10000\n",
      "Processing chunk 9951/10000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 10000\n",
      "Number of hits: 9998\n",
      "Average HR@10: 0.9998\n",
      "Average NDCG@10: 0.8973\n",
      "Evaluation completed in 105.90s\n",
      "HR@10: 0.9998\n",
      "NDCG@10: 0.8973\n"
     ]
    }
   ],
   "source": [
    "class GraphTransformerV2(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_feedforward, input_dim, num_weights=10, use_weights=True, dropout=0.1):\n",
    "        super(GraphTransformerV2, self).__init__()\n",
    "        self.num_weights = num_weights\n",
    "        self.use_weights = use_weights\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input projection layers\n",
    "        self.input_linear = Linear(input_dim, d_model)\n",
    "        self.dng_projection = Linear(input_dim, d_model)\n",
    "        \n",
    "        self.encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=d_feedforward, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Final output layers\n",
    "        self.pre_output = Linear(d_model, d_model)\n",
    "        self.output_linear = Linear(d_model, 1)\n",
    "        \n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.layer_norm = LayerNorm(d_model)\n",
    "        \n",
    "        if self.use_weights:\n",
    "            self.weight_linears = ModuleList([Linear(input_dim, d_model) for _ in range(num_weights)])\n",
    "\n",
    "    def compute_neighborhood_similarity(self, adjacency_matrix, x):\n",
    "        try:\n",
    "            binary_adj = (adjacency_matrix > 0).float()\n",
    "            intersection = binary_adj @ binary_adj.T\n",
    "            row_sums = binary_adj.sum(dim=1, keepdim=True)\n",
    "            col_sums = binary_adj.sum(dim=0, keepdim=True)\n",
    "            union = row_sums + col_sums.T - intersection\n",
    "            similarity = intersection / (union + 1e-8)\n",
    "            return similarity @ x\n",
    "        except RuntimeError:\n",
    "            return torch.zeros_like(x)\n",
    "\n",
    "    def project_graph_metrics(self, graph_metrics, target_dim):\n",
    "        if graph_metrics.size(1) < target_dim:\n",
    "            repeats = (target_dim + graph_metrics.size(1) - 1) // graph_metrics.size(1)\n",
    "            graph_metrics = graph_metrics.repeat(1, repeats)[:, :target_dim]\n",
    "        elif graph_metrics.size(1) > target_dim:\n",
    "            graph_metrics = graph_metrics[:, :target_dim]\n",
    "        return graph_metrics\n",
    "\n",
    "    def forward(self, x, adjacency_matrix, graph_metrics, weights=None):\n",
    "        adjacency_matrix = adjacency_matrix.float()\n",
    "        graph_metrics = graph_metrics.float()\n",
    "        batch_size, input_dim = x.shape\n",
    "        \n",
    "        if adjacency_matrix.size(0) != batch_size or adjacency_matrix.size(1) != batch_size:\n",
    "            adjacency_matrix = torch.eye(batch_size, device=x.device)\n",
    "\n",
    "        try:\n",
    "            # Direct connections\n",
    "            direct_scores = adjacency_matrix @ x\n",
    "            \n",
    "            # Neighborhood similarity\n",
    "            neighborhood_similarity = self.compute_neighborhood_similarity(adjacency_matrix, x)\n",
    "            \n",
    "            # Graph structure scores\n",
    "            if graph_metrics.dim() == 2:\n",
    "                graph_metrics_projected = self.project_graph_metrics(graph_metrics, input_dim)\n",
    "                graph_structure_scores = graph_metrics_projected * x\n",
    "            else:\n",
    "                graph_structure_scores = torch.zeros_like(x)\n",
    "\n",
    "            # Combine DNG scores and project to d_model dimension\n",
    "            dng_scores = direct_scores + neighborhood_similarity + graph_structure_scores\n",
    "            dng_scores = self.dng_projection(dng_scores)  # Project to d_model dimension\n",
    "            \n",
    "            # Process input through transformer\n",
    "            if self.use_weights and weights is not None:\n",
    "                weighted_x = torch.zeros_like(x)\n",
    "                for i, weight in enumerate(weights.T):\n",
    "                    weighted_x += self.weight_linears[i](x) * weight.unsqueeze(1)\n",
    "                transformer_input = weighted_x\n",
    "            else:\n",
    "                transformer_input = self.input_linear(x)  # Project to d_model dimension\n",
    "\n",
    "            # Apply transformer\n",
    "            transformer_input = self.layer_norm(transformer_input)\n",
    "            transformer_output = self.transformer_encoder(transformer_input.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Combine transformer output with DNG scores\n",
    "            combined = transformer_output + dng_scores\n",
    "            combined = self.dropout(combined)\n",
    "            \n",
    "            # Final output processing\n",
    "            output = self.pre_output(combined)\n",
    "            output = F.relu(output)\n",
    "            output = self.output_linear(output)\n",
    "            \n",
    "            return output.squeeze(-1)  # Return [batch_size] tensor\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError during forward pass: {e}\")\n",
    "            print(f\"x shape: {x.shape}, adjacency_matrix shape: {adjacency_matrix.shape}, graph_metrics shape: {graph_metrics.shape}\")\n",
    "            raise\n",
    "\n",
    "# cropped dataset\n",
    "class MultiBehaviorDataset:\n",
    "    def __init__(self, data_path='/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/', max_users=10000):\n",
    "        self.data_path = data_path\n",
    "        self.behaviors = ['pv', 'cart', 'buy']\n",
    "        self.trn_file = data_path + 'trn_'\n",
    "        self.tst_file = data_path + 'tst_'\n",
    "        self.max_users = max_users  # Limit number of users\n",
    "        \n",
    "        self.load_training_data()\n",
    "        self.load_test_data()\n",
    "    \n",
    "    def load_training_data(self):\n",
    "        print(\"Loading training data from:\", self.data_path)\n",
    "        self.trn_mats = []\n",
    "        for beh in self.behaviors:\n",
    "            path = self.trn_file + beh\n",
    "            print(f\"Loading behavior: {beh} from {path}\")\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"File not found: {path}\")\n",
    "            with open(path, 'rb') as fs:\n",
    "                mat = pickle.load(fs)\n",
    "                if not isinstance(mat, csr_matrix):\n",
    "                    mat = csr_matrix(mat)\n",
    "                mat = (mat != 0).astype(np.float32)\n",
    "                self.trn_mats.append(mat)\n",
    "        \n",
    "        self.trn_label = 1 * (self.trn_mats[-1] != 0)\n",
    "        self.n_users, self.n_items = self.trn_mats[0].shape\n",
    "        self.n_behaviors = len(self.behaviors)\n",
    "        print(f\"Dataset dimensions: {self.n_users} users, {self.n_items} items\")\n",
    "\n",
    "    def create_adjacency_matrix(self, users):\n",
    "        \"\"\"Optimized adjacency matrix creation\"\"\"\n",
    "        batch_size = len(users)\n",
    "        adj_matrix = torch.zeros(batch_size, batch_size, device=device)\n",
    "        \n",
    "        # Pre-compute user item sets\n",
    "        user_item_sets = []\n",
    "        for user in users:\n",
    "            user_items = set()\n",
    "            for mat in self.trn_mats:\n",
    "                user_items.update(mat[user].indices)\n",
    "            user_item_sets.append(user_items)\n",
    "        \n",
    "        # Compute similarities in parallel\n",
    "        for i in range(batch_size):\n",
    "            if not user_item_sets[i]:\n",
    "                continue\n",
    "            for j in range(i+1, batch_size):\n",
    "                if user_item_sets[j]:\n",
    "                    jaccard = len(user_item_sets[i] & user_item_sets[j]) / len(user_item_sets[i] | user_item_sets[j])\n",
    "                    adj_matrix[i,j] = adj_matrix[j,i] = jaccard\n",
    "        \n",
    "        return adj_matrix\n",
    "\n",
    "    def create_graph_metrics(self, users):\n",
    "        \"\"\"Optimized graph metrics creation\"\"\"\n",
    "        metrics = torch.zeros(len(users), 3, device=device)\n",
    "        \n",
    "        # Vectorized computation\n",
    "        for i, user in enumerate(users):\n",
    "            interactions = np.array([mat[user].nnz for mat in self.trn_mats])\n",
    "            metrics[i,0] = sum(interactions) / 100\n",
    "            metrics[i,1] = (interactions > 0).sum() / len(self.behaviors)\n",
    "            metrics[i,2] = interactions[-1] / max(interactions[0], 1)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def prepare_train_instances(self, max_samples=5000):\n",
    "        \"\"\"Optimized training instance preparation\"\"\"\n",
    "        print(\"Preparing training instances...\")\n",
    "        train_data = []\n",
    "        \n",
    "        # Randomly select users\n",
    "        selected_users = np.random.choice(min(self.n_users, self.max_users), size=min(1000, self.max_users), replace=False)\n",
    "        \n",
    "        for user in selected_users:\n",
    "            pos_items = self.trn_label[user].indices[:2]  # Limit positive items\n",
    "            \n",
    "            if len(pos_items) > 0:\n",
    "                for item in pos_items:\n",
    "                    behaviors = [float(mat[user, item]) for mat in self.trn_mats]\n",
    "                    train_data.append([user, item, 1.0] + behaviors)\n",
    "                    \n",
    "                    # Limited negative sampling\n",
    "                    neg_items = np.random.choice(self.n_items, size=2, replace=False)\n",
    "                    for neg_item in neg_items:\n",
    "                        behaviors = [float(mat[user, neg_item]) for mat in self.trn_mats]\n",
    "                        train_data.append([user, neg_item, 0.0] + behaviors)\n",
    "                        \n",
    "                    if len(train_data) >= max_samples:\n",
    "                        break\n",
    "            \n",
    "            if len(train_data) >= max_samples:\n",
    "                break\n",
    "        \n",
    "        print(f\"Generated {len(train_data)} training instances\")\n",
    "        return np.array(train_data)\n",
    "    def load_test_data(self):\n",
    "        \"\"\"Load test data\"\"\"\n",
    "        test_path = self.tst_file + 'int'\n",
    "        print(f\"Loading test data from: {test_path}\")\n",
    "        if not os.path.exists(test_path):\n",
    "            raise FileNotFoundError(f\"File not found: {test_path}\")\n",
    "        with open(test_path, 'rb') as fs:\n",
    "            self.tst_int = np.array(pickle.load(fs))\n",
    "        \n",
    "        self.tst_users = np.reshape(np.argwhere(self.tst_int != None), [-1])\n",
    "        # Limit test users for faster processing\n",
    "        self.tst_users = self.tst_users[:min(len(self.tst_users), self.max_users)]\n",
    "        print(f\"Using {len(self.tst_users)} test users\")\n",
    "\n",
    "    def get_test_instances(self, num_neg_samples=99):\n",
    "        \"\"\"Generate test instances with negative sampling\"\"\"\n",
    "        print(\"Preparing test instances...\")\n",
    "        test_instances = []\n",
    "        \n",
    "        for user in self.tst_users:\n",
    "            pos_item = self.tst_int[user]\n",
    "            if pos_item is not None:\n",
    "                # Add positive instance\n",
    "                test_instances.append([user, pos_item, 1.0])\n",
    "                \n",
    "                # Add negative instances\n",
    "                try:\n",
    "                    # Get all items and remove positive items\n",
    "                    all_items = set(range(self.n_items))\n",
    "                    pos_items_train = set(self.trn_label[user].indices)\n",
    "                    pos_items_train.add(pos_item)\n",
    "                    neg_items_pool = list(all_items - pos_items_train)\n",
    "                    \n",
    "                    # Sample negative items\n",
    "                    n_neg = min(num_neg_samples, len(neg_items_pool))\n",
    "                    if n_neg > 0:\n",
    "                        neg_items = np.random.choice(neg_items_pool, size=n_neg, replace=False)\n",
    "                        for neg_item in neg_items:\n",
    "                            test_instances.append([user, neg_item, 0.0])\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing test user {user}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Limit the number of test instances for faster processing\n",
    "                if len(test_instances) >= self.max_users * 100:\n",
    "                    break\n",
    "        \n",
    "        if not test_instances:\n",
    "            raise ValueError(\"No test instances were generated!\")\n",
    "            \n",
    "        test_instances = np.array(test_instances)\n",
    "        print(f\"Generated {len(test_instances)} test instances\")\n",
    "        return test_instances\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset, test_instances, k=10):\n",
    "    \"\"\"Improved evaluation function with better metrics calculation\"\"\"\n",
    "    model.eval()\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    # Process test instances in smaller chunks\n",
    "    chunk_size = 100\n",
    "    test_chunks = [test_instances[i:i + chunk_size] for i in range(0, len(test_instances), chunk_size)]\n",
    "    \n",
    "    print(f\"Evaluating {len(test_instances)} instances in {len(test_chunks)} chunks...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for chunk_idx, chunk in enumerate(test_chunks):\n",
    "            if chunk_idx % 50 == 0:  # Reduced frequency of progress updates\n",
    "                print(f\"Processing chunk {chunk_idx + 1}/{len(test_chunks)}\")\n",
    "            \n",
    "            # Group by user and ensure we have both positive and negative items\n",
    "            user_items = {}\n",
    "            for inst in chunk:\n",
    "                user = int(inst[0])\n",
    "                item = int(inst[1])\n",
    "                label = float(inst[2])\n",
    "                \n",
    "                if user not in user_items:\n",
    "                    user_items[user] = {'pos': [], 'neg': [], 'all_items': [], 'all_scores': []}\n",
    "                \n",
    "                user_items[user]['all_items'].append(item)\n",
    "                if label > 0.5:\n",
    "                    user_items[user]['pos'].append(item)\n",
    "                else:\n",
    "                    user_items[user]['neg'].append(item)\n",
    "            \n",
    "            # Process each user that has both positive and negative items\n",
    "            for user, data in user_items.items():\n",
    "                if not data['pos'] or not data['neg']:\n",
    "                    continue\n",
    "                \n",
    "                items = data['all_items']\n",
    "                batch_size = len(items)\n",
    "                \n",
    "                # Create input features\n",
    "                behaviors = torch.zeros(batch_size, dataset.n_behaviors, device=device)\n",
    "                for i, item in enumerate(items):\n",
    "                    for j, mat in enumerate(dataset.trn_mats):\n",
    "                        behaviors[i, j] = float(mat[user, item])\n",
    "                \n",
    "                x = torch.cat([\n",
    "                    behaviors,\n",
    "                    torch.zeros(batch_size, model.input_dim - behaviors.size(1), device=device)\n",
    "                ], dim=1)\n",
    "                \n",
    "                # Simplified adjacency matrix for evaluation\n",
    "                adj_matrix = torch.eye(batch_size, device=device)\n",
    "                graph_metrics = dataset.create_graph_metrics([user] * batch_size)\n",
    "                \n",
    "                try:\n",
    "                    predictions = model(x, adj_matrix, graph_metrics)\n",
    "                    predictions = predictions.cpu().numpy().flatten()\n",
    "                    \n",
    "                    # Store all scores\n",
    "                    for item, score in zip(items, predictions):\n",
    "                        data['all_scores'].append((item, score))\n",
    "                    \n",
    "                    # Sort items by score\n",
    "                    sorted_items = [x[0] for x in sorted(data['all_scores'], key=lambda x: x[1], reverse=True)]\n",
    "                    recommended_items = sorted_items[:k]\n",
    "                    \n",
    "                    # Calculate HR\n",
    "                    hit = False\n",
    "                    for pos_item in data['pos']:\n",
    "                        if pos_item in recommended_items:\n",
    "                            hit = True\n",
    "                            break\n",
    "                    hits.append(hit)\n",
    "                    \n",
    "                    # Calculate NDCG\n",
    "                    dcg = 0\n",
    "                    idcg = 1  # Ideal DCG for one relevant item\n",
    "                    for i, item in enumerate(recommended_items):\n",
    "                        if item in data['pos']:\n",
    "                            dcg += 1 / np.log2(i + 2)\n",
    "                    ndcg = dcg / idcg\n",
    "                    ndcgs.append(ndcg)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating user {user}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    hr = np.mean(hits) if hits else 0\n",
    "    ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(f\"\\nEvaluation Statistics:\")\n",
    "    print(f\"Total users evaluated: {len(hits)}\")\n",
    "    print(f\"Number of hits: {sum(hits)}\")\n",
    "    print(f\"Average HR@{k}: {hr:.4f}\")\n",
    "    print(f\"Average NDCG@{k}: {ndcg:.4f}\")\n",
    "    \n",
    "    return hr, ndcg\n",
    "\n",
    "def get_test_instances(self, num_neg_samples=99):\n",
    "    \"\"\"Improved test instance generation\"\"\"\n",
    "    print(\"Preparing test instances...\")\n",
    "    test_instances = []\n",
    "    max_test_users = 1000  # Limit number of test users\n",
    "    \n",
    "    # Randomly sample test users if there are too many\n",
    "    test_users = self.tst_users\n",
    "    if len(test_users) > max_test_users:\n",
    "        test_users = np.random.choice(test_users, max_test_users, replace=False)\n",
    "    \n",
    "    for user in test_users:\n",
    "        user = int(user)\n",
    "        pos_item = self.tst_int[user]\n",
    "        if pos_item is not None:\n",
    "            pos_item = int(pos_item)\n",
    "            test_instances.append([user, pos_item, 1.0])\n",
    "            \n",
    "            try:\n",
    "                # Get negative items\n",
    "                all_items = set(range(self.n_items))\n",
    "                pos_items = set(int(x) for x in self.trn_label[user].indices)\n",
    "                pos_items.add(pos_item)\n",
    "                neg_items_pool = list(all_items - pos_items)\n",
    "                \n",
    "                # Sample negative items\n",
    "                n_neg = min(num_neg_samples, len(neg_items_pool))\n",
    "                if n_neg > 0:\n",
    "                    neg_items = np.random.choice(neg_items_pool, size=n_neg, replace=False)\n",
    "                    for neg_item in neg_items:\n",
    "                        test_instances.append([user, int(neg_item), 0.0])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing test user {user}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    if not test_instances:\n",
    "        raise ValueError(\"No test instances were generated!\")\n",
    "    \n",
    "    test_instances = np.array(test_instances)\n",
    "    print(f\"Generated {len(test_instances)} test instances\")\n",
    "    print(f\"Number of unique users: {len(set(test_instances[:,0]))}\")\n",
    "    return test_instances\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'd_model': 32,\n",
    "        'num_heads': 2,\n",
    "        'num_layers': 1,\n",
    "        'd_feedforward': 64,\n",
    "        'input_dim': 64,\n",
    "        'num_weights': 3,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-6,\n",
    "        'dropout': 0.1,\n",
    "        'gradient_clip': 1.0,\n",
    "        'max_samples': 500000,\n",
    "        'eval_k': 10\n",
    "    }\n",
    "    \n",
    "    print(\"Initializing dataset...\")\n",
    "    dataset = MultiBehaviorDataset(max_users=10000)\n",
    "    train_data = dataset.prepare_train_instances(max_samples=config['max_samples'])\n",
    "    \n",
    "    print(f\"Using {len(train_data)} training instances\")\n",
    "\n",
    "    model = GraphTransformerV2(\n",
    "        num_layers=config['num_layers'],\n",
    "        d_model=config['d_model'],\n",
    "        num_heads=config['num_heads'],\n",
    "        d_feedforward=config['d_feedforward'],\n",
    "        input_dim=config['input_dim'],\n",
    "        num_weights=config['num_weights'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    print(\"Preparing training tensors...\")\n",
    "    users = torch.LongTensor(train_data[:, 0]).to(device)\n",
    "    items = torch.LongTensor(train_data[:, 1]).to(device)\n",
    "    labels = torch.FloatTensor(train_data[:, 2]).to(device)\n",
    "    behaviors = torch.FloatTensor(train_data[:, 3:]).to(device)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        model.train()\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Training\n",
    "        x = torch.cat([\n",
    "            behaviors,\n",
    "            torch.zeros(len(users), config['input_dim'] - behaviors.size(1), device=device)\n",
    "        ], dim=1)\n",
    "        \n",
    "        adj_matrix = dataset.create_adjacency_matrix(users)\n",
    "        graph_metrics = dataset.create_graph_metrics(users)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x, adj_matrix, graph_metrics)\n",
    "        predictions = predictions.view(-1)\n",
    "        labels = labels.view(-1)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip'])\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"Training completed in {training_time:.2f}s\")\n",
    "        print(f\"Training loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        print(\"\\nStarting evaluation...\")\n",
    "        eval_start_time = datetime.now()\n",
    "        \n",
    "        test_instances = dataset.get_test_instances()\n",
    "        hr, ndcg = evaluate_model(model, dataset, test_instances, k=config['eval_k'])\n",
    "        \n",
    "        eval_time = (datetime.now() - eval_start_time).total_seconds()\n",
    "        print(f\"Evaluation completed in {eval_time:.2f}s\")\n",
    "        print(f\"HR@{config['eval_k']}: {hr:.4f}\")\n",
    "        print(f\"NDCG@{config['eval_k']}: {ndcg:.4f}\")\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'training_time': training_time,\n",
    "            'eval_time': eval_time,\n",
    "            'training_loss': loss.item(),\n",
    "            'hr': hr,\n",
    "            'ndcg': ndcg\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        with open(f'results_{timestamp}.txt', 'w') as f:\n",
    "            for key, value in results.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiasMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropped dataset\n",
    "class MultiBehaviorDataset:\n",
    "    def __init__(self, data_path='/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/', max_users=10000):\n",
    "        self.data_path = data_path\n",
    "        self.behaviors = ['pv', 'cart', 'buy']\n",
    "        self.trn_file = data_path + 'trn_'\n",
    "        self.tst_file = data_path + 'tst_'\n",
    "        self.max_users = max_users  # Limit number of users\n",
    "        \n",
    "        self.load_training_data()\n",
    "        self.load_test_data()\n",
    "    \n",
    "    def load_training_data(self):\n",
    "        print(\"Loading training data from:\", self.data_path)\n",
    "        self.trn_mats = []\n",
    "        for beh in self.behaviors:\n",
    "            path = self.trn_file + beh\n",
    "            print(f\"Loading behavior: {beh} from {path}\")\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"File not found: {path}\")\n",
    "            with open(path, 'rb') as fs:\n",
    "                mat = pickle.load(fs)\n",
    "                if not isinstance(mat, csr_matrix):\n",
    "                    mat = csr_matrix(mat)\n",
    "                mat = (mat != 0).astype(np.float32)\n",
    "                self.trn_mats.append(mat)\n",
    "        \n",
    "        self.trn_label = 1 * (self.trn_mats[-1] != 0)\n",
    "        self.n_users, self.n_items = self.trn_mats[0].shape\n",
    "        self.n_behaviors = len(self.behaviors)\n",
    "        print(f\"Dataset dimensions: {self.n_users} users, {self.n_items} items\")\n",
    "\n",
    "    def create_adjacency_matrix(self, users):\n",
    "        \"\"\"Optimized adjacency matrix creation\"\"\"\n",
    "        batch_size = len(users)\n",
    "        adj_matrix = torch.zeros(batch_size, batch_size, device=device)\n",
    "        \n",
    "        # Pre-compute user item sets\n",
    "        user_item_sets = []\n",
    "        for user in users:\n",
    "            user_items = set()\n",
    "            for mat in self.trn_mats:\n",
    "                user_items.update(mat[user].indices)\n",
    "            user_item_sets.append(user_items)\n",
    "        \n",
    "        # Compute similarities in parallel\n",
    "        for i in range(batch_size):\n",
    "            if not user_item_sets[i]:\n",
    "                continue\n",
    "            for j in range(i+1, batch_size):\n",
    "                if user_item_sets[j]:\n",
    "                    jaccard = len(user_item_sets[i] & user_item_sets[j]) / len(user_item_sets[i] | user_item_sets[j])\n",
    "                    adj_matrix[i,j] = adj_matrix[j,i] = jaccard\n",
    "        \n",
    "        return adj_matrix\n",
    "\n",
    "    def create_graph_metrics(self, users):\n",
    "        \"\"\"Optimized graph metrics creation\"\"\"\n",
    "        metrics = torch.zeros(len(users), 3, device=device)\n",
    "        \n",
    "        # Vectorized computation\n",
    "        for i, user in enumerate(users):\n",
    "            interactions = np.array([mat[user].nnz for mat in self.trn_mats])\n",
    "            metrics[i,0] = sum(interactions) / 100\n",
    "            metrics[i,1] = (interactions > 0).sum() / len(self.behaviors)\n",
    "            metrics[i,2] = interactions[-1] / max(interactions[0], 1)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def prepare_train_instances(self, max_samples=5000):\n",
    "        \"\"\"Optimized training instance preparation\"\"\"\n",
    "        print(\"Preparing training instances...\")\n",
    "        train_data = []\n",
    "        \n",
    "        # Randomly select users\n",
    "        selected_users = np.random.choice(min(self.n_users, self.max_users), size=min(1000, self.max_users), replace=False)\n",
    "        \n",
    "        for user in selected_users:\n",
    "            pos_items = self.trn_label[user].indices[:2]  # Limit positive items\n",
    "            \n",
    "            if len(pos_items) > 0:\n",
    "                for item in pos_items:\n",
    "                    behaviors = [float(mat[user, item]) for mat in self.trn_mats]\n",
    "                    train_data.append([user, item, 1.0] + behaviors)\n",
    "                    \n",
    "                    # Limited negative sampling\n",
    "                    neg_items = np.random.choice(self.n_items, size=2, replace=False)\n",
    "                    for neg_item in neg_items:\n",
    "                        behaviors = [float(mat[user, neg_item]) for mat in self.trn_mats]\n",
    "                        train_data.append([user, neg_item, 0.0] + behaviors)\n",
    "                        \n",
    "                    if len(train_data) >= max_samples:\n",
    "                        break\n",
    "            \n",
    "            if len(train_data) >= max_samples:\n",
    "                break\n",
    "        \n",
    "        print(f\"Generated {len(train_data)} training instances\")\n",
    "        return np.array(train_data)\n",
    "    def load_test_data(self):\n",
    "        \"\"\"Load test data\"\"\"\n",
    "        test_path = self.tst_file + 'int'\n",
    "        print(f\"Loading test data from: {test_path}\")\n",
    "        if not os.path.exists(test_path):\n",
    "            raise FileNotFoundError(f\"File not found: {test_path}\")\n",
    "        with open(test_path, 'rb') as fs:\n",
    "            self.tst_int = np.array(pickle.load(fs))\n",
    "        \n",
    "        self.tst_users = np.reshape(np.argwhere(self.tst_int != None), [-1])\n",
    "        # Limit test users for faster processing\n",
    "        self.tst_users = self.tst_users[:min(len(self.tst_users), self.max_users)]\n",
    "        print(f\"Using {len(self.tst_users)} test users\")\n",
    "\n",
    "    def get_test_instances(self, num_neg_samples=99):\n",
    "        \"\"\"Generate test instances with negative sampling\"\"\"\n",
    "        print(\"Preparing test instances...\")\n",
    "        test_instances = []\n",
    "        \n",
    "        for user in self.tst_users:\n",
    "            pos_item = self.tst_int[user]\n",
    "            if pos_item is not None:\n",
    "                # Add positive instance\n",
    "                test_instances.append([user, pos_item, 1.0])\n",
    "                \n",
    "                # Add negative instances\n",
    "                try:\n",
    "                    # Get all items and remove positive items\n",
    "                    all_items = set(range(self.n_items))\n",
    "                    pos_items_train = set(self.trn_label[user].indices)\n",
    "                    pos_items_train.add(pos_item)\n",
    "                    neg_items_pool = list(all_items - pos_items_train)\n",
    "                    \n",
    "                    # Sample negative items\n",
    "                    n_neg = min(num_neg_samples, len(neg_items_pool))\n",
    "                    if n_neg > 0:\n",
    "                        neg_items = np.random.choice(neg_items_pool, size=n_neg, replace=False)\n",
    "                        for neg_item in neg_items:\n",
    "                            test_instances.append([user, neg_item, 0.0])\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing test user {user}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Limit the number of test instances for faster processing\n",
    "                if len(test_instances) >= self.max_users * 100:\n",
    "                    break\n",
    "        \n",
    "        if not test_instances:\n",
    "            raise ValueError(\"No test instances were generated!\")\n",
    "            \n",
    "        test_instances = np.array(test_instances)\n",
    "        print(f\"Generated {len(test_instances)} test instances\")\n",
    "        return test_instances\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset, test_instances, k=10):\n",
    "    \"\"\"Improved evaluation function with better metrics calculation\"\"\"\n",
    "    model.eval()\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    # Process test instances in smaller chunks\n",
    "    chunk_size = 100\n",
    "    test_chunks = [test_instances[i:i + chunk_size] for i in range(0, len(test_instances), chunk_size)]\n",
    "    \n",
    "    print(f\"Evaluating {len(test_instances)} instances in {len(test_chunks)} chunks...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for chunk_idx, chunk in enumerate(test_chunks):\n",
    "            if chunk_idx % 50 == 0:  # Reduced frequency of progress updates\n",
    "                print(f\"Processing chunk {chunk_idx + 1}/{len(test_chunks)}\")\n",
    "            \n",
    "            # Group by user and ensure we have both positive and negative items\n",
    "            user_items = {}\n",
    "            for inst in chunk:\n",
    "                user = int(inst[0])\n",
    "                item = int(inst[1])\n",
    "                label = float(inst[2])\n",
    "                \n",
    "                if user not in user_items:\n",
    "                    user_items[user] = {'pos': [], 'neg': [], 'all_items': [], 'all_scores': []}\n",
    "                \n",
    "                user_items[user]['all_items'].append(item)\n",
    "                if label > 0.5:\n",
    "                    user_items[user]['pos'].append(item)\n",
    "                else:\n",
    "                    user_items[user]['neg'].append(item)\n",
    "            \n",
    "            # Process each user that has both positive and negative items\n",
    "            for user, data in user_items.items():\n",
    "                if not data['pos'] or not data['neg']:\n",
    "                    continue\n",
    "                \n",
    "                items = data['all_items']\n",
    "                batch_size = len(items)\n",
    "                \n",
    "                # Create input features\n",
    "                behaviors = torch.zeros(batch_size, dataset.n_behaviors, device=device)\n",
    "                for i, item in enumerate(items):\n",
    "                    for j, mat in enumerate(dataset.trn_mats):\n",
    "                        behaviors[i, j] = float(mat[user, item])\n",
    "                \n",
    "                x = torch.cat([\n",
    "                    behaviors,\n",
    "                    torch.zeros(batch_size, model.input_dim - behaviors.size(1), device=device)\n",
    "                ], dim=1)\n",
    "                \n",
    "                # Simplified adjacency matrix for evaluation\n",
    "                adj_matrix = torch.eye(batch_size, device=device)\n",
    "                graph_metrics = dataset.create_graph_metrics([user] * batch_size)\n",
    "                \n",
    "                try:\n",
    "                    predictions = model(x, adj_matrix, graph_metrics)\n",
    "                    predictions = predictions.cpu().numpy().flatten()\n",
    "                    \n",
    "                    # Store all scores\n",
    "                    for item, score in zip(items, predictions):\n",
    "                        data['all_scores'].append((item, score))\n",
    "                    \n",
    "                    # Sort items by score\n",
    "                    sorted_items = [x[0] for x in sorted(data['all_scores'], key=lambda x: x[1], reverse=True)]\n",
    "                    recommended_items = sorted_items[:k]\n",
    "                    \n",
    "                    # Calculate HR\n",
    "                    hit = False\n",
    "                    for pos_item in data['pos']:\n",
    "                        if pos_item in recommended_items:\n",
    "                            hit = True\n",
    "                            break\n",
    "                    hits.append(hit)\n",
    "                    \n",
    "                    # Calculate NDCG\n",
    "                    dcg = 0\n",
    "                    idcg = 1  # Ideal DCG for one relevant item\n",
    "                    for i, item in enumerate(recommended_items):\n",
    "                        if item in data['pos']:\n",
    "                            dcg += 1 / np.log2(i + 2)\n",
    "                    ndcg = dcg / idcg\n",
    "                    ndcgs.append(ndcg)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating user {user}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    hr = np.mean(hits) if hits else 0\n",
    "    ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(f\"\\nEvaluation Statistics:\")\n",
    "    print(f\"Total users evaluated: {len(hits)}\")\n",
    "    print(f\"Number of hits: {sum(hits)}\")\n",
    "    print(f\"Average HR@{k}: {hr:.4f}\")\n",
    "    print(f\"Average NDCG@{k}: {ndcg:.4f}\")\n",
    "    \n",
    "    return hr, ndcg\n",
    "\n",
    "def get_test_instances(self, num_neg_samples=99):\n",
    "    \"\"\"Improved test instance generation\"\"\"\n",
    "    print(\"Preparing test instances...\")\n",
    "    test_instances = []\n",
    "    max_test_users = 1000  # Limit number of test users\n",
    "    \n",
    "    # Randomly sample test users if there are too many\n",
    "    test_users = self.tst_users\n",
    "    if len(test_users) > max_test_users:\n",
    "        test_users = np.random.choice(test_users, max_test_users, replace=False)\n",
    "    \n",
    "    for user in test_users:\n",
    "        user = int(user)\n",
    "        pos_item = self.tst_int[user]\n",
    "        if pos_item is not None:\n",
    "            pos_item = int(pos_item)\n",
    "            test_instances.append([user, pos_item, 1.0])\n",
    "            \n",
    "            try:\n",
    "                # Get negative items\n",
    "                all_items = set(range(self.n_items))\n",
    "                pos_items = set(int(x) for x in self.trn_label[user].indices)\n",
    "                pos_items.add(pos_item)\n",
    "                neg_items_pool = list(all_items - pos_items)\n",
    "                \n",
    "                # Sample negative items\n",
    "                n_neg = min(num_neg_samples, len(neg_items_pool))\n",
    "                if n_neg > 0:\n",
    "                    neg_items = np.random.choice(neg_items_pool, size=n_neg, replace=False)\n",
    "                    for neg_item in neg_items:\n",
    "                        test_instances.append([user, int(neg_item), 0.0])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing test user {user}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    if not test_instances:\n",
    "        raise ValueError(\"No test instances were generated!\")\n",
    "    \n",
    "    test_instances = np.array(test_instances)\n",
    "    print(f\"Generated {len(test_instances)} test instances\")\n",
    "    print(f\"Number of unique users: {len(set(test_instances[:,0]))}\")\n",
    "    return test_instances\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'latent_dim': 64,\n",
    "        'dropout': 0.1,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-6,\n",
    "        'gradient_clip': 1.0,\n",
    "        'max_samples': 5000,\n",
    "        'eval_k': 10\n",
    "    }\n",
    "    \n",
    "    print(\"Initializing dataset...\")\n",
    "    dataset = MultiBehaviorDataset(max_users=10000)\n",
    "    train_data = dataset.prepare_train_instances(max_samples=config['max_samples'])\n",
    "    \n",
    "    print(f\"Using {len(train_data)} training instances\")\n",
    "\n",
    "    # Initialize model with dataset dimensions\n",
    "    model = MultiBehaviorBiasMF(\n",
    "        n_users=dataset.n_users,\n",
    "        n_items=dataset.n_items,\n",
    "        n_behaviors=dataset.n_behaviors,\n",
    "        latent_dim=config['latent_dim'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    print(\"Preparing training tensors...\")\n",
    "    users = torch.LongTensor(train_data[:, 0]).to(device)\n",
    "    items = torch.LongTensor(train_data[:, 1]).to(device)\n",
    "    labels = torch.FloatTensor(train_data[:, 2]).to(device)\n",
    "    behaviors = torch.FloatTensor(train_data[:, 3:]).to(device)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        model.train()\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Training\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(users, items, behaviors)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip'])\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"Training completed in {training_time:.2f}s\")\n",
    "        print(f\"Training loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        print(\"\\nStarting evaluation...\")\n",
    "        eval_start_time = datetime.now()\n",
    "        \n",
    "        test_instances = dataset.get_test_instances()\n",
    "        hr, ndcg = evaluate_model(model, dataset, test_instances, k=config['eval_k'])\n",
    "        \n",
    "        eval_time = (datetime.now() - eval_start_time).total_seconds()\n",
    "        print(f\"Evaluation completed in {eval_time:.2f}s\")\n",
    "        print(f\"HR@{config['eval_k']}: {hr:.4f}\")\n",
    "        print(f\"NDCG@{config['eval_k']}: {ndcg:.4f}\")\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'training_time': training_time,\n",
    "            'eval_time': eval_time,\n",
    "            'training_loss': loss.item(),\n",
    "            'hr': hr,\n",
    "            'ndcg': ndcg\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        with open(f'results_{timestamp}.txt', 'w') as f:\n",
    "            for key, value in results.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during training: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBehaviorBiasMF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_behaviors, latent_dim=64, dropout=0.1):\n",
    "        super(MultiBehaviorBiasMF, self).__init__()\n",
    "        \n",
    "        self.n_behaviors = n_behaviors\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # User and item embeddings\n",
    "        self.user_embedding = nn.Embedding(n_users, latent_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, latent_dim)\n",
    "        \n",
    "        # Behavior-specific embeddings\n",
    "        self.behavior_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(2, latent_dim) for _ in range(n_behaviors)\n",
    "        ])\n",
    "        \n",
    "        # Bias terms\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        \n",
    "        # MLP for combining behavior signals\n",
    "        self.behavior_mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim * (2 + n_behaviors), latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(latent_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "        for beh_emb in self.behavior_embeddings:\n",
    "            nn.init.xavier_uniform_(beh_emb.weight)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        \n",
    "    def forward(self, user_indices, item_indices, behavior_data):\n",
    "        user_emb = self.user_embedding(user_indices)\n",
    "        item_emb = self.item_embedding(item_indices)\n",
    "        \n",
    "        behavior_embs = []\n",
    "        for i in range(self.n_behaviors):\n",
    "            beh_data = behavior_data[:, i].long()\n",
    "            beh_emb = self.behavior_embeddings[i](beh_data)\n",
    "            behavior_embs.append(beh_emb)\n",
    "        \n",
    "        combined = torch.cat([user_emb, item_emb] + behavior_embs, dim=1)\n",
    "        pred = self.behavior_mlp(combined).squeeze()\n",
    "        pred = pred + self.user_bias(user_indices).squeeze()\n",
    "        pred = pred + self.item_bias(item_indices).squeeze()\n",
    "        \n",
    "        return pred\n",
    "\n",
    "def evaluate_model(model, dataset, test_instances, k=10):\n",
    "    \"\"\"Evaluation function for MultiBehaviorBiasMF\"\"\"\n",
    "    model.eval()\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    chunk_size = 100\n",
    "    test_chunks = [test_instances[i:i + chunk_size] for i in range(0, len(test_instances), chunk_size)]\n",
    "    \n",
    "    print(f\"Evaluating {len(test_instances)} instances in {len(test_chunks)} chunks...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for chunk_idx, chunk in enumerate(test_chunks):\n",
    "            if chunk_idx % 50 == 0:\n",
    "                print(f\"Processing chunk {chunk_idx + 1}/{len(test_chunks)}\")\n",
    "            \n",
    "            user_items = {}\n",
    "            for inst in chunk:\n",
    "                user = int(inst[0])\n",
    "                item = int(inst[1])\n",
    "                label = float(inst[2])\n",
    "                \n",
    "                if user not in user_items:\n",
    "                    user_items[user] = {'pos': [], 'neg': [], 'all_items': [], 'all_scores': []}\n",
    "                \n",
    "                user_items[user]['all_items'].append(item)\n",
    "                if label > 0.5:\n",
    "                    user_items[user]['pos'].append(item)\n",
    "                else:\n",
    "                    user_items[user]['neg'].append(item)\n",
    "            \n",
    "            for user, data in user_items.items():\n",
    "                if not data['pos'] or not data['neg']:\n",
    "                    continue\n",
    "                \n",
    "                items = data['all_items']\n",
    "                batch_size = len(items)\n",
    "                \n",
    "                # Create input tensors\n",
    "                user_tensor = torch.LongTensor([user] * batch_size).to(device)\n",
    "                item_tensor = torch.LongTensor(items).to(device)\n",
    "                \n",
    "                # Create behavior tensor\n",
    "                behaviors = torch.zeros(batch_size, dataset.n_behaviors, device=device)\n",
    "                for i, item in enumerate(items):\n",
    "                    for j, mat in enumerate(dataset.trn_mats):\n",
    "                        behaviors[i, j] = float(mat[user, item])\n",
    "                \n",
    "                try:\n",
    "                    predictions = model(user_tensor, item_tensor, behaviors)\n",
    "                    predictions = predictions.cpu().numpy()\n",
    "                    \n",
    "                    for item, score in zip(items, predictions):\n",
    "                        data['all_scores'].append((item, score))\n",
    "                    \n",
    "                    sorted_items = [x[0] for x in sorted(data['all_scores'], key=lambda x: x[1], reverse=True)]\n",
    "                    recommended_items = sorted_items[:k]\n",
    "                    \n",
    "                    hit = False\n",
    "                    for pos_item in data['pos']:\n",
    "                        if pos_item in recommended_items:\n",
    "                            hit = True\n",
    "                            break\n",
    "                    hits.append(hit)\n",
    "                    \n",
    "                    dcg = 0\n",
    "                    idcg = 1\n",
    "                    for i, item in enumerate(recommended_items):\n",
    "                        if item in data['pos']:\n",
    "                            dcg += 1 / np.log2(i + 2)\n",
    "                    ndcg = dcg / idcg\n",
    "                    ndcgs.append(ndcg)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating user {user}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    hr = np.mean(hits) if hits else 0\n",
    "    ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "    \n",
    "    print(f\"\\nEvaluation Statistics:\")\n",
    "    print(f\"Total users evaluated: {len(hits)}\")\n",
    "    print(f\"Number of hits: {sum(hits)}\")\n",
    "    print(f\"Average HR@{k}: {hr:.4f}\")\n",
    "    print(f\"Average NDCG@{k}: {ndcg:.4f}\")\n",
    "    \n",
    "    return hr, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "Loading training data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/\n",
      "Loading behavior: pv from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_pv\n",
      "Loading behavior: cart from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_cart\n",
      "Loading behavior: buy from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_buy\n",
      "Dataset dimensions: 21716 users, 7977 items\n",
      "Loading test data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/tst_int\n",
      "Using 10000 test users\n",
      "Preparing training instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/20z1hn994jd04q4kl0gpgh740000gn/T/ipykernel_37763/3026481301.py:22: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  mat = pickle.load(fs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5001 training instances\n",
      "Using 5001 training instances\n",
      "Preparing training tensors...\n",
      "Starting training...\n",
      "Training completed in 0.08s\n",
      "Training loss: 0.6911\n",
      "\n",
      "Starting evaluation...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 1000000 instances in 10000 chunks...\n",
      "Processing chunk 1/10000\n",
      "Processing chunk 51/10000\n",
      "Processing chunk 101/10000\n",
      "Processing chunk 151/10000\n",
      "Processing chunk 201/10000\n",
      "Processing chunk 251/10000\n",
      "Processing chunk 301/10000\n",
      "Processing chunk 351/10000\n",
      "Processing chunk 401/10000\n",
      "Processing chunk 451/10000\n",
      "Processing chunk 501/10000\n",
      "Processing chunk 551/10000\n",
      "Processing chunk 601/10000\n",
      "Processing chunk 651/10000\n",
      "Processing chunk 701/10000\n",
      "Processing chunk 751/10000\n",
      "Processing chunk 801/10000\n",
      "Processing chunk 851/10000\n",
      "Processing chunk 901/10000\n",
      "Processing chunk 951/10000\n",
      "Processing chunk 1001/10000\n",
      "Processing chunk 1051/10000\n",
      "Processing chunk 1101/10000\n",
      "Processing chunk 1151/10000\n",
      "Processing chunk 1201/10000\n",
      "Processing chunk 1251/10000\n",
      "Processing chunk 1301/10000\n",
      "Processing chunk 1351/10000\n",
      "Processing chunk 1401/10000\n",
      "Processing chunk 1451/10000\n",
      "Processing chunk 1501/10000\n",
      "Processing chunk 1551/10000\n",
      "Processing chunk 1601/10000\n",
      "Processing chunk 1651/10000\n",
      "Processing chunk 1701/10000\n",
      "Processing chunk 1751/10000\n",
      "Processing chunk 1801/10000\n",
      "Processing chunk 1851/10000\n",
      "Processing chunk 1901/10000\n",
      "Processing chunk 1951/10000\n",
      "Processing chunk 2001/10000\n",
      "Processing chunk 2051/10000\n",
      "Processing chunk 2101/10000\n",
      "Processing chunk 2151/10000\n",
      "Processing chunk 2201/10000\n",
      "Processing chunk 2251/10000\n",
      "Processing chunk 2301/10000\n",
      "Processing chunk 2351/10000\n",
      "Processing chunk 2401/10000\n",
      "Processing chunk 2451/10000\n",
      "Processing chunk 2501/10000\n",
      "Processing chunk 2551/10000\n",
      "Processing chunk 2601/10000\n",
      "Processing chunk 2651/10000\n",
      "Processing chunk 2701/10000\n",
      "Processing chunk 2751/10000\n",
      "Processing chunk 2801/10000\n",
      "Processing chunk 2851/10000\n",
      "Processing chunk 2901/10000\n",
      "Processing chunk 2951/10000\n",
      "Processing chunk 3001/10000\n",
      "Processing chunk 3051/10000\n",
      "Processing chunk 3101/10000\n",
      "Processing chunk 3151/10000\n",
      "Processing chunk 3201/10000\n",
      "Processing chunk 3251/10000\n",
      "Processing chunk 3301/10000\n",
      "Processing chunk 3351/10000\n",
      "Processing chunk 3401/10000\n",
      "Processing chunk 3451/10000\n",
      "Processing chunk 3501/10000\n",
      "Processing chunk 3551/10000\n",
      "Processing chunk 3601/10000\n",
      "Processing chunk 3651/10000\n",
      "Processing chunk 3701/10000\n",
      "Processing chunk 3751/10000\n",
      "Processing chunk 3801/10000\n",
      "Processing chunk 3851/10000\n",
      "Processing chunk 3901/10000\n",
      "Processing chunk 3951/10000\n",
      "Processing chunk 4001/10000\n",
      "Processing chunk 4051/10000\n",
      "Processing chunk 4101/10000\n",
      "Processing chunk 4151/10000\n",
      "Processing chunk 4201/10000\n",
      "Processing chunk 4251/10000\n",
      "Processing chunk 4301/10000\n",
      "Processing chunk 4351/10000\n",
      "Processing chunk 4401/10000\n",
      "Processing chunk 4451/10000\n",
      "Processing chunk 4501/10000\n",
      "Processing chunk 4551/10000\n",
      "Processing chunk 4601/10000\n",
      "Processing chunk 4651/10000\n",
      "Processing chunk 4701/10000\n",
      "Processing chunk 4751/10000\n",
      "Processing chunk 4801/10000\n",
      "Processing chunk 4851/10000\n",
      "Processing chunk 4901/10000\n",
      "Processing chunk 4951/10000\n",
      "Processing chunk 5001/10000\n",
      "Processing chunk 5051/10000\n",
      "Processing chunk 5101/10000\n",
      "Processing chunk 5151/10000\n",
      "Processing chunk 5201/10000\n",
      "Processing chunk 5251/10000\n",
      "Processing chunk 5301/10000\n",
      "Processing chunk 5351/10000\n",
      "Processing chunk 5401/10000\n",
      "Processing chunk 5451/10000\n",
      "Processing chunk 5501/10000\n",
      "Processing chunk 5551/10000\n",
      "Processing chunk 5601/10000\n",
      "Processing chunk 5651/10000\n",
      "Processing chunk 5701/10000\n",
      "Processing chunk 5751/10000\n",
      "Processing chunk 5801/10000\n",
      "Processing chunk 5851/10000\n",
      "Processing chunk 5901/10000\n",
      "Processing chunk 5951/10000\n",
      "Processing chunk 6001/10000\n",
      "Processing chunk 6051/10000\n",
      "Processing chunk 6101/10000\n",
      "Processing chunk 6151/10000\n",
      "Processing chunk 6201/10000\n",
      "Processing chunk 6251/10000\n",
      "Processing chunk 6301/10000\n",
      "Processing chunk 6351/10000\n",
      "Processing chunk 6401/10000\n",
      "Processing chunk 6451/10000\n",
      "Processing chunk 6501/10000\n",
      "Processing chunk 6551/10000\n",
      "Processing chunk 6601/10000\n",
      "Processing chunk 6651/10000\n",
      "Processing chunk 6701/10000\n",
      "Processing chunk 6751/10000\n",
      "Processing chunk 6801/10000\n",
      "Processing chunk 6851/10000\n",
      "Processing chunk 6901/10000\n",
      "Processing chunk 6951/10000\n",
      "Processing chunk 7001/10000\n",
      "Processing chunk 7051/10000\n",
      "Processing chunk 7101/10000\n",
      "Processing chunk 7151/10000\n",
      "Processing chunk 7201/10000\n",
      "Processing chunk 7251/10000\n",
      "Processing chunk 7301/10000\n",
      "Processing chunk 7351/10000\n",
      "Processing chunk 7401/10000\n",
      "Processing chunk 7451/10000\n",
      "Processing chunk 7501/10000\n",
      "Processing chunk 7551/10000\n",
      "Processing chunk 7601/10000\n",
      "Processing chunk 7651/10000\n",
      "Processing chunk 7701/10000\n",
      "Processing chunk 7751/10000\n",
      "Processing chunk 7801/10000\n",
      "Processing chunk 7851/10000\n",
      "Processing chunk 7901/10000\n",
      "Processing chunk 7951/10000\n",
      "Processing chunk 8001/10000\n",
      "Processing chunk 8051/10000\n",
      "Processing chunk 8101/10000\n",
      "Processing chunk 8151/10000\n",
      "Processing chunk 8201/10000\n",
      "Processing chunk 8251/10000\n",
      "Processing chunk 8301/10000\n",
      "Processing chunk 8351/10000\n",
      "Processing chunk 8401/10000\n",
      "Processing chunk 8451/10000\n",
      "Processing chunk 8501/10000\n",
      "Processing chunk 8551/10000\n",
      "Processing chunk 8601/10000\n",
      "Processing chunk 8651/10000\n",
      "Processing chunk 8701/10000\n",
      "Processing chunk 8751/10000\n",
      "Processing chunk 8801/10000\n",
      "Processing chunk 8851/10000\n",
      "Processing chunk 8901/10000\n",
      "Processing chunk 8951/10000\n",
      "Processing chunk 9001/10000\n",
      "Processing chunk 9051/10000\n",
      "Processing chunk 9101/10000\n",
      "Processing chunk 9151/10000\n",
      "Processing chunk 9201/10000\n",
      "Processing chunk 9251/10000\n",
      "Processing chunk 9301/10000\n",
      "Processing chunk 9351/10000\n",
      "Processing chunk 9401/10000\n",
      "Processing chunk 9451/10000\n",
      "Processing chunk 9501/10000\n",
      "Processing chunk 9551/10000\n",
      "Processing chunk 9601/10000\n",
      "Processing chunk 9651/10000\n",
      "Processing chunk 9701/10000\n",
      "Processing chunk 9751/10000\n",
      "Processing chunk 9801/10000\n",
      "Processing chunk 9851/10000\n",
      "Processing chunk 9901/10000\n",
      "Processing chunk 9951/10000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 10000\n",
      "Number of hits: 6824\n",
      "Average HR@10: 0.6824\n",
      "Average NDCG@10: 0.6389\n",
      "Evaluation completed in 35.51s\n",
      "HR@10: 0.6824\n",
      "NDCG@10: 0.6389\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GT -replica of our above gt cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "Loading training data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/\n",
      "Loading behavior: pv from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_pv\n",
      "Loading behavior: cart from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_cart\n",
      "Loading behavior: buy from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_buy\n",
      "Dataset dimensions: 21716 users, 7977 items\n",
      "Loading test data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/tst_int\n",
      "Using 10000 test users\n",
      "Preparing training instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/20z1hn994jd04q4kl0gpgh740000gn/T/ipykernel_37763/525910023.py:147: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  mat = pickle.load(fs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6000 training instances\n",
      "Using 6000 training instances\n",
      "Preparing training tensors...\n",
      "Starting training...\n",
      "Training completed in 139.57s\n",
      "Training loss: 36.5297\n",
      "\n",
      "Starting evaluation...\n",
      "Preparing test instances...\n",
      "Generated 1000000 test instances\n",
      "Evaluating 1000000 instances in 10000 chunks...\n",
      "Processing chunk 1/10000\n",
      "Processing chunk 51/10000\n",
      "Processing chunk 101/10000\n",
      "Processing chunk 151/10000\n",
      "Processing chunk 201/10000\n",
      "Processing chunk 251/10000\n",
      "Processing chunk 301/10000\n",
      "Processing chunk 351/10000\n",
      "Processing chunk 401/10000\n",
      "Processing chunk 451/10000\n",
      "Processing chunk 501/10000\n",
      "Processing chunk 551/10000\n",
      "Processing chunk 601/10000\n",
      "Processing chunk 651/10000\n",
      "Processing chunk 701/10000\n",
      "Processing chunk 751/10000\n",
      "Processing chunk 801/10000\n",
      "Processing chunk 851/10000\n",
      "Processing chunk 901/10000\n",
      "Processing chunk 951/10000\n",
      "Processing chunk 1001/10000\n",
      "Processing chunk 1051/10000\n",
      "Processing chunk 1101/10000\n",
      "Processing chunk 1151/10000\n",
      "Processing chunk 1201/10000\n",
      "Processing chunk 1251/10000\n",
      "Processing chunk 1301/10000\n",
      "Processing chunk 1351/10000\n",
      "Processing chunk 1401/10000\n",
      "Processing chunk 1451/10000\n",
      "Processing chunk 1501/10000\n",
      "Processing chunk 1551/10000\n",
      "Processing chunk 1601/10000\n",
      "Processing chunk 1651/10000\n",
      "Processing chunk 1701/10000\n",
      "Processing chunk 1751/10000\n",
      "Processing chunk 1801/10000\n",
      "Processing chunk 1851/10000\n",
      "Processing chunk 1901/10000\n",
      "Processing chunk 1951/10000\n",
      "Processing chunk 2001/10000\n",
      "Processing chunk 2051/10000\n",
      "Processing chunk 2101/10000\n",
      "Processing chunk 2151/10000\n",
      "Processing chunk 2201/10000\n",
      "Processing chunk 2251/10000\n",
      "Processing chunk 2301/10000\n",
      "Processing chunk 2351/10000\n",
      "Processing chunk 2401/10000\n",
      "Processing chunk 2451/10000\n",
      "Processing chunk 2501/10000\n",
      "Processing chunk 2551/10000\n",
      "Processing chunk 2601/10000\n",
      "Processing chunk 2651/10000\n",
      "Processing chunk 2701/10000\n",
      "Processing chunk 2751/10000\n",
      "Processing chunk 2801/10000\n",
      "Processing chunk 2851/10000\n",
      "Processing chunk 2901/10000\n",
      "Processing chunk 2951/10000\n",
      "Processing chunk 3001/10000\n",
      "Processing chunk 3051/10000\n",
      "Processing chunk 3101/10000\n",
      "Processing chunk 3151/10000\n",
      "Processing chunk 3201/10000\n",
      "Processing chunk 3251/10000\n",
      "Processing chunk 3301/10000\n",
      "Processing chunk 3351/10000\n",
      "Processing chunk 3401/10000\n",
      "Processing chunk 3451/10000\n",
      "Processing chunk 3501/10000\n",
      "Processing chunk 3551/10000\n",
      "Processing chunk 3601/10000\n",
      "Processing chunk 3651/10000\n",
      "Processing chunk 3701/10000\n",
      "Processing chunk 3751/10000\n",
      "Processing chunk 3801/10000\n",
      "Processing chunk 3851/10000\n",
      "Processing chunk 3901/10000\n",
      "Processing chunk 3951/10000\n",
      "Processing chunk 4001/10000\n",
      "Processing chunk 4051/10000\n",
      "Processing chunk 4101/10000\n",
      "Processing chunk 4151/10000\n",
      "Processing chunk 4201/10000\n",
      "Processing chunk 4251/10000\n",
      "Processing chunk 4301/10000\n",
      "Processing chunk 4351/10000\n",
      "Processing chunk 4401/10000\n",
      "Processing chunk 4451/10000\n",
      "Processing chunk 4501/10000\n",
      "Processing chunk 4551/10000\n",
      "Processing chunk 4601/10000\n",
      "Processing chunk 4651/10000\n",
      "Processing chunk 4701/10000\n",
      "Processing chunk 4751/10000\n",
      "Processing chunk 4801/10000\n",
      "Processing chunk 4851/10000\n",
      "Processing chunk 4901/10000\n",
      "Processing chunk 4951/10000\n",
      "Processing chunk 5001/10000\n",
      "Processing chunk 5051/10000\n",
      "Processing chunk 5101/10000\n",
      "Processing chunk 5151/10000\n",
      "Processing chunk 5201/10000\n",
      "Processing chunk 5251/10000\n",
      "Processing chunk 5301/10000\n",
      "Processing chunk 5351/10000\n",
      "Processing chunk 5401/10000\n",
      "Processing chunk 5451/10000\n",
      "Processing chunk 5501/10000\n",
      "Processing chunk 5551/10000\n",
      "Processing chunk 5601/10000\n",
      "Processing chunk 5651/10000\n",
      "Processing chunk 5701/10000\n",
      "Processing chunk 5751/10000\n",
      "Processing chunk 5801/10000\n",
      "Processing chunk 5851/10000\n",
      "Processing chunk 5901/10000\n",
      "Processing chunk 5951/10000\n",
      "Processing chunk 6001/10000\n",
      "Processing chunk 6051/10000\n",
      "Processing chunk 6101/10000\n",
      "Processing chunk 6151/10000\n",
      "Processing chunk 6201/10000\n",
      "Processing chunk 6251/10000\n",
      "Processing chunk 6301/10000\n",
      "Processing chunk 6351/10000\n",
      "Processing chunk 6401/10000\n",
      "Processing chunk 6451/10000\n",
      "Processing chunk 6501/10000\n",
      "Processing chunk 6551/10000\n",
      "Processing chunk 6601/10000\n",
      "Processing chunk 6651/10000\n",
      "Processing chunk 6701/10000\n",
      "Processing chunk 6751/10000\n",
      "Processing chunk 6801/10000\n",
      "Processing chunk 6851/10000\n",
      "Processing chunk 6901/10000\n",
      "Processing chunk 6951/10000\n",
      "Processing chunk 7001/10000\n",
      "Processing chunk 7051/10000\n",
      "Processing chunk 7101/10000\n",
      "Processing chunk 7151/10000\n",
      "Processing chunk 7201/10000\n",
      "Processing chunk 7251/10000\n",
      "Processing chunk 7301/10000\n",
      "Processing chunk 7351/10000\n",
      "Processing chunk 7401/10000\n",
      "Processing chunk 7451/10000\n",
      "Processing chunk 7501/10000\n",
      "Processing chunk 7551/10000\n",
      "Processing chunk 7601/10000\n",
      "Processing chunk 7651/10000\n",
      "Processing chunk 7701/10000\n",
      "Processing chunk 7751/10000\n",
      "Processing chunk 7801/10000\n",
      "Processing chunk 7851/10000\n",
      "Processing chunk 7901/10000\n",
      "Processing chunk 7951/10000\n",
      "Processing chunk 8001/10000\n",
      "Processing chunk 8051/10000\n",
      "Processing chunk 8101/10000\n",
      "Processing chunk 8151/10000\n",
      "Processing chunk 8201/10000\n",
      "Processing chunk 8251/10000\n",
      "Processing chunk 8301/10000\n",
      "Processing chunk 8351/10000\n",
      "Processing chunk 8401/10000\n",
      "Processing chunk 8451/10000\n",
      "Processing chunk 8501/10000\n",
      "Processing chunk 8551/10000\n",
      "Processing chunk 8601/10000\n",
      "Processing chunk 8651/10000\n",
      "Processing chunk 8701/10000\n",
      "Processing chunk 8751/10000\n",
      "Processing chunk 8801/10000\n",
      "Processing chunk 8851/10000\n",
      "Processing chunk 8901/10000\n",
      "Processing chunk 8951/10000\n",
      "Processing chunk 9001/10000\n",
      "Processing chunk 9051/10000\n",
      "Processing chunk 9101/10000\n",
      "Processing chunk 9151/10000\n",
      "Processing chunk 9201/10000\n",
      "Processing chunk 9251/10000\n",
      "Processing chunk 9301/10000\n",
      "Processing chunk 9351/10000\n",
      "Processing chunk 9401/10000\n",
      "Processing chunk 9451/10000\n",
      "Processing chunk 9501/10000\n",
      "Processing chunk 9551/10000\n",
      "Processing chunk 9601/10000\n",
      "Processing chunk 9651/10000\n",
      "Processing chunk 9701/10000\n",
      "Processing chunk 9751/10000\n",
      "Processing chunk 9801/10000\n",
      "Processing chunk 9851/10000\n",
      "Processing chunk 9901/10000\n",
      "Processing chunk 9951/10000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 10000\n",
      "Number of hits: 9998\n",
      "Average HR@10: 0.9998\n",
      "Average NDCG@10: 0.8973\n",
      "Evaluation completed in 105.90s\n",
      "HR@10: 0.9998\n",
      "NDCG@10: 0.8973\n"
     ]
    }
   ],
   "source": [
    "class GraphTransformerV2(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_feedforward, input_dim, num_weights=10, use_weights=True, dropout=0.1):\n",
    "        super(GraphTransformerV2, self).__init__()\n",
    "        self.num_weights = num_weights\n",
    "        self.use_weights = use_weights\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input projection layers\n",
    "        self.input_linear = Linear(input_dim, d_model)\n",
    "        self.dng_projection = Linear(input_dim, d_model)\n",
    "        \n",
    "        self.encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=d_feedforward, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Final output layers\n",
    "        self.pre_output = Linear(d_model, d_model)\n",
    "        self.output_linear = Linear(d_model, 1)\n",
    "        \n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.layer_norm = LayerNorm(d_model)\n",
    "        \n",
    "        if self.use_weights:\n",
    "            self.weight_linears = ModuleList([Linear(input_dim, d_model) for _ in range(num_weights)])\n",
    "\n",
    "    def compute_neighborhood_similarity(self, adjacency_matrix, x):\n",
    "        try:\n",
    "            binary_adj = (adjacency_matrix > 0).float()\n",
    "            intersection = binary_adj @ binary_adj.T\n",
    "            row_sums = binary_adj.sum(dim=1, keepdim=True)\n",
    "            col_sums = binary_adj.sum(dim=0, keepdim=True)\n",
    "            union = row_sums + col_sums.T - intersection\n",
    "            similarity = intersection / (union + 1e-8)\n",
    "            return similarity @ x\n",
    "        except RuntimeError:\n",
    "            return torch.zeros_like(x)\n",
    "\n",
    "    def project_graph_metrics(self, graph_metrics, target_dim):\n",
    "        if graph_metrics.size(1) < target_dim:\n",
    "            repeats = (target_dim + graph_metrics.size(1) - 1) // graph_metrics.size(1)\n",
    "            graph_metrics = graph_metrics.repeat(1, repeats)[:, :target_dim]\n",
    "        elif graph_metrics.size(1) > target_dim:\n",
    "            graph_metrics = graph_metrics[:, :target_dim]\n",
    "        return graph_metrics\n",
    "\n",
    "    def forward(self, x, adjacency_matrix, graph_metrics, weights=None):\n",
    "        adjacency_matrix = adjacency_matrix.float()\n",
    "        graph_metrics = graph_metrics.float()\n",
    "        batch_size, input_dim = x.shape\n",
    "        \n",
    "        if adjacency_matrix.size(0) != batch_size or adjacency_matrix.size(1) != batch_size:\n",
    "            adjacency_matrix = torch.eye(batch_size, device=x.device)\n",
    "\n",
    "        try:\n",
    "            # Direct connections\n",
    "            direct_scores = adjacency_matrix @ x\n",
    "            \n",
    "            # Neighborhood similarity\n",
    "            neighborhood_similarity = self.compute_neighborhood_similarity(adjacency_matrix, x)\n",
    "            \n",
    "            # Graph structure scores\n",
    "            if graph_metrics.dim() == 2:\n",
    "                graph_metrics_projected = self.project_graph_metrics(graph_metrics, input_dim)\n",
    "                graph_structure_scores = graph_metrics_projected * x\n",
    "            else:\n",
    "                graph_structure_scores = torch.zeros_like(x)\n",
    "\n",
    "            # Combine DNG scores and project to d_model dimension\n",
    "            dng_scores = direct_scores + neighborhood_similarity + graph_structure_scores\n",
    "            dng_scores = self.dng_projection(dng_scores)  # Project to d_model dimension\n",
    "            \n",
    "            # Process input through transformer\n",
    "            if self.use_weights and weights is not None:\n",
    "                weighted_x = torch.zeros_like(x)\n",
    "                for i, weight in enumerate(weights.T):\n",
    "                    weighted_x += self.weight_linears[i](x) * weight.unsqueeze(1)\n",
    "                transformer_input = weighted_x\n",
    "            else:\n",
    "                transformer_input = self.input_linear(x)  # Project to d_model dimension\n",
    "\n",
    "            # Apply transformer\n",
    "            transformer_input = self.layer_norm(transformer_input)\n",
    "            transformer_output = self.transformer_encoder(transformer_input.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Combine transformer output with DNG scores\n",
    "            combined = transformer_output + dng_scores\n",
    "            combined = self.dropout(combined)\n",
    "            \n",
    "            # Final output processing\n",
    "            output = self.pre_output(combined)\n",
    "            output = F.relu(output)\n",
    "            output = self.output_linear(output)\n",
    "            \n",
    "            return output.squeeze(-1)  # Return [batch_size] tensor\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError during forward pass: {e}\")\n",
    "            print(f\"x shape: {x.shape}, adjacency_matrix shape: {adjacency_matrix.shape}, graph_metrics shape: {graph_metrics.shape}\")\n",
    "            raise\n",
    "\n",
    "# cropped dataset\n",
    "class MultiBehaviorDataset:\n",
    "    def __init__(self, data_path='/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/', max_users=10000):\n",
    "        self.data_path = data_path\n",
    "        self.behaviors = ['pv', 'cart', 'buy']\n",
    "        self.trn_file = data_path + 'trn_'\n",
    "        self.tst_file = data_path + 'tst_'\n",
    "        self.max_users = max_users  # Limit number of users\n",
    "        \n",
    "        self.load_training_data()\n",
    "        self.load_test_data()\n",
    "    \n",
    "    def load_training_data(self):\n",
    "        print(\"Loading training data from:\", self.data_path)\n",
    "        self.trn_mats = []\n",
    "        for beh in self.behaviors:\n",
    "            path = self.trn_file + beh\n",
    "            print(f\"Loading behavior: {beh} from {path}\")\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"File not found: {path}\")\n",
    "            with open(path, 'rb') as fs:\n",
    "                mat = pickle.load(fs)\n",
    "                if not isinstance(mat, csr_matrix):\n",
    "                    mat = csr_matrix(mat)\n",
    "                mat = (mat != 0).astype(np.float32)\n",
    "                self.trn_mats.append(mat)\n",
    "        \n",
    "        self.trn_label = 1 * (self.trn_mats[-1] != 0)\n",
    "        self.n_users, self.n_items = self.trn_mats[0].shape\n",
    "        self.n_behaviors = len(self.behaviors)\n",
    "        print(f\"Dataset dimensions: {self.n_users} users, {self.n_items} items\")\n",
    "\n",
    "    def create_adjacency_matrix(self, users):\n",
    "        \"\"\"Optimized adjacency matrix creation\"\"\"\n",
    "        batch_size = len(users)\n",
    "        adj_matrix = torch.zeros(batch_size, batch_size, device=device)\n",
    "        \n",
    "        # Pre-compute user item sets\n",
    "        user_item_sets = []\n",
    "        for user in users:\n",
    "            user_items = set()\n",
    "            for mat in self.trn_mats:\n",
    "                user_items.update(mat[user].indices)\n",
    "            user_item_sets.append(user_items)\n",
    "        \n",
    "        # Compute similarities in parallel\n",
    "        for i in range(batch_size):\n",
    "            if not user_item_sets[i]:\n",
    "                continue\n",
    "            for j in range(i+1, batch_size):\n",
    "                if user_item_sets[j]:\n",
    "                    jaccard = len(user_item_sets[i] & user_item_sets[j]) / len(user_item_sets[i] | user_item_sets[j])\n",
    "                    adj_matrix[i,j] = adj_matrix[j,i] = jaccard\n",
    "        \n",
    "        return adj_matrix\n",
    "\n",
    "    def create_graph_metrics(self, users):\n",
    "        \"\"\"Optimized graph metrics creation\"\"\"\n",
    "        metrics = torch.zeros(len(users), 3, device=device)\n",
    "        \n",
    "        # Vectorized computation\n",
    "        for i, user in enumerate(users):\n",
    "            interactions = np.array([mat[user].nnz for mat in self.trn_mats])\n",
    "            metrics[i,0] = sum(interactions) / 100\n",
    "            metrics[i,1] = (interactions > 0).sum() / len(self.behaviors)\n",
    "            metrics[i,2] = interactions[-1] / max(interactions[0], 1)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def prepare_train_instances(self, max_samples=5000):\n",
    "        \"\"\"Optimized training instance preparation\"\"\"\n",
    "        print(\"Preparing training instances...\")\n",
    "        train_data = []\n",
    "        \n",
    "        # Randomly select users\n",
    "        selected_users = np.random.choice(min(self.n_users, self.max_users), size=min(1000, self.max_users), replace=False)\n",
    "        \n",
    "        for user in selected_users:\n",
    "            pos_items = self.trn_label[user].indices[:2]  # Limit positive items\n",
    "            \n",
    "            if len(pos_items) > 0:\n",
    "                for item in pos_items:\n",
    "                    behaviors = [float(mat[user, item]) for mat in self.trn_mats]\n",
    "                    train_data.append([user, item, 1.0] + behaviors)\n",
    "                    \n",
    "                    # Limited negative sampling\n",
    "                    neg_items = np.random.choice(self.n_items, size=2, replace=False)\n",
    "                    for neg_item in neg_items:\n",
    "                        behaviors = [float(mat[user, neg_item]) for mat in self.trn_mats]\n",
    "                        train_data.append([user, neg_item, 0.0] + behaviors)\n",
    "                        \n",
    "                    if len(train_data) >= max_samples:\n",
    "                        break\n",
    "            \n",
    "            if len(train_data) >= max_samples:\n",
    "                break\n",
    "        \n",
    "        print(f\"Generated {len(train_data)} training instances\")\n",
    "        return np.array(train_data)\n",
    "    def load_test_data(self):\n",
    "        \"\"\"Load test data\"\"\"\n",
    "        test_path = self.tst_file + 'int'\n",
    "        print(f\"Loading test data from: {test_path}\")\n",
    "        if not os.path.exists(test_path):\n",
    "            raise FileNotFoundError(f\"File not found: {test_path}\")\n",
    "        with open(test_path, 'rb') as fs:\n",
    "            self.tst_int = np.array(pickle.load(fs))\n",
    "        \n",
    "        self.tst_users = np.reshape(np.argwhere(self.tst_int != None), [-1])\n",
    "        # Limit test users for faster processing\n",
    "        self.tst_users = self.tst_users[:min(len(self.tst_users), self.max_users)]\n",
    "        print(f\"Using {len(self.tst_users)} test users\")\n",
    "\n",
    "    def get_test_instances(self, num_neg_samples=99):\n",
    "        \"\"\"Generate test instances with negative sampling\"\"\"\n",
    "        print(\"Preparing test instances...\")\n",
    "        test_instances = []\n",
    "        \n",
    "        for user in self.tst_users:\n",
    "            pos_item = self.tst_int[user]\n",
    "            if pos_item is not None:\n",
    "                # Add positive instance\n",
    "                test_instances.append([user, pos_item, 1.0])\n",
    "                \n",
    "                # Add negative instances\n",
    "                try:\n",
    "                    # Get all items and remove positive items\n",
    "                    all_items = set(range(self.n_items))\n",
    "                    pos_items_train = set(self.trn_label[user].indices)\n",
    "                    pos_items_train.add(pos_item)\n",
    "                    neg_items_pool = list(all_items - pos_items_train)\n",
    "                    \n",
    "                    # Sample negative items\n",
    "                    n_neg = min(num_neg_samples, len(neg_items_pool))\n",
    "                    if n_neg > 0:\n",
    "                        neg_items = np.random.choice(neg_items_pool, size=n_neg, replace=False)\n",
    "                        for neg_item in neg_items:\n",
    "                            test_instances.append([user, neg_item, 0.0])\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing test user {user}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Limit the number of test instances for faster processing\n",
    "                if len(test_instances) >= self.max_users * 100:\n",
    "                    break\n",
    "        \n",
    "        if not test_instances:\n",
    "            raise ValueError(\"No test instances were generated!\")\n",
    "            \n",
    "        test_instances = np.array(test_instances)\n",
    "        print(f\"Generated {len(test_instances)} test instances\")\n",
    "        return test_instances\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset, test_instances, k=10):\n",
    "    \"\"\"Improved evaluation function with better metrics calculation\"\"\"\n",
    "    model.eval()\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    # Process test instances in smaller chunks\n",
    "    chunk_size = 100\n",
    "    test_chunks = [test_instances[i:i + chunk_size] for i in range(0, len(test_instances), chunk_size)]\n",
    "    \n",
    "    print(f\"Evaluating {len(test_instances)} instances in {len(test_chunks)} chunks...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for chunk_idx, chunk in enumerate(test_chunks):\n",
    "            if chunk_idx % 50 == 0:  # Reduced frequency of progress updates\n",
    "                print(f\"Processing chunk {chunk_idx + 1}/{len(test_chunks)}\")\n",
    "            \n",
    "            # Group by user and ensure we have both positive and negative items\n",
    "            user_items = {}\n",
    "            for inst in chunk:\n",
    "                user = int(inst[0])\n",
    "                item = int(inst[1])\n",
    "                label = float(inst[2])\n",
    "                \n",
    "                if user not in user_items:\n",
    "                    user_items[user] = {'pos': [], 'neg': [], 'all_items': [], 'all_scores': []}\n",
    "                \n",
    "                user_items[user]['all_items'].append(item)\n",
    "                if label > 0.5:\n",
    "                    user_items[user]['pos'].append(item)\n",
    "                else:\n",
    "                    user_items[user]['neg'].append(item)\n",
    "            \n",
    "            # Process each user that has both positive and negative items\n",
    "            for user, data in user_items.items():\n",
    "                if not data['pos'] or not data['neg']:\n",
    "                    continue\n",
    "                \n",
    "                items = data['all_items']\n",
    "                batch_size = len(items)\n",
    "                \n",
    "                # Create input features\n",
    "                behaviors = torch.zeros(batch_size, dataset.n_behaviors, device=device)\n",
    "                for i, item in enumerate(items):\n",
    "                    for j, mat in enumerate(dataset.trn_mats):\n",
    "                        behaviors[i, j] = float(mat[user, item])\n",
    "                \n",
    "                x = torch.cat([\n",
    "                    behaviors,\n",
    "                    torch.zeros(batch_size, model.input_dim - behaviors.size(1), device=device)\n",
    "                ], dim=1)\n",
    "                \n",
    "                # Simplified adjacency matrix for evaluation\n",
    "                adj_matrix = torch.eye(batch_size, device=device)\n",
    "                graph_metrics = dataset.create_graph_metrics([user] * batch_size)\n",
    "                \n",
    "                try:\n",
    "                    predictions = model(x, adj_matrix, graph_metrics)\n",
    "                    predictions = predictions.cpu().numpy().flatten()\n",
    "                    \n",
    "                    # Store all scores\n",
    "                    for item, score in zip(items, predictions):\n",
    "                        data['all_scores'].append((item, score))\n",
    "                    \n",
    "                    # Sort items by score\n",
    "                    sorted_items = [x[0] for x in sorted(data['all_scores'], key=lambda x: x[1], reverse=True)]\n",
    "                    recommended_items = sorted_items[:k]\n",
    "                    \n",
    "                    # Calculate HR\n",
    "                    hit = False\n",
    "                    for pos_item in data['pos']:\n",
    "                        if pos_item in recommended_items:\n",
    "                            hit = True\n",
    "                            break\n",
    "                    hits.append(hit)\n",
    "                    \n",
    "                    # Calculate NDCG\n",
    "                    dcg = 0\n",
    "                    idcg = 1  # Ideal DCG for one relevant item\n",
    "                    for i, item in enumerate(recommended_items):\n",
    "                        if item in data['pos']:\n",
    "                            dcg += 1 / np.log2(i + 2)\n",
    "                    ndcg = dcg / idcg\n",
    "                    ndcgs.append(ndcg)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating user {user}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    hr = np.mean(hits) if hits else 0\n",
    "    ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(f\"\\nEvaluation Statistics:\")\n",
    "    print(f\"Total users evaluated: {len(hits)}\")\n",
    "    print(f\"Number of hits: {sum(hits)}\")\n",
    "    print(f\"Average HR@{k}: {hr:.4f}\")\n",
    "    print(f\"Average NDCG@{k}: {ndcg:.4f}\")\n",
    "    \n",
    "    return hr, ndcg\n",
    "\n",
    "def get_test_instances(self, num_neg_samples=99):\n",
    "    \"\"\"Improved test instance generation\"\"\"\n",
    "    print(\"Preparing test instances...\")\n",
    "    test_instances = []\n",
    "    max_test_users = 1000  # Limit number of test users\n",
    "    \n",
    "    # Randomly sample test users if there are too many\n",
    "    test_users = self.tst_users\n",
    "    if len(test_users) > max_test_users:\n",
    "        test_users = np.random.choice(test_users, max_test_users, replace=False)\n",
    "    \n",
    "    for user in test_users:\n",
    "        user = int(user)\n",
    "        pos_item = self.tst_int[user]\n",
    "        if pos_item is not None:\n",
    "            pos_item = int(pos_item)\n",
    "            test_instances.append([user, pos_item, 1.0])\n",
    "            \n",
    "            try:\n",
    "                # Get negative items\n",
    "                all_items = set(range(self.n_items))\n",
    "                pos_items = set(int(x) for x in self.trn_label[user].indices)\n",
    "                pos_items.add(pos_item)\n",
    "                neg_items_pool = list(all_items - pos_items)\n",
    "                \n",
    "                # Sample negative items\n",
    "                n_neg = min(num_neg_samples, len(neg_items_pool))\n",
    "                if n_neg > 0:\n",
    "                    neg_items = np.random.choice(neg_items_pool, size=n_neg, replace=False)\n",
    "                    for neg_item in neg_items:\n",
    "                        test_instances.append([user, int(neg_item), 0.0])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing test user {user}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    if not test_instances:\n",
    "        raise ValueError(\"No test instances were generated!\")\n",
    "    \n",
    "    test_instances = np.array(test_instances)\n",
    "    print(f\"Generated {len(test_instances)} test instances\")\n",
    "    print(f\"Number of unique users: {len(set(test_instances[:,0]))}\")\n",
    "    return test_instances\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'd_model': 32,\n",
    "        'num_heads': 2,\n",
    "        'num_layers': 1,\n",
    "        'd_feedforward': 64,\n",
    "        'input_dim': 64,\n",
    "        'num_weights': 3,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-6,\n",
    "        'dropout': 0.1,\n",
    "        'gradient_clip': 1.0,\n",
    "        'max_samples': 500000,\n",
    "        'eval_k': 10\n",
    "    }\n",
    "    \n",
    "    print(\"Initializing dataset...\")\n",
    "    dataset = MultiBehaviorDataset(max_users=10000)\n",
    "    train_data = dataset.prepare_train_instances(max_samples=config['max_samples'])\n",
    "    \n",
    "    print(f\"Using {len(train_data)} training instances\")\n",
    "\n",
    "    model = GraphTransformerV2(\n",
    "        num_layers=config['num_layers'],\n",
    "        d_model=config['d_model'],\n",
    "        num_heads=config['num_heads'],\n",
    "        d_feedforward=config['d_feedforward'],\n",
    "        input_dim=config['input_dim'],\n",
    "        num_weights=config['num_weights'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    print(\"Preparing training tensors...\")\n",
    "    users = torch.LongTensor(train_data[:, 0]).to(device)\n",
    "    items = torch.LongTensor(train_data[:, 1]).to(device)\n",
    "    labels = torch.FloatTensor(train_data[:, 2]).to(device)\n",
    "    behaviors = torch.FloatTensor(train_data[:, 3:]).to(device)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        model.train()\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Training\n",
    "        x = torch.cat([\n",
    "            behaviors,\n",
    "            torch.zeros(len(users), config['input_dim'] - behaviors.size(1), device=device)\n",
    "        ], dim=1)\n",
    "        \n",
    "        adj_matrix = dataset.create_adjacency_matrix(users)\n",
    "        graph_metrics = dataset.create_graph_metrics(users)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x, adj_matrix, graph_metrics)\n",
    "        predictions = predictions.view(-1)\n",
    "        labels = labels.view(-1)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip'])\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"Training completed in {training_time:.2f}s\")\n",
    "        print(f\"Training loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        print(\"\\nStarting evaluation...\")\n",
    "        eval_start_time = datetime.now()\n",
    "        \n",
    "        test_instances = dataset.get_test_instances()\n",
    "        hr, ndcg = evaluate_model(model, dataset, test_instances, k=config['eval_k'])\n",
    "        \n",
    "        eval_time = (datetime.now() - eval_start_time).total_seconds()\n",
    "        print(f\"Evaluation completed in {eval_time:.2f}s\")\n",
    "        print(f\"HR@{config['eval_k']}: {hr:.4f}\")\n",
    "        print(f\"NDCG@{config['eval_k']}: {ndcg:.4f}\")\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'training_time': training_time,\n",
    "            'eval_time': eval_time,\n",
    "            'training_loss': loss.item(),\n",
    "            'hr': hr,\n",
    "            'ndcg': ndcg\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        with open(f'results_{timestamp}.txt', 'w') as f:\n",
    "            for key, value in results.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoRec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "Loading training data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/\n",
      "Loading behavior: pv from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_pv\n",
      "Loading behavior: cart from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_cart\n",
      "Loading behavior: buy from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_buy\n",
      "Dataset dimensions: 21716 users, 7977 items\n",
      "Loading test data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/tst_int\n",
      "Using 10000 test users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/20z1hn994jd04q4kl0gpgh740000gn/T/ipykernel_37763/2348124140.py:22: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  mat = pickle.load(fs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction matrix shape: (21716, 7977)\n",
      "Non-zero elements: 282860\n",
      "Average interactions per user: 13.03\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1, Batch 0\n",
      "Loss: 0.6935\n",
      "Max prediction: 0.5299\n",
      "Min prediction: 0.4656\n",
      "Num positive targets: 346\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 1, Batch 10\n",
      "Loss: 0.6460\n",
      "Max prediction: 0.5256\n",
      "Min prediction: 0.3673\n",
      "Num positive targets: 400\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 1, Batch 20\n",
      "Loss: 0.4938\n",
      "Max prediction: 0.4960\n",
      "Min prediction: 0.0443\n",
      "Num positive targets: 414\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 1, Batch 30\n",
      "Loss: 0.2724\n",
      "Max prediction: 0.4318\n",
      "Min prediction: 0.0006\n",
      "Num positive targets: 444\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 1 completed. Average loss: 0.5399\n",
      "\n",
      "Epoch 2, Batch 0\n",
      "Loss: 0.2507\n",
      "Max prediction: 0.4218\n",
      "Min prediction: 0.0031\n",
      "Num positive targets: 390\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 2, Batch 10\n",
      "Loss: 0.1366\n",
      "Max prediction: 0.3505\n",
      "Min prediction: 0.0001\n",
      "Num positive targets: 394\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 2, Batch 20\n",
      "Loss: 0.0930\n",
      "Max prediction: 0.3631\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 420\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 2, Batch 30\n",
      "Loss: 0.0586\n",
      "Max prediction: 0.3674\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 377\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 2 completed. Average loss: 0.1132\n",
      "\n",
      "Epoch 3, Batch 0\n",
      "Loss: 0.0322\n",
      "Max prediction: 0.3128\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 415\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 3, Batch 10\n",
      "Loss: 0.0293\n",
      "Max prediction: 0.3396\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 366\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 3, Batch 20\n",
      "Loss: 0.0345\n",
      "Max prediction: 0.3530\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 363\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 3, Batch 30\n",
      "Loss: 0.0267\n",
      "Max prediction: 0.3475\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 485\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 3 completed. Average loss: 0.0315\n",
      "\n",
      "Epoch 4, Batch 0\n",
      "Loss: 0.0217\n",
      "Max prediction: 0.3618\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 390\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 4, Batch 10\n",
      "Loss: 0.0241\n",
      "Max prediction: 0.3434\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 423\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 4, Batch 20\n",
      "Loss: 0.0152\n",
      "Max prediction: 0.3282\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 399\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 4, Batch 30\n",
      "Loss: 0.0200\n",
      "Max prediction: 0.3627\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 323\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 4 completed. Average loss: 0.0202\n",
      "\n",
      "Epoch 5, Batch 0\n",
      "Loss: 0.0173\n",
      "Max prediction: 0.3659\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 369\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 5, Batch 10\n",
      "Loss: 0.0185\n",
      "Max prediction: 0.3352\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 373\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 5, Batch 20\n",
      "Loss: 0.0150\n",
      "Max prediction: 0.2937\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 398\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 5, Batch 30\n",
      "Loss: 0.0171\n",
      "Max prediction: 0.3146\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 421\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 5 completed. Average loss: 0.0165\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 20000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 20000 instances in 200 chunks...\n",
      "Processing chunk 1/200\n",
      "Processing chunk 51/200\n",
      "Processing chunk 101/200\n",
      "Processing chunk 151/200\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 812\n",
      "Average HR@10: 0.8120\n",
      "Average NDCG@10: 0.5818\n",
      "HR@10: 0.8120\n",
      "NDCG@10: 0.5818\n",
      "\n",
      "Epoch 6, Batch 0\n",
      "Loss: 0.0145\n",
      "Max prediction: 0.2614\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 491\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 6, Batch 10\n",
      "Loss: 0.0149\n",
      "Max prediction: 0.3567\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 352\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 6, Batch 20\n",
      "Loss: 0.0132\n",
      "Max prediction: 0.3086\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 369\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 6, Batch 30\n",
      "Loss: 0.0147\n",
      "Max prediction: 0.2944\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 433\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 6 completed. Average loss: 0.0149\n",
      "\n",
      "Epoch 7, Batch 0\n",
      "Loss: 0.0135\n",
      "Max prediction: 0.3043\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 377\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 7, Batch 10\n",
      "Loss: 0.0185\n",
      "Max prediction: 0.2894\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 492\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 7, Batch 20\n",
      "Loss: 0.0143\n",
      "Max prediction: 0.3198\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 384\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 7, Batch 30\n",
      "Loss: 0.0124\n",
      "Max prediction: 0.3067\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 402\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 7 completed. Average loss: 0.0139\n",
      "\n",
      "Epoch 8, Batch 0\n",
      "Loss: 0.0144\n",
      "Max prediction: 0.2795\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 456\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 8, Batch 10\n",
      "Loss: 0.0116\n",
      "Max prediction: 0.3160\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 371\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 8, Batch 20\n",
      "Loss: 0.0134\n",
      "Max prediction: 0.3307\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 362\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 8, Batch 30\n",
      "Loss: 0.0132\n",
      "Max prediction: 0.3280\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 385\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 8 completed. Average loss: 0.0132\n",
      "\n",
      "Epoch 9, Batch 0\n",
      "Loss: 0.0129\n",
      "Max prediction: 0.3039\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 403\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 9, Batch 10\n",
      "Loss: 0.0142\n",
      "Max prediction: 0.2769\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 447\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 9, Batch 20\n",
      "Loss: 0.0135\n",
      "Max prediction: 0.3265\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 415\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 9, Batch 30\n",
      "Loss: 0.0144\n",
      "Max prediction: 0.2948\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 424\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 9 completed. Average loss: 0.0128\n",
      "\n",
      "Epoch 10, Batch 0\n",
      "Loss: 0.0119\n",
      "Max prediction: 0.3051\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 412\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 10, Batch 10\n",
      "Loss: 0.0119\n",
      "Max prediction: 0.3123\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 381\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 10, Batch 20\n",
      "Loss: 0.0137\n",
      "Max prediction: 0.2782\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 482\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 10, Batch 30\n",
      "Loss: 0.0134\n",
      "Max prediction: 0.2984\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 419\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 10 completed. Average loss: 0.0124\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 20000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 20000 instances in 200 chunks...\n",
      "Processing chunk 1/200\n",
      "Processing chunk 51/200\n",
      "Processing chunk 101/200\n",
      "Processing chunk 151/200\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 788\n",
      "Average HR@10: 0.7880\n",
      "Average NDCG@10: 0.5754\n",
      "HR@10: 0.7880\n",
      "NDCG@10: 0.5754\n",
      "\n",
      "Epoch 11, Batch 0\n",
      "Loss: 0.0138\n",
      "Max prediction: 0.3139\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 456\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 11, Batch 10\n",
      "Loss: 0.0114\n",
      "Max prediction: 0.2353\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 425\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 11, Batch 20\n",
      "Loss: 0.0119\n",
      "Max prediction: 0.2836\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 426\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 11, Batch 30\n",
      "Loss: 0.0113\n",
      "Max prediction: 0.3115\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 399\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 11 completed. Average loss: 0.0120\n",
      "\n",
      "Epoch 12, Batch 0\n",
      "Loss: 0.0109\n",
      "Max prediction: 0.2764\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 379\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 12, Batch 10\n",
      "Loss: 0.0122\n",
      "Max prediction: 0.2630\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 424\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 12, Batch 20\n",
      "Loss: 0.0113\n",
      "Max prediction: 0.2995\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 401\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 12, Batch 30\n",
      "Loss: 0.0133\n",
      "Max prediction: 0.2448\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 449\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 12 completed. Average loss: 0.0118\n",
      "\n",
      "Epoch 13, Batch 0\n",
      "Loss: 0.0102\n",
      "Max prediction: 0.2739\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 398\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 13, Batch 10\n",
      "Loss: 0.0113\n",
      "Max prediction: 0.2611\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 425\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 13, Batch 20\n",
      "Loss: 0.0104\n",
      "Max prediction: 0.2738\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 383\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 13, Batch 30\n",
      "Loss: 0.0116\n",
      "Max prediction: 0.2497\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 419\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 13 completed. Average loss: 0.0116\n",
      "\n",
      "Epoch 14, Batch 0\n",
      "Loss: 0.0121\n",
      "Max prediction: 0.2797\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 407\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 14, Batch 10\n",
      "Loss: 0.0112\n",
      "Max prediction: 0.3103\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 405\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 14, Batch 20\n",
      "Loss: 0.0089\n",
      "Max prediction: 0.2602\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 367\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 14, Batch 30\n",
      "Loss: 0.0132\n",
      "Max prediction: 0.2501\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 442\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 14 completed. Average loss: 0.0112\n",
      "\n",
      "Epoch 15, Batch 0\n",
      "Loss: 0.0105\n",
      "Max prediction: 0.2799\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 370\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 15, Batch 10\n",
      "Loss: 0.0105\n",
      "Max prediction: 0.2461\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 415\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 15, Batch 20\n",
      "Loss: 0.0132\n",
      "Max prediction: 0.3860\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 493\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 15, Batch 30\n",
      "Loss: 0.0114\n",
      "Max prediction: 0.2346\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 415\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 15 completed. Average loss: 0.0109\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 20000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 20000 instances in 200 chunks...\n",
      "Processing chunk 1/200\n",
      "Processing chunk 51/200\n",
      "Processing chunk 101/200\n",
      "Processing chunk 151/200\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 799\n",
      "Average HR@10: 0.7990\n",
      "Average NDCG@10: 0.5897\n",
      "HR@10: 0.7990\n",
      "NDCG@10: 0.5897\n",
      "\n",
      "Epoch 16, Batch 0\n",
      "Loss: 0.0108\n",
      "Max prediction: 0.2534\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 431\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 16, Batch 10\n",
      "Loss: 0.0109\n",
      "Max prediction: 0.2460\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 384\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 16, Batch 20\n",
      "Loss: 0.0123\n",
      "Max prediction: 0.2759\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 448\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 16, Batch 30\n",
      "Loss: 0.0096\n",
      "Max prediction: 0.2723\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 359\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 16 completed. Average loss: 0.0107\n",
      "\n",
      "Epoch 17, Batch 0\n",
      "Loss: 0.0118\n",
      "Max prediction: 0.2803\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 423\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 17, Batch 10\n",
      "Loss: 0.0100\n",
      "Max prediction: 0.2311\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 403\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 17, Batch 20\n",
      "Loss: 0.0126\n",
      "Max prediction: 0.2926\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 469\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 17, Batch 30\n",
      "Loss: 0.0099\n",
      "Max prediction: 0.5421\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 424\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 17 completed. Average loss: 0.0105\n",
      "\n",
      "Epoch 18, Batch 0\n",
      "Loss: 0.0105\n",
      "Max prediction: 0.3227\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 447\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 18, Batch 10\n",
      "Loss: 0.0107\n",
      "Max prediction: 0.3093\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 441\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 18, Batch 20\n",
      "Loss: 0.0112\n",
      "Max prediction: 0.3781\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 447\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 18, Batch 30\n",
      "Loss: 0.0100\n",
      "Max prediction: 0.3611\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 394\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 18 completed. Average loss: 0.0103\n",
      "\n",
      "Epoch 19, Batch 0\n",
      "Loss: 0.0105\n",
      "Max prediction: 0.1928\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 397\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 19, Batch 10\n",
      "Loss: 0.0103\n",
      "Max prediction: 0.5348\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 409\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 19, Batch 20\n",
      "Loss: 0.0092\n",
      "Max prediction: 0.6482\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 391\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 19, Batch 30\n",
      "Loss: 0.0098\n",
      "Max prediction: 0.2424\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 331\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 19 completed. Average loss: 0.0102\n",
      "\n",
      "Epoch 20, Batch 0\n",
      "Loss: 0.0098\n",
      "Max prediction: 0.5898\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 421\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 20, Batch 10\n",
      "Loss: 0.0103\n",
      "Max prediction: 0.2347\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 409\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 20, Batch 20\n",
      "Loss: 0.0105\n",
      "Max prediction: 0.2030\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 449\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 20, Batch 30\n",
      "Loss: 0.0100\n",
      "Max prediction: 0.2795\n",
      "Min prediction: 0.0000\n",
      "Num positive targets: 392\n",
      "Batch shape: torch.Size([32, 7977])\n",
      "\n",
      "Epoch 20 completed. Average loss: 0.0100\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 20000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 20000 instances in 200 chunks...\n",
      "Processing chunk 1/200\n",
      "Processing chunk 51/200\n",
      "Processing chunk 101/200\n",
      "Processing chunk 151/200\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 799\n",
      "Average HR@10: 0.7990\n",
      "Average NDCG@10: 0.5630\n",
      "HR@10: 0.7990\n",
      "NDCG@10: 0.5630\n"
     ]
    }
   ],
   "source": [
    "# cropped dataset\n",
    "class MultiBehaviorDataset:\n",
    "    def __init__(self, data_path='/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/', max_users=10000):\n",
    "        self.data_path = data_path\n",
    "        self.behaviors = ['pv', 'cart', 'buy']\n",
    "        self.trn_file = data_path + 'trn_'\n",
    "        self.tst_file = data_path + 'tst_'\n",
    "        self.max_users = max_users  # Limit number of users\n",
    "        \n",
    "        self.load_training_data()\n",
    "        self.load_test_data()\n",
    "    \n",
    "    def load_training_data(self):\n",
    "        print(\"Loading training data from:\", self.data_path)\n",
    "        self.trn_mats = []\n",
    "        for beh in self.behaviors:\n",
    "            path = self.trn_file + beh\n",
    "            print(f\"Loading behavior: {beh} from {path}\")\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"File not found: {path}\")\n",
    "            with open(path, 'rb') as fs:\n",
    "                mat = pickle.load(fs)\n",
    "                if not isinstance(mat, csr_matrix):\n",
    "                    mat = csr_matrix(mat)\n",
    "                mat = (mat != 0).astype(np.float32)\n",
    "                self.trn_mats.append(mat)\n",
    "        \n",
    "        self.trn_label = 1 * (self.trn_mats[-1] != 0)\n",
    "        self.n_users, self.n_items = self.trn_mats[0].shape\n",
    "        self.n_behaviors = len(self.behaviors)\n",
    "        print(f\"Dataset dimensions: {self.n_users} users, {self.n_items} items\")\n",
    "\n",
    "    def create_adjacency_matrix(self, users):\n",
    "        \"\"\"Optimized adjacency matrix creation\"\"\"\n",
    "        batch_size = len(users)\n",
    "        adj_matrix = torch.zeros(batch_size, batch_size, device=device)\n",
    "        \n",
    "        # Pre-compute user item sets\n",
    "        user_item_sets = []\n",
    "        for user in users:\n",
    "            user_items = set()\n",
    "            for mat in self.trn_mats:\n",
    "                user_items.update(mat[user].indices)\n",
    "            user_item_sets.append(user_items)\n",
    "        \n",
    "        # Compute similarities in parallel\n",
    "        for i in range(batch_size):\n",
    "            if not user_item_sets[i]:\n",
    "                continue\n",
    "            for j in range(i+1, batch_size):\n",
    "                if user_item_sets[j]:\n",
    "                    jaccard = len(user_item_sets[i] & user_item_sets[j]) / len(user_item_sets[i] | user_item_sets[j])\n",
    "                    adj_matrix[i,j] = adj_matrix[j,i] = jaccard\n",
    "        \n",
    "        return adj_matrix\n",
    "\n",
    "    def create_graph_metrics(self, users):\n",
    "        \"\"\"Optimized graph metrics creation\"\"\"\n",
    "        metrics = torch.zeros(len(users), 3, device=device)\n",
    "        \n",
    "        # Vectorized computation\n",
    "        for i, user in enumerate(users):\n",
    "            interactions = np.array([mat[user].nnz for mat in self.trn_mats])\n",
    "            metrics[i,0] = sum(interactions) / 100\n",
    "            metrics[i,1] = (interactions > 0).sum() / len(self.behaviors)\n",
    "            metrics[i,2] = interactions[-1] / max(interactions[0], 1)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def prepare_train_instances(self, max_samples=5000):\n",
    "        \"\"\"Optimized training instance preparation\"\"\"\n",
    "        print(\"Preparing training instances...\")\n",
    "        train_data = []\n",
    "        \n",
    "        # Randomly select users\n",
    "        selected_users = np.random.choice(min(self.n_users, self.max_users), size=min(1000, self.max_users), replace=False)\n",
    "        \n",
    "        for user in selected_users:\n",
    "            pos_items = self.trn_label[user].indices[:2]  # Limit positive items\n",
    "            \n",
    "            if len(pos_items) > 0:\n",
    "                for item in pos_items:\n",
    "                    behaviors = [float(mat[user, item]) for mat in self.trn_mats]\n",
    "                    train_data.append([user, item, 1.0] + behaviors)\n",
    "                    \n",
    "                    # Limited negative sampling\n",
    "                    neg_items = np.random.choice(self.n_items, size=2, replace=False)\n",
    "                    for neg_item in neg_items:\n",
    "                        behaviors = [float(mat[user, neg_item]) for mat in self.trn_mats]\n",
    "                        train_data.append([user, neg_item, 0.0] + behaviors)\n",
    "                        \n",
    "                    if len(train_data) >= max_samples:\n",
    "                        break\n",
    "            \n",
    "            if len(train_data) >= max_samples:\n",
    "                break\n",
    "        \n",
    "        print(f\"Generated {len(train_data)} training instances\")\n",
    "        return np.array(train_data)\n",
    "    \n",
    "    def load_test_data(self):\n",
    "        \"\"\"Load test data\"\"\"\n",
    "        test_path = self.tst_file + 'int'\n",
    "        print(f\"Loading test data from: {test_path}\")\n",
    "        if not os.path.exists(test_path):\n",
    "            raise FileNotFoundError(f\"File not found: {test_path}\")\n",
    "        with open(test_path, 'rb') as fs:\n",
    "            self.tst_int = np.array(pickle.load(fs))\n",
    "        \n",
    "        self.tst_users = np.reshape(np.argwhere(self.tst_int != None), [-1])\n",
    "        # Limit test users for faster processing\n",
    "        self.tst_users = self.tst_users[:min(len(self.tst_users), self.max_users)]\n",
    "        print(f\"Using {len(self.tst_users)} test users\")\n",
    "\n",
    "    def get_test_instances(self, num_neg_samples=99, user_subset=None):\n",
    "        \"\"\"Generate test instances with negative sampling for specific users\"\"\"\n",
    "        print(\"Preparing test instances...\")\n",
    "        test_instances = []\n",
    "        \n",
    "        # Use subset of users if provided, otherwise use all test users\n",
    "        test_users = user_subset if user_subset is not None else self.tst_users\n",
    "        max_test_users = 1000  # Limit number of test users\n",
    "        \n",
    "        # Randomly sample test users if there are too many\n",
    "        if len(test_users) > max_test_users:\n",
    "            test_users = np.random.choice(test_users, max_test_users, replace=False)\n",
    "        \n",
    "        for user in test_users:\n",
    "            user = int(user)\n",
    "            pos_item = self.tst_int[user]\n",
    "            if pos_item is not None:\n",
    "                pos_item = int(pos_item)\n",
    "                test_instances.append([user, pos_item, 1.0])\n",
    "                \n",
    "                try:\n",
    "                    # Get negative items\n",
    "                    all_items = set(range(self.n_items))\n",
    "                    pos_items = set(int(x) for x in self.trn_label[user].indices)\n",
    "                    pos_items.add(pos_item)\n",
    "                    neg_items_pool = list(all_items - pos_items)\n",
    "                    \n",
    "                    # Sample negative items\n",
    "                    n_neg = min(num_neg_samples, len(neg_items_pool))\n",
    "                    if n_neg > 0:\n",
    "                        neg_items = np.random.choice(neg_items_pool, size=n_neg, replace=False)\n",
    "                        for neg_item in neg_items:\n",
    "                            test_instances.append([user, int(neg_item), 0.0])\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing test user {user}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        if not test_instances:\n",
    "            raise ValueError(\"No test instances were generated!\")\n",
    "        \n",
    "        test_instances = np.array(test_instances)\n",
    "        print(f\"Generated {len(test_instances)} test instances\")\n",
    "        print(f\"Number of unique users: {len(set(test_instances[:,0]))}\")\n",
    "        return test_instances\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset, test_instances, k=10):\n",
    "    \"\"\"Improved evaluation function with better metrics calculation\"\"\"\n",
    "    model.eval()\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    # Process test instances in smaller chunks\n",
    "    chunk_size = 100\n",
    "    test_chunks = [test_instances[i:i + chunk_size] for i in range(0, len(test_instances), chunk_size)]\n",
    "    \n",
    "    print(f\"Evaluating {len(test_instances)} instances in {len(test_chunks)} chunks...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for chunk_idx, chunk in enumerate(test_chunks):\n",
    "            if chunk_idx % 50 == 0:  # Reduced frequency of progress updates\n",
    "                print(f\"Processing chunk {chunk_idx + 1}/{len(test_chunks)}\")\n",
    "            \n",
    "            # Group by user and ensure we have both positive and negative items\n",
    "            user_items = {}\n",
    "            for inst in chunk:\n",
    "                user = int(inst[0])\n",
    "                item = int(inst[1])\n",
    "                label = float(inst[2])\n",
    "                \n",
    "                if user not in user_items:\n",
    "                    user_items[user] = {'pos': [], 'neg': [], 'all_items': [], 'all_scores': []}\n",
    "                \n",
    "                user_items[user]['all_items'].append(item)\n",
    "                if label > 0.5:\n",
    "                    user_items[user]['pos'].append(item)\n",
    "                else:\n",
    "                    user_items[user]['neg'].append(item)\n",
    "            \n",
    "            # Process each user that has both positive and negative items\n",
    "            for user, data in user_items.items():\n",
    "                if not data['pos'] or not data['neg']:\n",
    "                    continue\n",
    "                \n",
    "                items = data['all_items']\n",
    "                batch_size = len(items)\n",
    "                \n",
    "                # Create input features\n",
    "                behaviors = torch.zeros(batch_size, dataset.n_behaviors, device=device)\n",
    "                for i, item in enumerate(items):\n",
    "                    for j, mat in enumerate(dataset.trn_mats):\n",
    "                        behaviors[i, j] = float(mat[user, item])\n",
    "                \n",
    "                x = torch.cat([\n",
    "                    behaviors,\n",
    "                    torch.zeros(batch_size, model.input_dim - behaviors.size(1), device=device)\n",
    "                ], dim=1)\n",
    "                \n",
    "                # Simplified adjacency matrix for evaluation\n",
    "                adj_matrix = torch.eye(batch_size, device=device)\n",
    "                graph_metrics = dataset.create_graph_metrics([user] * batch_size)\n",
    "                \n",
    "                try:\n",
    "                    predictions = model(x, adj_matrix, graph_metrics)\n",
    "                    predictions = predictions.cpu().numpy().flatten()\n",
    "                    \n",
    "                    # Store all scores\n",
    "                    for item, score in zip(items, predictions):\n",
    "                        data['all_scores'].append((item, score))\n",
    "                    \n",
    "                    # Sort items by score\n",
    "                    sorted_items = [x[0] for x in sorted(data['all_scores'], key=lambda x: x[1], reverse=True)]\n",
    "                    recommended_items = sorted_items[:k]\n",
    "                    \n",
    "                    # Calculate HR\n",
    "                    hit = False\n",
    "                    for pos_item in data['pos']:\n",
    "                        if pos_item in recommended_items:\n",
    "                            hit = True\n",
    "                            break\n",
    "                    hits.append(hit)\n",
    "                    \n",
    "                    # Calculate NDCG\n",
    "                    dcg = 0\n",
    "                    idcg = 1  # Ideal DCG for one relevant item\n",
    "                    for i, item in enumerate(recommended_items):\n",
    "                        if item in data['pos']:\n",
    "                            dcg += 1 / np.log2(i + 2)\n",
    "                    ndcg = dcg / idcg\n",
    "                    ndcgs.append(ndcg)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating user {user}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    hr = np.mean(hits) if hits else 0\n",
    "    ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(f\"\\nEvaluation Statistics:\")\n",
    "    print(f\"Total users evaluated: {len(hits)}\")\n",
    "    print(f\"Number of hits: {sum(hits)}\")\n",
    "    print(f\"Average HR@{k}: {hr:.4f}\")\n",
    "    print(f\"Average NDCG@{k}: {ndcg:.4f}\")\n",
    "    \n",
    "    return hr, ndcg\n",
    "\n",
    "def get_test_instances(self, num_neg_samples=99):\n",
    "    \"\"\"Improved test instance generation\"\"\"\n",
    "    print(\"Preparing test instances...\")\n",
    "    test_instances = []\n",
    "    max_test_users = 1000  # Limit number of test users\n",
    "    \n",
    "    # Randomly sample test users if there are too many\n",
    "    test_users = self.tst_users\n",
    "    if len(test_users) > max_test_users:\n",
    "        test_users = np.random.choice(test_users, max_test_users, replace=False)\n",
    "    \n",
    "    for user in test_users:\n",
    "        user = int(user)\n",
    "        pos_item = self.tst_int[user]\n",
    "        if pos_item is not None:\n",
    "            pos_item = int(pos_item)\n",
    "            test_instances.append([user, pos_item, 1.0])\n",
    "            \n",
    "            try:\n",
    "                # Get negative items\n",
    "                all_items = set(range(self.n_items))\n",
    "                pos_items = set(int(x) for x in self.trn_label[user].indices)\n",
    "                pos_items.add(pos_item)\n",
    "                neg_items_pool = list(all_items - pos_items)\n",
    "                \n",
    "                # Sample negative items\n",
    "                n_neg = min(num_neg_samples, len(neg_items_pool))\n",
    "                if n_neg > 0:\n",
    "                    neg_items = np.random.choice(neg_items_pool, size=n_neg, replace=False)\n",
    "                    for neg_item in neg_items:\n",
    "                        test_instances.append([user, int(neg_item), 0.0])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing test user {user}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    if not test_instances:\n",
    "        raise ValueError(\"No test instances were generated!\")\n",
    "    \n",
    "    test_instances = np.array(test_instances)\n",
    "    print(f\"Generated {len(test_instances)} test instances\")\n",
    "    print(f\"Number of unique users: {len(set(test_instances[:,0]))}\")\n",
    "    return test_instances\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'latent_dim': 64,\n",
    "        'dropout': 0.1,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-6,\n",
    "        'gradient_clip': 1.0,\n",
    "        'max_samples': 5000,\n",
    "        'eval_k': 10\n",
    "    }\n",
    "    \n",
    "    print(\"Initializing dataset...\")\n",
    "    dataset = MultiBehaviorDataset(max_users=10000)\n",
    "    train_data = dataset.prepare_train_instances(max_samples=config['max_samples'])\n",
    "    \n",
    "    print(f\"Using {len(train_data)} training instances\")\n",
    "\n",
    "    # Initialize model with dataset dimensions\n",
    "    model = MultiBehaviorBiasMF(\n",
    "        n_users=dataset.n_users,\n",
    "        n_items=dataset.n_items,\n",
    "        n_behaviors=dataset.n_behaviors,\n",
    "        latent_dim=config['latent_dim'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    print(\"Preparing training tensors...\")\n",
    "    users = torch.LongTensor(train_data[:, 0]).to(device)\n",
    "    items = torch.LongTensor(train_data[:, 1]).to(device)\n",
    "    labels = torch.FloatTensor(train_data[:, 2]).to(device)\n",
    "    behaviors = torch.FloatTensor(train_data[:, 3:]).to(device)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        model.train()\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Training\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(users, items, behaviors)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip'])\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"Training completed in {training_time:.2f}s\")\n",
    "        print(f\"Training loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        print(\"\\nStarting evaluation...\")\n",
    "        eval_start_time = datetime.now()\n",
    "        \n",
    "        test_instances = dataset.get_test_instances()\n",
    "        hr, ndcg = evaluate_model(model, dataset, test_instances, k=config['eval_k'])\n",
    "        \n",
    "        eval_time = (datetime.now() - eval_start_time).total_seconds()\n",
    "        print(f\"Evaluation completed in {eval_time:.2f}s\")\n",
    "        print(f\"HR@{config['eval_k']}: {hr:.4f}\")\n",
    "        print(f\"NDCG@{config['eval_k']}: {ndcg:.4f}\")\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'training_time': training_time,\n",
    "            'eval_time': eval_time,\n",
    "            'training_loss': loss.item(),\n",
    "            'hr': hr,\n",
    "            'ndcg': ndcg\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        with open(f'results_{timestamp}.txt', 'w') as f:\n",
    "            for key, value in results.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "class AutoRec(nn.Module):\n",
    "    def __init__(self, num_items, hidden_dim=256):\n",
    "        super(AutoRec, self).__init__()\n",
    "        self.input_dim = num_items\n",
    "        \n",
    "        # Simplified architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_items, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, num_items),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, adjacency_matrix=None, graph_metrics=None):\n",
    "        if x.dim() > 2:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Normalize input\n",
    "        x = torch.clamp(x, 0, 1)  # Ensure input is between 0 and 1\n",
    "        \n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        return out\n",
    "\n",
    "def main():\n",
    "    # Simplified configuration\n",
    "    config = {\n",
    "        'batch_size': 32,\n",
    "        'num_epochs': 20,\n",
    "        'learning_rate': 0.001,\n",
    "        'hidden_dim': 256,\n",
    "        'eval_k': 10,\n",
    "        'max_samples': 10000\n",
    "    }\n",
    "    \n",
    "    print(\"Initializing dataset...\")\n",
    "    dataset = MultiBehaviorDataset(max_users=1000)\n",
    "    \n",
    "    # Create binary interaction matrix (simplify to just purchases)\n",
    "    interaction_matrix = dataset.trn_mats[-1].toarray()  # Use only purchase behavior\n",
    "    print(f\"Interaction matrix shape: {interaction_matrix.shape}\")\n",
    "    print(f\"Non-zero elements: {np.count_nonzero(interaction_matrix)}\")\n",
    "    \n",
    "    model = AutoRec(\n",
    "        num_items=dataset.n_items,\n",
    "        hidden_dim=config['hidden_dim']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        for epoch in range(config['num_epochs']):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            # Create batches of users\n",
    "            user_indices = np.random.permutation(min(1000, dataset.n_users))\n",
    "            \n",
    "            for start_idx in range(0, len(user_indices), config['batch_size']):\n",
    "                batch_users = user_indices[start_idx:start_idx + config['batch_size']]\n",
    "                batch_interactions = torch.FloatTensor(interaction_matrix[batch_users]).to(device)\n",
    "                \n",
    "                # Skip empty batches\n",
    "                if torch.sum(batch_interactions) == 0:\n",
    "                    continue\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(batch_interactions)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(predictions, batch_interactions)\n",
    "                \n",
    "                if num_batches % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}, Batch {num_batches}\")\n",
    "                    print(f\"Loss: {loss.item():.4f}\")\n",
    "                    print(f\"Max prediction: {torch.max(predictions):.4f}\")\n",
    "                    print(f\"Min prediction: {torch.min(predictions):.4f}\")\n",
    "                    print(f\"Num positive targets: {torch.sum(batch_interactions > 0).item()}\")\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            avg_loss = total_loss / (num_batches + 1e-8)\n",
    "            print(f\"\\nEpoch {epoch+1} completed. Average loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Evaluation\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(\"\\nEvaluating...\")\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    test_instances = dataset.get_test_instances(num_neg_samples=19)  # Reduced negative samples\n",
    "                    hr, ndcg = evaluate_model(model, dataset, test_instances, k=config['eval_k'])\n",
    "                    print(f\"HR@{config['eval_k']}: {hr:.4f}\")\n",
    "                    print(f\"NDCG@{config['eval_k']}: {ndcg:.4f}\")\n",
    "        \n",
    "        # Final evaluation\n",
    "        print(\"\\nFinal evaluation...\")\n",
    "        model.eval()\n",
    "        test_instances = dataset.get_test_instances(num_neg_samples=99)\n",
    "        hr, ndcg = evaluate_model(model, dataset, test_instances, k=config['eval_k'])\n",
    "        \n",
    "        print(f\"\\nFinal Results:\")\n",
    "        print(f\"HR@{config['eval_k']}: {hr:.4f}\")\n",
    "        print(f\"NDCG@{config['eval_k']}: {ndcg:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset, test_instances, k=10):\n",
    "    \"\"\"Fixed evaluation function\"\"\"\n",
    "    model.eval()\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    chunk_size = 100\n",
    "    test_chunks = [test_instances[i:i + chunk_size] for i in range(0, len(test_instances), chunk_size)]\n",
    "    \n",
    "    print(f\"Evaluating {len(test_instances)} instances in {len(test_chunks)} chunks...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for chunk_idx, chunk in enumerate(test_chunks):\n",
    "            if chunk_idx % 50 == 0:\n",
    "                print(f\"Processing chunk {chunk_idx + 1}/{len(test_chunks)}\")\n",
    "            \n",
    "            user_items = {}\n",
    "            for inst in chunk:\n",
    "                user = int(inst[0])\n",
    "                item = int(inst[1])\n",
    "                label = float(inst[2])\n",
    "                \n",
    "                if user not in user_items:\n",
    "                    user_items[user] = {'pos': [], 'neg': [], 'items': [], 'scores': []}\n",
    "                \n",
    "                user_items[user]['items'].append(item)\n",
    "                if label > 0.5:\n",
    "                    user_items[user]['pos'].append(item)\n",
    "                else:\n",
    "                    user_items[user]['neg'].append(item)\n",
    "            \n",
    "            for user, data in user_items.items():\n",
    "                if not data['pos'] or not data['neg']:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Get user's interaction vector (fixed broadcasting)\n",
    "                    user_vector = torch.zeros(1, dataset.n_items, device=device)\n",
    "                    for j, mat in enumerate(dataset.trn_mats):\n",
    "                        user_interactions = mat[user].toarray().reshape(1, -1)  # Ensure 2D\n",
    "                        user_vector += torch.tensor(user_interactions, device=device)\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    predictions = model(user_vector)\n",
    "                    predictions = predictions.squeeze().cpu().numpy()  # Properly squeeze\n",
    "                    \n",
    "                    # Get scores for test items\n",
    "                    for item in data['items']:\n",
    "                        data['scores'].append((item, predictions[item]))\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    sorted_items = [x[0] for x in sorted(data['scores'], key=lambda x: x[1], reverse=True)]\n",
    "                    recommended_items = sorted_items[:k]\n",
    "                    \n",
    "                    hit = len(set(recommended_items) & set(data['pos'])) > 0\n",
    "                    hits.append(hit)\n",
    "                    \n",
    "                    # Calculate NDCG\n",
    "                    dcg = 0\n",
    "                    idcg = 1\n",
    "                    for i, item in enumerate(recommended_items):\n",
    "                        if item in data['pos']:\n",
    "                            dcg += 1 / np.log2(i + 2)\n",
    "                    ndcg = dcg / idcg\n",
    "                    ndcgs.append(ndcg)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing user {user}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    hr = np.mean(hits) if hits else 0\n",
    "    ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "    \n",
    "    print(f\"\\nEvaluation Statistics:\")\n",
    "    print(f\"Total users evaluated: {len(hits)}\")\n",
    "    print(f\"Number of hits: {sum(hits)}\")\n",
    "    print(f\"Average HR@{k}: {hr:.4f}\")\n",
    "    print(f\"Average NDCG@{k}: {ndcg:.4f}\")\n",
    "    \n",
    "    return hr, ndcg\n",
    "\n",
    "def main():\n",
    "    # Add debugging for data shapes\n",
    "    config = {\n",
    "        'batch_size': 32,\n",
    "        'num_epochs': 20,\n",
    "        'learning_rate': 0.001,\n",
    "        'hidden_dim': 256,\n",
    "        'eval_k': 10,\n",
    "        'max_samples': 10000\n",
    "    }\n",
    "    \n",
    "    print(\"Initializing dataset...\")\n",
    "    dataset = MultiBehaviorDataset(max_users=10000)\n",
    "    \n",
    "    # Create binary interaction matrix (simplify to just purchases)\n",
    "    interaction_matrix = dataset.trn_mats[-1].toarray()\n",
    "    print(f\"Interaction matrix shape: {interaction_matrix.shape}\")\n",
    "    print(f\"Non-zero elements: {np.count_nonzero(interaction_matrix)}\")\n",
    "    print(f\"Average interactions per user: {np.mean(np.sum(interaction_matrix, axis=1)):.2f}\")\n",
    "    \n",
    "    model = AutoRec(\n",
    "        num_items=dataset.n_items,\n",
    "        hidden_dim=config['hidden_dim']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    try:\n",
    "        for epoch in range(config['num_epochs']):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            # Create batches of users\n",
    "            user_indices = np.random.permutation(min(1000, dataset.n_users))\n",
    "            \n",
    "            for start_idx in range(0, len(user_indices), config['batch_size']):\n",
    "                batch_users = user_indices[start_idx:start_idx + config['batch_size']]\n",
    "                batch_interactions = torch.FloatTensor(interaction_matrix[batch_users]).to(device)\n",
    "                \n",
    "                # Skip empty batches\n",
    "                if torch.sum(batch_interactions) == 0:\n",
    "                    continue\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(batch_interactions)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(predictions, batch_interactions)\n",
    "                \n",
    "                if num_batches % 10 == 0:\n",
    "                    print(f\"\\nEpoch {epoch+1}, Batch {num_batches}\")\n",
    "                    print(f\"Loss: {loss.item():.4f}\")\n",
    "                    print(f\"Max prediction: {torch.max(predictions):.4f}\")\n",
    "                    print(f\"Min prediction: {torch.min(predictions):.4f}\")\n",
    "                    print(f\"Num positive targets: {torch.sum(batch_interactions > 0).item()}\")\n",
    "                    print(f\"Batch shape: {batch_interactions.shape}\")\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            avg_loss = total_loss / (num_batches + 1e-8)\n",
    "            print(f\"\\nEpoch {epoch+1} completed. Average loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Evaluation every 5 epochs\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(\"\\nEvaluating...\")\n",
    "                model.eval()\n",
    "                test_instances = dataset.get_test_instances(num_neg_samples=19)\n",
    "                hr, ndcg = evaluate_model(model, dataset, test_instances, k=config['eval_k'])\n",
    "                print(f\"HR@{config['eval_k']}: {hr:.4f}\")\n",
    "                print(f\"NDCG@{config['eval_k']}: {ndcg:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCF --very low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "import operator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "Loading training data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/\n",
      "Loading behavior: pv from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_pv\n",
      "Loading behavior: cart from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_cart\n",
      "Loading behavior: buy from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_buy\n",
      "Dataset dimensions: 21716 users, 7977 items\n",
      "Loading test data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/tst_int\n",
      "Using 10000 test users\n",
      "Preparing training instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/20z1hn994jd04q4kl0gpgh740000gn/T/ipykernel_37763/1719090174.py:35: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  mat = pickle.load(fs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 66745 training instances\n",
      "Starting training...\n",
      "Batch 0: Loss = 0.7201\n",
      "\n",
      "Epoch 1/50\n",
      "Average Loss: 0.7207\n",
      "Time: 3.57s\n",
      "Batch 0: Loss = 0.7215\n",
      "\n",
      "Epoch 2/50\n",
      "Average Loss: 0.7186\n",
      "Time: 3.43s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 115\n",
      "Average HR@10: 0.1150\n",
      "Average NDCG@10: 0.0495\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.3190\n",
      "NDCG@10 std: 0.1514\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "New best model saved!\n",
      "Batch 0: Loss = 0.7159\n",
      "\n",
      "Epoch 3/50\n",
      "Average Loss: 0.7113\n",
      "Time: 3.42s\n",
      "Batch 0: Loss = 0.7069\n",
      "\n",
      "Epoch 4/50\n",
      "Average Loss: 0.6995\n",
      "Time: 3.52s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 113\n",
      "Average HR@10: 0.1130\n",
      "Average NDCG@10: 0.0541\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.3166\n",
      "NDCG@10 std: 0.1678\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Batch 0: Loss = 0.6913\n",
      "\n",
      "Epoch 5/50\n",
      "Average Loss: 0.6824\n",
      "Time: 3.62s\n",
      "Batch 0: Loss = 0.6737\n",
      "\n",
      "Epoch 6/50\n",
      "Average Loss: 0.6612\n",
      "Time: 3.18s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 124\n",
      "Average HR@10: 0.1240\n",
      "Average NDCG@10: 0.0554\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.3296\n",
      "NDCG@10 std: 0.1615\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "New best model saved!\n",
      "Batch 0: Loss = 0.6479\n",
      "\n",
      "Epoch 7/50\n",
      "Average Loss: 0.6339\n",
      "Time: 3.47s\n",
      "Batch 0: Loss = 0.6174\n",
      "\n",
      "Epoch 8/50\n",
      "Average Loss: 0.6022\n",
      "Time: 3.22s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 151\n",
      "Average HR@10: 0.1510\n",
      "Average NDCG@10: 0.0676\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.3580\n",
      "NDCG@10 std: 0.1778\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "New best model saved!\n",
      "Batch 0: Loss = 0.5840\n",
      "\n",
      "Epoch 9/50\n",
      "Average Loss: 0.5660\n",
      "Time: 3.39s\n",
      "Batch 0: Loss = 0.5463\n",
      "\n",
      "Epoch 10/50\n",
      "Average Loss: 0.5267\n",
      "Time: 3.37s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 165\n",
      "Average HR@10: 0.1650\n",
      "Average NDCG@10: 0.0843\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.3712\n",
      "NDCG@10 std: 0.2138\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "New best model saved!\n",
      "Batch 0: Loss = 0.5073\n",
      "\n",
      "Epoch 11/50\n",
      "Average Loss: 0.4876\n",
      "Time: 3.35s\n",
      "Batch 0: Loss = 0.4671\n",
      "\n",
      "Epoch 12/50\n",
      "Average Loss: 0.4523\n",
      "Time: 3.32s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 203\n",
      "Average HR@10: 0.2030\n",
      "Average NDCG@10: 0.1014\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.4022\n",
      "NDCG@10 std: 0.2247\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "New best model saved!\n",
      "Batch 0: Loss = 0.4354\n",
      "\n",
      "Epoch 13/50\n",
      "Average Loss: 0.4199\n",
      "Time: 3.40s\n",
      "Batch 0: Loss = 0.4047\n",
      "\n",
      "Epoch 14/50\n",
      "Average Loss: 0.3904\n",
      "Time: 3.49s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 147\n",
      "Average HR@10: 0.1470\n",
      "Average NDCG@10: 0.0805\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.3541\n",
      "NDCG@10 std: 0.2167\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Batch 0: Loss = 0.3751\n",
      "\n",
      "Epoch 15/50\n",
      "Average Loss: 0.3621\n",
      "Time: 3.18s\n",
      "Batch 0: Loss = 0.3477\n",
      "\n",
      "Epoch 16/50\n",
      "Average Loss: 0.3360\n",
      "Time: 3.39s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 181\n",
      "Average HR@10: 0.1810\n",
      "Average NDCG@10: 0.0936\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.3850\n",
      "NDCG@10 std: 0.2205\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Batch 0: Loss = 0.3213\n",
      "\n",
      "Epoch 17/50\n",
      "Average Loss: 0.3109\n",
      "Time: 3.26s\n",
      "Batch 0: Loss = 0.2978\n",
      "\n",
      "Epoch 18/50\n",
      "Average Loss: 0.2871\n",
      "Time: 3.43s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 219\n",
      "Average HR@10: 0.2190\n",
      "Average NDCG@10: 0.1101\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.4136\n",
      "NDCG@10 std: 0.2348\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "New best model saved!\n",
      "Batch 0: Loss = 0.2746\n",
      "\n",
      "Epoch 19/50\n",
      "Average Loss: 0.2651\n",
      "Time: 3.27s\n",
      "Batch 0: Loss = 0.2534\n",
      "\n",
      "Epoch 20/50\n",
      "Average Loss: 0.2440\n",
      "Time: 3.48s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 203\n",
      "Average HR@10: 0.2030\n",
      "Average NDCG@10: 0.1016\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.4022\n",
      "NDCG@10 std: 0.2292\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Batch 0: Loss = 0.2358\n",
      "\n",
      "Epoch 21/50\n",
      "Average Loss: 0.2240\n",
      "Time: 3.46s\n",
      "Batch 0: Loss = 0.2139\n",
      "\n",
      "Epoch 22/50\n",
      "Average Loss: 0.2054\n",
      "Time: 3.24s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 224\n",
      "Average HR@10: 0.2240\n",
      "Average NDCG@10: 0.1056\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.4169\n",
      "NDCG@10 std: 0.2227\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "New best model saved!\n",
      "Batch 0: Loss = 0.1970\n",
      "\n",
      "Epoch 23/50\n",
      "Average Loss: 0.1886\n",
      "Time: 3.50s\n",
      "Batch 0: Loss = 0.1783\n",
      "\n",
      "Epoch 24/50\n",
      "Average Loss: 0.1723\n",
      "Time: 3.18s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 225\n",
      "Average HR@10: 0.2250\n",
      "Average NDCG@10: 0.1100\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.4176\n",
      "NDCG@10 std: 0.2279\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "New best model saved!\n",
      "Batch 0: Loss = 0.1640\n",
      "\n",
      "Epoch 25/50\n",
      "Average Loss: 0.1577\n",
      "Time: 3.34s\n",
      "Batch 0: Loss = 0.1498\n",
      "\n",
      "Epoch 26/50\n",
      "Average Loss: 0.1440\n",
      "Time: 3.49s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 194\n",
      "Average HR@10: 0.1940\n",
      "Average NDCG@10: 0.0967\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.3954\n",
      "NDCG@10 std: 0.2198\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Batch 0: Loss = 0.1372\n",
      "\n",
      "Epoch 27/50\n",
      "Average Loss: 0.1314\n",
      "Time: 3.44s\n",
      "Batch 0: Loss = 0.1245\n",
      "\n",
      "Epoch 28/50\n",
      "Average Loss: 0.1196\n",
      "Time: 3.21s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 205\n",
      "Average HR@10: 0.2050\n",
      "Average NDCG@10: 0.1056\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.4037\n",
      "NDCG@10 std: 0.2335\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Batch 0: Loss = 0.1121\n",
      "\n",
      "Epoch 29/50\n",
      "Average Loss: 0.1093\n",
      "Time: 3.47s\n",
      "Batch 0: Loss = 0.1045\n",
      "\n",
      "Epoch 30/50\n",
      "Average Loss: 0.1000\n",
      "Time: 3.22s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 197\n",
      "Average HR@10: 0.1970\n",
      "Average NDCG@10: 0.1018\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.3977\n",
      "NDCG@10 std: 0.2322\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Batch 0: Loss = 0.0950\n",
      "\n",
      "Epoch 31/50\n",
      "Average Loss: 0.0912\n",
      "Time: 3.30s\n",
      "Batch 0: Loss = 0.0874\n",
      "\n",
      "Epoch 32/50\n",
      "Average Loss: 0.0837\n",
      "Time: 3.31s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 218\n",
      "Average HR@10: 0.2180\n",
      "Average NDCG@10: 0.1176\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.4129\n",
      "NDCG@10 std: 0.2529\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.6000e-05.\n",
      "Batch 0: Loss = 0.0811\n",
      "\n",
      "Epoch 33/50\n",
      "Average Loss: 0.0782\n",
      "Time: 3.30s\n",
      "Batch 0: Loss = 0.0753\n",
      "\n",
      "Epoch 34/50\n",
      "Average Loss: 0.0736\n",
      "Time: 3.34s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 195\n",
      "Average HR@10: 0.1950\n",
      "Average NDCG@10: 0.0939\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.3962\n",
      "NDCG@10 std: 0.2138\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Batch 0: Loss = 0.0681\n",
      "\n",
      "Epoch 35/50\n",
      "Average Loss: 0.0672\n",
      "Time: 3.33s\n",
      "Batch 0: Loss = 0.0644\n",
      "\n",
      "Epoch 36/50\n",
      "Average Loss: 0.0617\n",
      "Time: 3.40s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 194\n",
      "Average HR@10: 0.1940\n",
      "Average NDCG@10: 0.1017\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.3954\n",
      "NDCG@10 std: 0.2358\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Batch 0: Loss = 0.0592\n",
      "\n",
      "Epoch 37/50\n",
      "Average Loss: 0.0569\n",
      "Time: 3.25s\n",
      "Batch 0: Loss = 0.0555\n",
      "\n",
      "Epoch 38/50\n",
      "Average Loss: 0.0527\n",
      "Time: 3.37s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 214\n",
      "Average HR@10: 0.2140\n",
      "Average NDCG@10: 0.1110\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.4101\n",
      "NDCG@10 std: 0.2412\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Batch 0: Loss = 0.0481\n",
      "\n",
      "Epoch 39/50\n",
      "Average Loss: 0.0486\n",
      "Time: 3.27s\n",
      "Batch 0: Loss = 0.0472\n",
      "\n",
      "Epoch 40/50\n",
      "Average Loss: 0.0451\n",
      "Time: 3.40s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 207\n",
      "Average HR@10: 0.2070\n",
      "Average NDCG@10: 0.1048\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.4052\n",
      "NDCG@10 std: 0.2319\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Epoch 00020: reducing learning rate of group 0 to 2.0000e-05.\n",
      "Batch 0: Loss = 0.0435\n",
      "\n",
      "Epoch 41/50\n",
      "Average Loss: 0.0429\n",
      "Time: 3.29s\n",
      "Batch 0: Loss = 0.0419\n",
      "\n",
      "Epoch 42/50\n",
      "Average Loss: 0.0404\n",
      "Time: 3.84s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 196\n",
      "Average HR@10: 0.1960\n",
      "Average NDCG@10: 0.0998\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.3970\n",
      "NDCG@10 std: 0.2281\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Batch 0: Loss = 0.0384\n",
      "\n",
      "Epoch 43/50\n",
      "Average Loss: 0.0375\n",
      "Time: 3.32s\n",
      "Batch 0: Loss = 0.0349\n",
      "\n",
      "Epoch 44/50\n",
      "Average Loss: 0.0353\n",
      "Time: 3.43s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 204\n",
      "Average HR@10: 0.2040\n",
      "Average NDCG@10: 0.1043\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.4030\n",
      "NDCG@10 std: 0.2319\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Batch 0: Loss = 0.0352\n",
      "\n",
      "Epoch 45/50\n",
      "Average Loss: 0.0329\n",
      "Time: 3.24s\n",
      "Batch 0: Loss = 0.0295\n",
      "\n",
      "Epoch 46/50\n",
      "Average Loss: 0.0311\n",
      "Time: 3.38s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 212\n",
      "Average HR@10: 0.2120\n",
      "Average NDCG@10: 0.1056\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.4087\n",
      "NDCG@10 std: 0.2319\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Batch 0: Loss = 0.0277\n",
      "\n",
      "Epoch 47/50\n",
      "Average Loss: 0.0295\n",
      "Time: 3.27s\n",
      "Batch 0: Loss = 0.0318\n",
      "\n",
      "Epoch 48/50\n",
      "Average Loss: 0.0276\n",
      "Time: 3.48s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 207\n",
      "Average HR@10: 0.2070\n",
      "Average NDCG@10: 0.0964\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.4052\n",
      "NDCG@10 std: 0.2123\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "Epoch 00024: reducing learning rate of group 0 to 2.4000e-05.\n",
      "Batch 0: Loss = 0.0259\n",
      "\n",
      "Epoch 49/50\n",
      "Average Loss: 0.0263\n",
      "Time: 3.34s\n",
      "Batch 0: Loss = 0.0263\n",
      "\n",
      "Epoch 50/50\n",
      "Average Loss: 0.0256\n",
      "Time: 3.46s\n",
      "\n",
      "Evaluating...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances...\n",
      "Processing user 100/1000\n",
      "Processing user 200/1000\n",
      "Processing user 300/1000\n",
      "Processing user 400/1000\n",
      "Processing user 500/1000\n",
      "Processing user 600/1000\n",
      "Processing user 700/1000\n",
      "Processing user 800/1000\n",
      "Processing user 900/1000\n",
      "Processing user 1000/1000\n",
      "\n",
      "Evaluation Statistics:\n",
      "Total users evaluated: 1000\n",
      "Number of hits: 189\n",
      "Average HR@10: 0.1890\n",
      "Average NDCG@10: 0.0968\n",
      "\n",
      "Detailed Metrics:\n",
      "HR@10 std: 0.3915\n",
      "NDCG@10 std: 0.2285\n",
      "HR@10 median: 0.0000\n",
      "NDCG@10 median: 0.0000\n",
      "\n",
      "Training completed!\n",
      "Best HR@10: 0.2250\n",
      "Best NDCG@10: 0.1100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "import operator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class MultiBehaviorDataset:\n",
    "    def __init__(self, data_path='/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/', max_users=10000):\n",
    "        self.data_path = data_path\n",
    "        self.behaviors = ['pv', 'cart', 'buy']\n",
    "        self.trn_file = data_path + 'trn_'\n",
    "        self.tst_file = data_path + 'tst_'\n",
    "        self.max_users = max_users\n",
    "        \n",
    "        self.load_training_data()\n",
    "        self.load_test_data()\n",
    "    \n",
    "    def load_training_data(self):\n",
    "        print(\"Loading training data from:\", self.data_path)\n",
    "        self.trn_mats = []\n",
    "        for beh in self.behaviors:\n",
    "            path = self.trn_file + beh\n",
    "            print(f\"Loading behavior: {beh} from {path}\")\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"File not found: {path}\")\n",
    "            with open(path, 'rb') as fs:\n",
    "                mat = pickle.load(fs)\n",
    "                if not isinstance(mat, csr_matrix):\n",
    "                    mat = csr_matrix(mat)\n",
    "                mat = (mat != 0).astype(np.float32)\n",
    "                self.trn_mats.append(mat)\n",
    "        \n",
    "        self.trn_label = 1 * (self.trn_mats[-1] != 0)\n",
    "        self.n_users, self.n_items = self.trn_mats[0].shape\n",
    "        self.n_behaviors = len(self.behaviors)\n",
    "        print(f\"Dataset dimensions: {self.n_users} users, {self.n_items} items\")\n",
    "\n",
    "    def prepare_train_instances(self, max_samples=5000):\n",
    "        print(\"Preparing training instances...\")\n",
    "        train_data = []\n",
    "        \n",
    "        selected_users = np.random.choice(min(self.n_users, self.max_users), \n",
    "                                        size=min(1000, self.max_users), \n",
    "                                        replace=False)\n",
    "        \n",
    "        for user in selected_users:\n",
    "            pos_items = self.trn_label[user].indices\n",
    "            \n",
    "            if len(pos_items) > 0:\n",
    "                for item in pos_items:\n",
    "                    behaviors = [float(mat[user, item]) for mat in self.trn_mats]\n",
    "                    train_data.append([user, item, 1.0] + behaviors)\n",
    "                    \n",
    "                    # Negative sampling\n",
    "                    neg_items = np.random.choice(self.n_items, size=4, replace=False)\n",
    "                    for neg_item in neg_items:\n",
    "                        behaviors = [float(mat[user, neg_item]) for mat in self.trn_mats]\n",
    "                        train_data.append([user, neg_item, 0.0] + behaviors)\n",
    "                        \n",
    "                    if len(train_data) >= max_samples:\n",
    "                        break\n",
    "            \n",
    "            if len(train_data) >= max_samples:\n",
    "                break\n",
    "        \n",
    "        print(f\"Generated {len(train_data)} training instances\")\n",
    "        return np.array(train_data)\n",
    "    \n",
    "    def load_test_data(self):\n",
    "        test_path = self.tst_file + 'int'\n",
    "        print(f\"Loading test data from: {test_path}\")\n",
    "        if not os.path.exists(test_path):\n",
    "            raise FileNotFoundError(f\"File not found: {test_path}\")\n",
    "        with open(test_path, 'rb') as fs:\n",
    "            self.tst_int = np.array(pickle.load(fs))\n",
    "        \n",
    "        self.tst_users = np.reshape(np.argwhere(self.tst_int != None), [-1])\n",
    "        self.tst_users = self.tst_users[:min(len(self.tst_users), self.max_users)]\n",
    "        print(f\"Using {len(self.tst_users)} test users\")\n",
    "\n",
    "    def get_test_instances(self, num_neg_samples=99):\n",
    "        print(\"Preparing test instances...\")\n",
    "        test_instances = []\n",
    "        max_test_users = 1000\n",
    "        \n",
    "        test_users = self.tst_users\n",
    "        if len(test_users) > max_test_users:\n",
    "            test_users = np.random.choice(test_users, max_test_users, replace=False)\n",
    "        \n",
    "        for user in test_users:\n",
    "            user = int(user)\n",
    "            pos_item = self.tst_int[user]\n",
    "            if pos_item is not None:\n",
    "                pos_item = int(pos_item)\n",
    "                test_instances.append([user, pos_item, 1.0])\n",
    "                \n",
    "                try:\n",
    "                    all_items = set(range(self.n_items))\n",
    "                    pos_items = set(int(x) for x in self.trn_label[user].indices)\n",
    "                    pos_items.add(pos_item)\n",
    "                    neg_items_pool = list(all_items - pos_items)\n",
    "                    \n",
    "                    n_neg = min(num_neg_samples, len(neg_items_pool))\n",
    "                    if n_neg > 0:\n",
    "                        neg_items = np.random.choice(neg_items_pool, size=n_neg, replace=False)\n",
    "                        for neg_item in neg_items:\n",
    "                            test_instances.append([user, int(neg_item), 0.0])\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing test user {user}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        if not test_instances:\n",
    "            raise ValueError(\"No test instances were generated!\")\n",
    "        \n",
    "        test_instances = np.array(test_instances)\n",
    "        print(f\"Generated {len(test_instances)} test instances\")\n",
    "        print(f\"Number of unique users: {len(set(test_instances[:,0]))}\")\n",
    "        return test_instances\n",
    "\n",
    "\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_behaviors=3, \n",
    "                 gmf_dim=256, mlp_dim=256, layers=[512, 256, 128], dropout=0.2):\n",
    "        super(NCF, self).__init__()\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_behaviors = num_behaviors\n",
    "        self.gmf_dim = gmf_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "\n",
    "        # Embedding layers\n",
    "        self.user_gmf_embedding = nn.Embedding(num_users, gmf_dim)\n",
    "        self.item_gmf_embedding = nn.Embedding(num_items, gmf_dim)\n",
    "        self.user_mlp_embedding = nn.Embedding(num_users, mlp_dim)\n",
    "        self.item_mlp_embedding = nn.Embedding(num_items, mlp_dim)\n",
    "        \n",
    "        # Behavior-specific embeddings\n",
    "        self.behavior_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(2, mlp_dim) for _ in range(num_behaviors)\n",
    "        ])\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self._init_weights()\n",
    "\n",
    "        # MLP layers\n",
    "        mlp_input_dim = mlp_dim * (3 + num_behaviors)\n",
    "        self.mlp_layers = nn.ModuleList()\n",
    "        for dim in layers:\n",
    "            self.mlp_layers.extend([\n",
    "                nn.Linear(mlp_input_dim, dim),\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            mlp_input_dim = dim\n",
    "\n",
    "        # Final prediction layers\n",
    "        self.final_layer = nn.Linear(gmf_dim + layers[-1], layers[-1])\n",
    "        self.output_layer = nn.Linear(layers[-1], 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in [self.user_gmf_embedding, self.item_gmf_embedding,\n",
    "                      self.user_mlp_embedding, self.item_mlp_embedding]:\n",
    "            nn.init.normal_(module.weight, std=0.01)\n",
    "        \n",
    "        for behavior_emb in self.behavior_embeddings:\n",
    "            nn.init.normal_(behavior_emb.weight, std=0.01)\n",
    "\n",
    "    def forward(self, user_indices, item_indices, behaviors=None):\n",
    "        batch_size = user_indices.size(0)\n",
    "        \n",
    "        # GMF part\n",
    "        gmf_user_emb = self.dropout(self.user_gmf_embedding(user_indices))\n",
    "        gmf_item_emb = self.dropout(self.item_gmf_embedding(item_indices))\n",
    "        gmf_output = gmf_user_emb * gmf_item_emb / np.sqrt(self.gmf_dim)\n",
    "\n",
    "        # MLP part\n",
    "        mlp_user_emb = self.user_mlp_embedding(user_indices)\n",
    "        mlp_item_emb = self.item_mlp_embedding(item_indices)\n",
    "        \n",
    "        # Process behaviors\n",
    "        if behaviors is not None:\n",
    "            behavior_embs = []\n",
    "            for i in range(self.num_behaviors):\n",
    "                behavior = (behaviors[:, i] > 0).long()\n",
    "                behavior_embs.append(self.behavior_embeddings[i](behavior))\n",
    "            behavior_concat = torch.cat(behavior_embs, dim=1)\n",
    "        else:\n",
    "            behavior_concat = torch.zeros(batch_size, self.num_behaviors * self.mlp_dim, \n",
    "                                       device=user_indices.device)\n",
    "        \n",
    "        # Enhanced interaction features\n",
    "        interaction_features = mlp_user_emb * mlp_item_emb\n",
    "        \n",
    "        # Concatenate all features\n",
    "        mlp_input = torch.cat([\n",
    "            mlp_user_emb, \n",
    "            mlp_item_emb, \n",
    "            interaction_features,\n",
    "            behavior_concat\n",
    "        ], dim=1)\n",
    "        \n",
    "        # MLP forward with residual connections\n",
    "        x = mlp_input\n",
    "        for i in range(0, len(self.mlp_layers), 4):\n",
    "            linear = self.mlp_layers[i]\n",
    "            norm = self.mlp_layers[i+1]\n",
    "            relu = self.mlp_layers[i+2]\n",
    "            drop = self.mlp_layers[i+3]\n",
    "            \n",
    "            x = linear(x)\n",
    "            x = norm(x)\n",
    "            x = relu(x)\n",
    "            x = drop(x)\n",
    "            \n",
    "            if x.size(1) == mlp_input.size(1):\n",
    "                x = x + mlp_input\n",
    "        \n",
    "        # Final prediction\n",
    "        concat_output = torch.cat([gmf_output, x], dim=1)\n",
    "        prediction = self.final_layer(concat_output)\n",
    "        prediction = F.relu(prediction)\n",
    "        prediction = self.output_layer(prediction)\n",
    "        \n",
    "        return torch.sigmoid(prediction)\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "    \n",
    "    for batch_idx, (users, items, labels, behaviors) in enumerate(train_loader):\n",
    "        users = users.to(device)\n",
    "        items = items.to(device)\n",
    "        labels = labels.to(device)\n",
    "        behaviors = behaviors.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(users, items, behaviors)\n",
    "        loss = criterion(predictions.view(-1), labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss / total_batches\n",
    "\n",
    "def evaluate_model(model, dataset, test_instances, k=10):\n",
    "    model.eval()\n",
    "    user_metrics = {}\n",
    "    \n",
    "    print(f\"Evaluating {len(test_instances)} instances...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        user_items = {}\n",
    "        for inst in test_instances:\n",
    "            user = int(inst[0])\n",
    "            item = int(inst[1])\n",
    "            label = float(inst[2])\n",
    "            \n",
    "            if user not in user_items:\n",
    "                user_items[user] = {'pos': [], 'neg': [], 'items': [], 'labels': []}\n",
    "            \n",
    "            user_items[user]['items'].append(item)\n",
    "            user_items[user]['labels'].append(label)\n",
    "            if label > 0.5:\n",
    "                user_items[user]['pos'].append(item)\n",
    "            else:\n",
    "                user_items[user]['neg'].append(item)\n",
    "        \n",
    "        total_users = len(user_items)\n",
    "        for idx, (user, data) in enumerate(user_items.items()):\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Processing user {idx + 1}/{total_users}\")\n",
    "            \n",
    "            if not data['pos'] or not data['items']:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                items = torch.LongTensor(data['items']).to(device)\n",
    "                users = torch.LongTensor([user] * len(items)).to(device)\n",
    "                \n",
    "                predictions = model(users, items)\n",
    "                predictions = predictions.cpu().numpy().flatten()\n",
    "                \n",
    "                item_scores = list(zip(data['items'], predictions))\n",
    "                item_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                recommended_items = [x[0] for x in item_scores[:k]]\n",
    "                \n",
    "                hit = any(item in recommended_items for item in data['pos'])\n",
    "                \n",
    "                dcg = 0\n",
    "                idcg = 0\n",
    "                for i, item in enumerate(recommended_items):\n",
    "                    if item in data['pos']:\n",
    "                        dcg += 1 / np.log2(i + 2)\n",
    "                \n",
    "                for i in range(min(len(data['pos']), k)):\n",
    "                    idcg += 1 / np.log2(i + 2)\n",
    "                \n",
    "                ndcg = dcg / idcg if idcg > 0 else 0\n",
    "                \n",
    "                user_metrics[user] = {\n",
    "                    'hit': hit,\n",
    "                    'ndcg': ndcg\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating user {user}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    total_users = len(user_metrics)\n",
    "    if total_users == 0:\n",
    "        return 0, 0\n",
    "    \n",
    "    hr = sum(m['hit'] for m in user_metrics.values()) / total_users\n",
    "    ndcg = sum(m['ndcg'] for m in user_metrics.values()) / total_users\n",
    "    \n",
    "    print(f\"\\nEvaluation Statistics:\")\n",
    "    print(f\"Total users evaluated: {total_users}\")\n",
    "    print(f\"Number of hits: {sum(m['hit'] for m in user_metrics.values())}\")\n",
    "    print(f\"Average HR@{k}: {hr:.4f}\")\n",
    "    print(f\"Average NDCG@{k}: {ndcg:.4f}\")\n",
    "    \n",
    "    hit_rates = [m['hit'] for m in user_metrics.values()]\n",
    "    ndcg_values = [m['ndcg'] for m in user_metrics.values()]\n",
    "    \n",
    "    print(f\"\\nDetailed Metrics:\")\n",
    "    print(f\"HR@{k} std: {np.std(hit_rates):.4f}\")\n",
    "    print(f\"NDCG@{k} std: {np.std(ndcg_values):.4f}\")\n",
    "    print(f\"HR@{k} median: {np.median(hit_rates):.4f}\")\n",
    "    print(f\"NDCG@{k} median: {np.median(ndcg_values):.4f}\")\n",
    "    \n",
    "    return hr, ndcg\n",
    "\n",
    "def main():\n",
    "    config = {\n",
    "        'gmf_dim': 256,\n",
    "        'mlp_dim': 256,\n",
    "        'layers': [512, 256, 128],\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-5,\n",
    "        'dropout': 0.2,\n",
    "        'batch_size': 4096,\n",
    "        'num_epochs': 50,\n",
    "        'eval_k': 10,\n",
    "        'max_samples': 500000,\n",
    "        'warmup_steps': 1000,\n",
    "        'scheduler_patience': 3,\n",
    "        'scheduler_factor': 0.5\n",
    "    }\n",
    "    \n",
    "    print(\"Initializing dataset...\")\n",
    "    dataset = MultiBehaviorDataset(max_users=100000)\n",
    "    train_data = dataset.prepare_train_instances(max_samples=config['max_samples'])\n",
    "    \n",
    "    train_dataset = TensorDataset(\n",
    "        torch.LongTensor(train_data[:, 0]),\n",
    "        torch.LongTensor(train_data[:, 1]),\n",
    "        torch.FloatTensor(train_data[:, 2]),\n",
    "        torch.LongTensor(train_data[:, 3:])\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    model = NCF(\n",
    "        num_users=dataset.n_users,\n",
    "        num_items=dataset.n_items,\n",
    "        num_behaviors=dataset.n_behaviors,\n",
    "        gmf_dim=config['gmf_dim'],\n",
    "        mlp_dim=config['mlp_dim'],\n",
    "        layers=config['layers'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < config['warmup_steps']:\n",
    "            return float(step) / float(max(1, config['warmup_steps']))\n",
    "        return 1.0\n",
    "    \n",
    "    warmup_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    plateau_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max',\n",
    "        factor=config['scheduler_factor'],\n",
    "        patience=config['scheduler_patience'],\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    best_hr = 0\n",
    "    best_ndcg = 0\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(config['num_epochs']):\n",
    "            epoch_start_time = datetime.now()\n",
    "            \n",
    "            model.train()\n",
    "            avg_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            warmup_scheduler.step()\n",
    "            \n",
    "            epoch_time = (datetime.now() - epoch_start_time).total_seconds()\n",
    "            print(f\"\\nEpoch {epoch + 1}/{config['num_epochs']}\")\n",
    "            print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "            print(f\"Time: {epoch_time:.2f}s\")\n",
    "            \n",
    "            if (epoch + 1) % 2 == 0:\n",
    "                print(\"\\nEvaluating...\")\n",
    "                test_instances = dataset.get_test_instances()\n",
    "                hr, ndcg = evaluate_model(model, dataset, test_instances, k=config['eval_k'])\n",
    "                \n",
    "                plateau_scheduler.step(hr)\n",
    "                \n",
    "                if hr > best_hr:\n",
    "                    best_hr = hr\n",
    "                    best_ndcg = ndcg\n",
    "                    torch.save(model.state_dict(), f'best_model_{datetime.now():%Y%m%d_%H%M%S}.pt')\n",
    "                    print(\"New best model saved!\")\n",
    "        \n",
    "        print(\"\\nTraining completed!\")\n",
    "        print(f\"Best HR@10: {best_hr:.4f}\")\n",
    "        print(f\"Best NDCG@10: {best_ndcg:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # wholesome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAJOCAYAAACqS2TfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAADNaklEQVR4nOzde3zP9f//8fs2dmBMzA4OGYo5ZKthjTL6Lgs5RQ6pzcrZCutEaXMoIwyxiAyJSKEDIStJToUVyfnY2Oa41bCxvX5/+O398W4bw957G7fr5fK+1Pv5er5e78fz/Xq/X/Z4P5+v59PGMAxDAAAAAACg0NlaOwAAAAAAAO5WJN0AAAAAAFgISTcAAAAAABZC0g0AAAAAgIWQdAMAAAAAYCEk3QAAAAAAWAhJNwAAAAAAFkLSDQAAAACAhZB0AwAAAABgISTdAHAdNjY2GjFihLXDuG3z58+Xt7e3SpYsqXLlylk7HPyHl5eXevbsae0wgFyaN2+u5s2bm54fOXJENjY2mjt3rlm9VatWydfXV46OjrKxsdH58+clce2xlHXr1snGxkbr1q276X3nzp0rGxsbHTlypNDjApA3km4A13Xw4EH17dtXNWrUkKOjo8qWLaumTZtqypQpunjxorXDQwHs2bNHPXv2VM2aNTVr1izNnDkz37ojRoyQjY2NTp8+ned2Ly8vPf3002ZlNjY2Zo+yZcsqMDBQK1asuKk4//nnHxmGccN6W7du1YABA+Tn56eSJUvKxsbmuvVnz56tOnXqyNHRUQ8++KCmTp16U3HdSXLe44kTJ+balvOH9G+//WYqyzmfOY9SpUrp/vvvV9u2bTVnzhxlZGTk+1rr1q3TM888Iw8PD9nb28vNzU1t27bV0qVLc9VNS0vTe++9p4YNG8rFxUUODg6qVq2aunbtelOfg3///VfZ2dk3rLd3714NGTJETZo0MSV510sgvv76az3yyCNydHTU/fffr6ioKF25cqXAceH6zpw5oy5dusjJyUmxsbGaP3++SpcufVPXHmvbvXu3RowYQSIKwCJKWDsAAHeuFStW6Nlnn5WDg4NCQkJUv359ZWZmasOGDXr99df1559/3tF/RBWGixcvqkSJ4n2pXLdunbKzszVlyhQ98MADFnmNJ598UiEhITIMQ0ePHtX06dPVtm1bfffddwoODs5znytXrmju3Ln69NNPtXnzZmVkZMjBwUENGjTQ888/r759+8rBwSHXfitXrtTHH3+sBg0aqEaNGtq3b1++cX300Ufq16+fOnXqpIiICP3888965ZVXdOHCBb355puF1v6iNn78ePXv31+lSpUqUP3p06fL2dlZGRkZSkxM1OrVq/Xiiy9q8uTJ+vbbb1W1alWz+lFRURo1apQefPBB9e3bV9WqVdOZM2e0cuVKderUSQsWLNBzzz0nSTpw4ICCg4N19OhRdezYUSEhIXJ2dtbx48e1cuVKPf300/rkk0/0wgsv5IrLMAx98cUXmjNnjtavX6/09HSVLFlStWvXVrdu3fTyyy+rbNmyufbbtGmTPvjgA9WtW1d16tRRQkJCvm3/7rvv1KFDBzVv3lxTp07Vzp079e677yolJUXTp08v0PuH/6lWrZouXryokiVLmsp+/fVX/fPPPxo9erSCgoJM5UVx7Sksu3fv1siRI9W8eXN5eXlZOxwAd5ni/ZckAIs5fPiwunXrpmrVqumHH36Qp6enadvAgQN14MCBm+7JLC6ys7OVmZkpR0dHOTo6Wjuc25aSkiJJFh3aWatWLT3//POm5506dVLdunU1ZcqUPJPugwcPqn379jpy5Ig6d+6snj17yt3dXWfOnNGmTZs0cuRITZ8+XV988YXq1atntm///v315ptvysnJSeHh4fkm3RcvXtTbb7+tNm3a6IsvvpAk9e7dW9nZ2Ro9erT69Omj++67rxDfhaLh6+urhIQEzZgxQxEREQXap3PnznJ1dTU9j4yM1IIFCxQSEqJnn31WmzdvNm374osvNGrUKHXu3FkLFy40S65ef/11rV69WpcvX5Z09YeTjh07Kjk5WT/99JOaNm1q9rpRUVFas2aNsrKycsV06tQpderUSZs3b1aHDh0UExOjKlWqKDU1VTt27ND06dM1ffp0LVy4UM2aNTPbt127djp//rzKlCmjCRMmXDfpfu2119SgQQOtWbPG9ANa2bJlNWbMGA0aNEje3t4Feg8t4cqVK8rOzpa9vb3VYrhZNjY2ua6L+V1jLHHtSU9PV+nSpQvteABQJAwAyEO/fv0MScYvv/xSoPqXL182Ro0aZdSoUcOwt7c3qlWrZgwbNsy4dOmSWb1q1aoZbdq0MX788UfDz8/PcHR0NOrXr2/8+OOPhmEYxpdffmnUr1/fcHBwMB555BFj+/btZvuHhoYapUuXNg4ePGi0bNnSKFWqlOHp6WmMHDnSyM7ONqs7fvx4IyAgwChfvrzh6OhoPPLII8aSJUtyxS7JGDhwoPHpp58adevWNUqUKGEsW7bMtC0qKspUNy0tzRg0aJBRrVo1w97e3qhYsaIRFBRkbNu2zeyYn3/+ufHII48Yjo6ORoUKFYwePXoYf//9d55t+fvvv4327dsbpUuXNlxdXY1XX33VuHLlSoHe99jYWKNu3bqGvb294enpaQwYMMA4d+6c2fstyexxbXv+KyoqypBknDp1Ks/tOefvWjnv33+5uroatWrVylX+999/G+7u7sYTTzxhnDhxIs/XOXv2rNG9e3fD09PTOHz4cL7xDhw40Mjvn7IVK1YYkowVK1aYlW/cuNGQZMyfPz/f4xqGYRw5csTo37+/UatWLcPR0dEoX7680blz51zxzJkzx5BkbNiwwRgyZIjh6upqlCpVyujQoYORkpJiVjc7O9sYPXq0UblyZcPJyclo3ry5sWvXLqNatWpGaGjodeMxjP+910888YTh7u5uXLhwIVccv/76q6nsRuezT58+hiRjzZo1pjJvb2+jfPnyRlpa2g3jWbhwoSHJGDt27A3rXistLc2oU6eO0aBBA2Pfvn151rlw4YIxePBgw9nZ2axN/zV+/HhDUp6fkz///NOQZMTGxpqVJyYmGpKM0aNHXzfOM2fOGK+++qpRv359o3Tp0kaZMmWMp556ykhISMhV9+LFi0ZUVJTx4IMPGg4ODoaHh4fRsWNH48CBA4ZhGMbhw4cNScb48eONSZMmGTVq1DBsbW2NHTt2GIZhGPHx8cZjjz1mlCpVynBxcTHatWtn7N692+w1CnL92bdvn/HMM88Y7u7uhoODg1G5cmWja9euxvnz56/bVsMwjI8++sioUaOG4ejoaDRq1MhYv369ERgYaAQGBprq5LRjzpw5hmEYRmBgYK5rTGho6A2vPStXrjS119nZ2WjdurWxa9cus3hyrpEHDhwwWrVqZTg7Oxvt27c3DMMwsrKyjEmTJhl169Y1HBwcDDc3N6NPnz7G2bNnzY6Rc836+eefjUaNGhkODg5G9erVjXnz5pnq5Hx3/vvI+XcpLzmxHT161GjTpo1RunRpo1KlSsa0adMMwzCMP/74w2jRooVRqlQp4/777zcWLFiQ6xgHDx40OnfubNx3332Gk5OT4e/vb3z77be56h0/ftxo3769UapUKaNixYrG4MGDjVWrVuUZ4+bNm43g4GCjbNmyhpOTk9GsWTNjw4YNZnVy2nvtd+bXX381WrZsaVSoUMFwdHQ0vLy8jLCwsHzbD+DmcE83gDx98803qlGjhpo0aVKg+r169VJkZKQeeeQRTZo0SYGBgYqOjla3bt1y1T1w4ICee+45tW3bVtHR0Tp37pzatm2rBQsWaMiQIXr++ec1cuRIHTx4UF26dMl1j2dWVpaeeuopubu76/3335efn5+ioqIUFRVlVm/KlCl6+OGHNWrUKI0ZM0YlSpTQs88+m2cP/Q8//KAhQ4aoa9eumjJlSr7DC/v166fp06erU6dO+vDDD/Xaa6/JyclJf/31l6nO3Llz1aVLF9nZ2Sk6Olq9e/fW0qVL9dhjj5kmF7q2LcHBwapQoYImTJigwMBATZw4sUDD9keMGKGBAweqUqVKmjhxojp16qSPPvpILVu2NPVETp48WR07dpR0dYjx/Pnz9cwzz9zw2GfPntXp06dzPQpyv60kpaam6ty5c3n2JIeEhKhBgwZatWqVaQTF5cuXdenSJUlSRkaGbG1ttWDBAjVr1kz9+/cv0Gv+144dOyRJDRs2NCv38/OTra2taXt+fv31V23cuFHdunXTBx98oH79+ik+Pl7NmzfXhQsXctV/+eWX9fvvvysqKkr9+/fXN998o/DwcLM6kZGReuedd+Tj46Px48erRo0aatmypdLT02+qbSNGjFBycvJtD4/OGfK9Zs0aSdL+/fu1Z88edejQQWXKlLnh/t98840kmY1yKIjBgwerRIkS2rBhgx588EFJV78LOe9rzudh0qRJGjBggEJDQwv82btWfp+BSpUqqUqVKjf8DBw6dEjLly/X008/rZiYGL3++uvauXOnAgMDdeLECVO9rKwsPf300xo5cqT8/Pw0ceJEDRo0SKmpqdq1a5fZMefMmaOpU6eqT58+mjhxosqXL6+1a9cqODhYKSkpGjFihCIiIrRx40Y1bdrU7B7jG11/MjMzFRwcrM2bN+vll19WbGys+vTpo0OHDuW69vzX7Nmz1bdvX3l4eOj9999X06ZN1a5dOx0/fvy6+7399tvq06ePJGnUqFGaP3+++vbte91rz/z589WmTRs5Oztr3Lhxeuedd7R792499thjue6pvnLlioKDg+Xm5qYJEyaoU6dOkqS+ffvq9ddfN80xEhYWpgULFig4ONh0/ctx4MABde7cWU8++aQmTpyo++67Tz179tSff/4pSWrWrJleeeUVSdJbb72l+fPna/78+apTp851256VlaVWrVqpatWqev/99+Xl5aXw8HDNnTtXTz31lBo2bKhx48apTJkyCgkJ0eHDh037Jicnq0mTJlq9erUGDBig9957T5cuXVK7du20bNkyU72LFy/q//7v/7R69WqFh4fr7bff1s8//6w33ngjVzw//PCDmjVrprS0NEVFRWnMmDE6f/68nnjiCW3dujXfdqSkpKhly5Y6cuSIhg4dqqlTp6pHjx5mI2AA3CZrZ/0A7jypqamGJFOPwo0kJCQYkoxevXqZlb/22muGJOOHH34wleX0fmzcuNFUtnr1akOS4eTkZBw9etRU/tFHH+X6JT80NNSQZLz88sumsuzsbKNNmzaGvb29WY/etb2AhmEYmZmZRv369Y0nnnjCrFySYWtra/z555+52qb/9M64uLjk2at77Wu4ubkZ9evXNy5evGgq//bbbw1JRmRkZK62jBo1yuwYDz/8sOHn55fvaxiGYaSkpBj29vZGy5YtjaysLFP5tGnTDElGXFycqexGvZ3Xyql7vUdePd0vvfSScerUKSMlJcX47bffjKeeesrUq3etdevWGaVLlzYSExMNw7g6QmLAgAGGvb29YWNjY7Rp08aYMGGCqWctJSXFcHR0zLc39Ho93QMHDjTs7Ozy3FaxYkWjW7du130v/vv5MQzD2LRpkyHJ+OSTT0xlOb1GQUFBZqMthgwZYtjZ2Zl6GHPOWZs2bczqvfXWW6bewRvRNaMKWrRoYXh4eJjivJWe7nPnzhmSjI4dOxqGYRhfffWVIcmYNGnSDWMxjKuf1XLlyuUq//fff41Tp06ZHqmpqaZtBw4cMEqUKGHq4TUMwxg5cqRRunRpQ5LRpEkTIy4uzqhWrZphGIaRkZFheHh4mPXGX+t6Pd05244dO5ZrW6NGjYxHH330uu27dOmS2ffLMK729Do4OJh9b+Pi4gxJRkxMTK5j5JzrnB7ismXL5hoB4evra7i5uRlnzpwxlf3++++Gra2tERISYiq70fVnx44dhqQ8R/RcT851y9fX18jIyDCVz5w505B03Z5uw8j7s2cYeX/+/vnnH6NcuXJG7969zeomJSUZLi4uZuU518ihQ4ea1f35558NSbl6j3N6f68tz/k3Z/369aaylJQUw8HBwXj11VdNZUuWLLlh7/a1cmIbM2aMqezcuXOGk5OTYWNjYyxatMhUvmfPnlz/lgwePNiQZPz8889m70316tUNLy8v0+du8uTJhiTj888/N9VLT083HnjgAbN4s7OzjQcffNAIDg42u75cuHDBqF69uvHkk0+ayv7b071s2bI8zx+AwkNPN4Bc0tLSJKlAPV3S1YmtJOW6v/TVV1+VpFw9y3Xr1lVAQIDpub+/vyTpiSee0P3335+r/NChQ7le89oeRBsbG4WHhyszM1Nr1641lTs5OZn+/9y5c0pNTdXjjz+u7du35zpeYGCg6tate4OWXr03ccuWLWa9XNf67bfflJKSogEDBpjd99imTRt5e3vn2cver18/s+ePP/54nm2+1tq1a5WZmanBgwfL1vZ/l/LevXurbNmyt32//Zdffqnvv/8+18Pd3T3P+rNnz1bFihXl5uamhg0bKj4+Xm+88Uauz8SSJUvUqVMnVapUSZI0depUzZkzR5GRkVq6dKnc3d0VGRlpql+xYkUFBATc0rI4Fy9ezPdeWUdHxxvOvn/t5+fy5cs6c+aMHnjgAZUrVy7Pz1CfPn3MZlJ//PHHlZWVpaNHj0r63zl7+eWXzeoNHjz4ZpplMmLECCUlJWnGjBm3tL8kOTs7S7o6c7x089/9tLQ00zGu9fbbb6tixYqmR86ka5K0bNkyNWnSRL6+vqbnI0eO1IABA7R8+XIFBASYeh0lyd7eXq1atbrlz4CkPCfkK8hnwMHBwfT9ysrK0pkzZ+Ts7KzatWubfQa+/PJLubq66uWXX851jP/Ort+pUydVrFjR9PzkyZNKSEhQz549Vb58eVN5gwYN9OSTT5qur9KNrz8uLi6SpNWrV+c5GiM/Odetfv36mX1nevbsaTpmYfn+++91/vx5de/e3WwUjZ2dnfz9/fXjjz/m2ue/o12WLFkiFxcXPfnkk2bH8PPzk7Ozc65j1K1bV48//rjpecWKFVW7du0bXmcLolevXqb/L1eunGrXrq3SpUurS5cupvLatWurXLlyZq+3cuVKNW7cWI899pipzNnZWX369NGRI0e0e/duUz1PT0917tzZVK9UqVKm0QU5EhIStH//fj333HM6c+aM6T1JT0/X//3f/2n9+vX5jhbJuef+22+/zTVKAEDhYCI1ALnkzBac84f4jRw9elS2tra5Zqf18PBQuXLlTElHjmsTa+l/fyj+dwblnPJz586Zldva2qpGjRpmZbVq1ZIks6GJ3377rd59910lJCSYLY2U1xJT1atXz7d913r//fcVGhqqqlWrys/PT61bt1ZISIgpnpy21q5dO9e+3t7e2rBhg1mZo6Oj2R/gknTfffflavN/5fc69vb2qlGjRq73/GY1a9bMbOKta+PNS/v27U0/fPz6668aM2aMLly4YPaDgCRt27ZNffv2NT2fNWuWhg4dqrfffluS1KFDBx08eNBsH3d3d506deqm2+Dk5KTMzMw8t126dMksqc7LxYsXFR0drTlz5igxMdFsObPU1NRc9f/7uc4ZWp9zLnPOSc5w6hwVK1a8pQndmjVrphYtWuj999/P9cNNQf3777+S/pdk3+x3v0yZMjpz5kyu8gEDBpiWlvvv0PNt27apRYsWpuezZs1SaGio3n//fUlXP0unT582S7Jv5zMgKc+l0QryGciZefvDDz/U4cOHzSaEq1Chgun/Dx48qNq1axdopYP/Xmuud82oU6eOVq9ebZo87EbXn+rVqysiIkIxMTFasGCBHn/8cbVr107PP//8dZPn/D6bJUuWzHWtvV379++XdPVH1rz8d7b6EiVKqEqVKrmOkZqaKjc3tzyPkTOBW47/fjelgl1nbySv67eLi4uqVKmS698ZFxcXs9c7evSo6Yfla+UMaT969Kjq16+vo0eP6oEHHsh1vP9+XnLe19DQ0HzjTU1NzfNaExgYqE6dOmnkyJGaNGmSmjdvrg4dOui5557L8wcrADePpBtALmXLllWlSpVy3Yt4IzdaLzmHnZ3dTZVfm+wU1M8//6x27dqpWbNm+vDDD+Xp6amSJUtqzpw5WrhwYa76N/rjO0eXLl30+OOPa9myZVqzZo3Gjx+vcePGaenSpWrVqtVNx5lfm4ubKlWqmJYKat26tVxdXRUeHq4WLVqY3UN+5swZUy+3dPVHkkaNGpkdq3Hjxmb3Hx4/flzNmze/6Zg8PT2VlZWllJQUsz/OMzMzc8WRl5dffllz5szR4MGDFRAQIBcXF9nY2Khbt2559hgV5ue3oKKiotS8eXN99NFHtzRDdM53POcHs5yZvHfu3Fmg/b29vZWQkKDExERVrlzZVF6rVi3TD2H//aEmr89A27Ztzeo0btzYLOk+fvx4rh/lCiJnzoCTJ0/m2v/kyZNq3LjxdfcfM2aM3nnnHb344osaPXq0ypcvL1tbWw0ePPiW7jGXCn6tyUtBrj8TJ05Uz5499dVXX2nNmjV65ZVXFB0drc2bN+dKXq0h532bP3++PDw8cm3/7w8X1442uPYYbm5uWrBgQZ6v8d9E2FLfzaL4t6ygct7X8ePHm0aR/Fdeo1Kkq/92f/HFF9q8ebO++eYb05KCEydO1ObNm/PdD0DBMbwcQJ6efvppHTx4UJs2bbph3WrVqik7O9v0S3uO5ORknT9/XtWqVSvU2LKzs3MNC8xZNipnArQvv/xSjo6Opj8eWrVqZbZ+7O3w9PQ0DYU9fPiwKlSooPfee0+STG3du3dvrv327t1baO9Ffq+TmZmpw4cPF/p7frP69u2rmjVravjw4WZ/aJYtW9asl9jDwyNXz/a15/bPP//Uli1b8u0Vu56cPzx/++03s/LffvtN2dnZ+f5hmuOLL75QaGioJk6caJqEKa/J8Aoq55z893ty6tSpW+5xCwwMVPPmzTVu3LgbDpXOy/z58yXJtKxbrVq1VLt2bX311VemXvDryenNzi/5ycvNfgZSUlL01Vdf3dL3N7/PwIkTJ/T3338X6DPQokULzZ49W926dVPLli0VFBSU6zNQs2ZN7d2795aG5l7vmrFnzx65urqaLZF1vetPjoceekjDhw/X+vXr9fPPPysxMfG6tyHk99m8fPmy2eRfhaFmzZqSJDc3NwUFBeV6FOQHtpo1a+rMmTNq2rRpnsfw8fG56bgK+qNxYalWrVq+5zxne85/Dx48mCth/+++Oe9r2bJl83xPgoKCzJb/y8ujjz6q9957T7/99psWLFigP//8U4sWLbrlNgL4H5JuAHl64403VLp0afXq1UvJycm5th88eFBTpkyRdLVnU7o6U/a1YmJiJF29n7mwTZs2zfT/hmFo2rRpKlmypP7v//5P0tWeBhsbG7PhoEeOHNHy5ctv+TWzsrJyDSt2c3NTpUqVTMNXGzZsKDc3N82YMcNsSOt3332nv/76q9Dei6CgINnb2+uDDz4w+2Ns9uzZSk1Ntch7fjNKlCihV199VX/99Ze++uorU3mdOnW0ZcsW0/OOHTvq3Xff1YoVK3T06FF9+OGH+uqrr5SRkaEvv/xSwcHBeumll3INey2IJ554QuXLl881w/f06dNVqlSpG75HdnZ2uf7QnTp1ap5rThdEzh+9U6dONTvuf783Nyvn3u6CzHh/rYULF+rjjz9WQECA6XsjSSNHjtSZM2fUq1cvXblyJdd+a9as0bfffivpas9r3bp1NXr06HxnOv7ve5jXZ2DGjBlauHChjh49qs8++0wzZ85UVlaWVq9erRYtWuixxx4zi7Gg6tWrJ29vb9PxckyfPl02NjZm98nmJa/PwJIlS5SYmGhW1qlTJ50+fdrsupTjRr2bnp6e8vX11bx588yS+V27dmnNmjWm62tBrj9paWm5ztlDDz0kW1vbPIfY52jYsKEqVqyoGTNmmN2SMXfu3Fv+kSk/wcHBpnXS8/qRoiC3EXTp0kVZWVkaPXp0rm1Xrly5pZhzftgo7Pbmp3Xr1tq6davZD9vp6emaOXOmvLy8THOMtG7dWidOnNAXX3xhqnfhwoVc33c/Pz/VrFlTEyZMyPMHs+u9r+fOncv1Oc35Qep6nxsABcfwcgB5qlmzphYuXKiuXbuqTp06CgkJUf369ZWZmamNGzdqyZIl6tmzpyTJx8dHoaGhmjlzps6fP6/AwEBt3bpV8+bNU4cOHczu3ywMjo6OWrVqlUJDQ+Xv76/vvvtOK1as0FtvvWUaVtimTRvFxMToqaee0nPPPaeUlBTFxsbqgQce0B9//HFLr/vPP/+oSpUq6ty5s3x8fOTs7Ky1a9fq119/1cSJEyVdvQdy3LhxCgsLU2BgoLp3767k5GTTMmRDhgwplPegYsWKGjZsmEaOHKmnnnpK7dq10969e/Xhhx+qUaNGN72EkyX07NlTkZGRGjdunDp06CDpas/okCFDNHr0aDk5OSkyMlKbNm0y9ZhWq1ZNERERev/999WzZ08NHjw411JwR48eNfXQ5vRgvvvuu6b9c5bBcnJy0ujRozVw4EA9++yzCg4O1s8//6xPP/1U7733ntmkVXl5+umnNX/+fLm4uKhu3bratGmT1q5da3Yv782oWLGiXnvtNUVHR+vpp59W69attWPHDn333Xd53j9fUIGBgQoMDNRPP/2Ub50vvvhCzs7OyszMVGJiolavXq1ffvlFPj4+WrJkiVndrl27aufOnXrvvfe0Y8cOde/eXdWqVdOZM2e0atUqxcfHm27RKFmypJYtW6bg4GA99thjeuaZZ/T444+rdOnSSkxM1Ndff61jx46Z/cDx9NNPa+LEiTp58qQ8PT3Vr18/rV27Vj169JB09V7p119/XZGRkWrXrp1eeuklTZgwwSzG1NRUTZ06VZL0yy+/SLr6Q1y5cuVUrlw5s4kWx48fr3bt2qlly5bq1q2bdu3apWnTpqlXr143XBLq6aef1qhRoxQWFqYmTZpo586dWrBgQa77nENCQvTJJ58oIiJCW7du1eOPP6709HStXbtWAwYMUPv27a/7OuPHj1erVq0UEBCgl156SRcvXtTUqVPl4uKiESNGSCrY9eeHH35QeHi4nn32WdWqVUtXrlzR/PnzZWdnZ1pqKy8lS5bUu+++q759++qJJ55Q165ddfjwYc2ZM6fQ7+kuW7aspk+frhdeeEGPPPKIunXrpooVK+rYsWNasWKFmjZtmuePF9cKDAxU3759FR0drYSEBLVs2VIlS5bU/v37tWTJEk2ZMuWGP6j8l6+vr+zs7DRu3DilpqbKwcFBTzzxRL73jd+uoUOH6rPPPlOrVq30yiuvqHz58po3b54OHz6sL7/80jSkvnfv3po2bZpCQkK0bds2eXp6av78+SpVqpTZ8WxtbfXxxx+rVatWqlevnsLCwlS5cmUlJibqxx9/VNmyZU1L/P3XvHnz9OGHH6pjx46qWbOm/vnnH82aNUtly5Y1/egD4DZZYcZ0AMXIvn37jN69exteXl6Gvb29UaZMGaNp06bG1KlTjUuXLpnqXb582Rg5cqRRvXp1o2TJkkbVqlWNYcOGmdUxjKvLt/x3ySnDMF8KKUfO0jTXLjsVGhpqlC5d2jh48KDRsmVLo1SpUoa7u7sRFRWVa2mf2bNnGw8++KDh4OBgeHt7G3PmzDEtYXOj1752W84yLxkZGcbrr79u+Pj4GGXKlDFKly5t+Pj4GB9++GGu/RYvXmw8/PDDhoODg1G+fHmjR48ext9//21WJ6ct/5VXjPmZNm2a4e3tbZQsWdJwd3c3+vfvb5w7dy7P493MkmH51c3r/F3v/RsxYoTZsjaXL182atasaQwePNhUJzs729ixY4exefNmIzMz0zh58qSxbds2s6WLrvXjjz/mu5zZtUsb5Zg5c6ZRu3Ztw97e3qhZs6YxadIksyV18nPu3DkjLCzMcHV1NZydnY3g4GBjz549RrVq1cyW98pvuaScOK9dgigrK8sYOXKk4enpaTg5ORnNmzc3du3aleuY+cnvvb72PclrybCch6Ojo1GlShXj6aefNuLi4nJ9P68VHx9vtG/f3nBzczNKlChhVKxY0Wjbtq3x1Vdf5ap7/vx5Y9SoUcbDDz9sODs7G/b29kbVqlWNzp07G998802u+oGBgUbHjh3NzsPu3buNX375xUhPTzfOnTtnbN261UhPT88ztpxrQ16PnKXGrrVs2TLD19fXcHBwMKpUqWIMHz7cyMzMzLftOS5dumS8+uqrpvPVtGlTY9OmTUZgYGCuz9qFCxeMt99+23QN9PDwMDp37mwcPHjQLOb/LqOXY+3atUbTpk0NJycno2zZskbbtm2N3bt3m7YX5Ppz6NAh48UXXzRq1qxpODo6GuXLlzdatGhhrF279oZtNQzD+PDDD43q1asbDg4ORsOGDY3169fnauvtLhmW48cffzSCg4MNFxcXw9HR0ahZs6bRs2dP47fffjPVye8amWPmzJmGn5+f4eTkZJQpU8Z46KGHjDfeeMM4ceKEqU5+/+bkdQ5nzZpl1KhRw7Czs7vh8mH5xRYYGGjUq1cvV3lecRw8eNDo3LmzUa5cOcPR0dFo3Lix8e233+ba9+jRo0a7du2MUqVKGa6ursagQYNMy6P9N8YdO3YYzzzzjFGhQgXDwcHBqFatmtGlSxcjPj7eVOe/S4Zt377d6N69u3H//fcbDg4Ohpubm/H000+bnQsAt8fGMCw4qwMAFLKePXvqiy++KND9prgz/fLLL2rRooWGDh2qESNG5JokSZLOnj2r3377TS1btrRChLC0/fv3q1GjRurUqZOmT5+e59JuFy9e1Pfff6927dpZIUIAAAoPSTeAYoWk++7wzTffqHv37qpRo4YGDBigJk2a6L777tOJEye0atUqxcbGqkaNGtq4cWOeSTmKvy1btqhdu3YqXbq0wsPDFRgYKDc3N50+fVo//PCDPvjgA9nZ2emPP/5g9mQAQLFG0g2gWCHpvnscPnxYUVFRWrZsmdn5rFKlisLDwzVo0KB81wXH3eHUqVMaNWqUFixYYDaDu6urq3r16qWhQ4ded31pAACKA5JuAMUKSffdJyMjQ3v37tX58+fl7u6u2rVrWzskFLGsrCzt3btXp0+fVoUKFeTt7X3XrGEPAABJNwAAAAAAFsKNcgAAAAAAWAhJNwAAAAAAFlLC2gEUtezsbJ04cUJlypSRjY2NtcMBAAAAABRDhmHon3/+UaVKla672so9l3SfOHFCVatWtXYYAAAAAIC7wPHjx1WlSpV8t99zSXeZMmUkXX1jypYta+VogJs3a9YsffDBB0pOTlb9+vU1fvx4+fn55Vn38uXLiomJ0cKFC3Xy5Ek9+OCDGjlypIKCgkx1srKyFB0drcWLFyslJUUeHh7q0aOHXn/9dUaDAAAAAPlIS0tT1apVTTlmfu65pDsniShbtixJN4qdxYsX66233tKMGTPk7++vyZMn65lnntHevXvl5uaWq/6bb76pTz/9VLNmzZK3t7dWr16tHj16aOPGjXr44YclSWPGjFFcXJzmzZunevXq6bffflNYWJjc3Nz0yiuvFHUTAQAAgGLlRh1V99ySYWlpaXJxcVFqaipJN4odf39/NWrUSNOmTZN0dY6CqlWr6uWXX9bQoUNz1a9UqZLefvttDRw40FTWqVMnOTk56dNPP5UkPf3003J3d9fs2bPzrQMAAADAXEFzS2YvB4qJzMxMbdu2zWxouK2trYKCgrRp06Y898nIyJCjo6NZmZOTkzZs2GB63qRJE8XHx2vfvn2SpN9//10bNmxQq1atLNAKAAAA4N5yzw0vB4qr06dPKysrS+7u7mbl7u7u2rNnT577BAcHKyYmRs2aNVPNmjUVHx+vpUuXKisry1Rn6NChSktLk7e3t+zs7JSVlaX33ntPPXr0sGh7AAAAgHsBSTdwF5syZYp69+4tb29v2djYqGbNmgoLC1NcXJypzueff64FCxZo4cKFqlevnhISEjR48GBVqlRJoaGhVoweAADgzpeVlaXLly9bOwxYQMmSJWVnZ3fbxyHpBooJV1dX2dnZKTk52aw8OTlZHh4eee5TsWJFLV++XJcuXdKZM2dUqVIlDR06VDVq1DDVef311zV06FB169ZNkvTQQw/p6NGjio6OJukGrCA2Nlbjx49XUlKSfHx8NHXqVDVu3DjPupcvX1Z0dLTmzZunxMRE1a5dW+PGjdNTTz1lquPl5aWjR4/m2nfAgAGKjY21WDsA4G5nGIaSkpJ0/vx5a4cCCypXrpw8PDxua1Ufqybd69ev1/jx47Vt2zadPHlSy5YtU4cOHa67z7p16xQREaE///xTVatW1fDhw9WzZ88iiRewJnt7e/n5+Sk+Pt70PcnOzlZ8fLzCw8Ovu6+jo6MqV66sy5cv68svv1SXLl1M2y5cuCBbW/PpHezs7JSdnV3obQBwfYsXL1ZERITZCgXBwcH5rlAwfPjwXCsUdOzY0WyFgl9//dXslpJdu3bpySef1LPPPltk7QKAu1FOwu3m5qZSpUqx1OpdxjAMXbhwQSkpKZIkT0/PWz6WVZPu9PR0+fj46MUXX9Qzzzxzw/qHDx9WmzZt1K9fPy1YsEDx8fHq1auXPD09FRwcXAQRA9YVERGh0NBQNWzYUI0bN9bkyZOVnp6usLAwSVJISIgqV66s6OhoSdKWLVuUmJgoX19fJSYmasSIEcrOztYbb7xhOmbbtm313nvv6f7771e9evW0Y8cOxcTE6MUXX7RKG4F7WUxMjHr37m36Ts+YMUMrVqxQXFxcnisUzJ8/X2+//bZat24tSerfv7/Wrl2riRMnmlYfqFixotk+Y8eOVc2aNRUYGGjh1gDA3SsrK8uUcFeoUMHa4cBCnJycJEkpKSlyc3O75aHmVk26W7VqdVMzJM+YMUPVq1fXxIkTJUl16tTRhg0bNGnSJJJu3BO6du2qU6dOKTIyUklJSfL19dWqVatMk6sdO3bMrNf60qVLGj58uA4dOiRnZ2e1bt1a8+fPV7ly5Ux1pk6dqnfeeUcDBgxQSkqKKlWqpL59+yoyMrKomwfc03JWKBg2bJiprDBWKPjva3z66aeKiIigRwYAbkPOPdylSpWyciSwtJxzfPny5eKZdN+sTZs2mS2XJF2dnXnw4MHWCQiwgvDw8HyHk69bt87seWBgoHbv3n3d45UpU0aTJ0/W5MmTCylCALfCUisUXGv58uU6f/48t2UBQCHhB8y7X2Gc42K1TndSUlKef4ykpaXp4sWLee6TkZGhtLQ0swcAFCexsbHy8vKSo6Oj/P39tXXr1nzrXr58WaNGjVLNmjXl6OgoHx8frVq1yqzOiBEjZGNjY/bw9va2dDNgAVOmTNGDDz4ob29v2dvbKzw8XGFhYbnmacgxe/ZstWrVSpUqVSriSAEAuHcVq6T7VkRHR8vFxcX0qFq1qrVDAoACy5lYKyoqStu3b5ePj4+Cg4NNk3r81/Dhw/XRRx9p6tSp2r17t/r166eOHTtqx44dZvXq1aunkydPmh75DUdG0bmdFQrS09N19OhR7dmzR87OzmYrFOQ4evSo1q5dq169elkkfgAAkLdiNbzcw8Mjzz9GypYta7rJ/b+GDRumiIgI0/O0tDQSbwDFhiUm1pKkEiVK5JvIwTostUJBjjlz5sjNzU1t2rSxRPgAAEleQ1cU6esdGXvz1/SePXvq/PnzWr58uVn5unXr1KJFC507d04JCQlq0aKFaZurq6saNWqkcePG6aGHHsp1zMuXL2vOnDn6/PPP9ddffykrK0s1atTQM888owEDBuS6933p0qWaMWOGtm3bprNnz2rHjh3y9fU1q3Pp0iW9+uqrWrRokTIyMhQcHKwPP/ww18jn4qBY9XQHBAQoPj7erOz7779XQEBAvvs4ODiobNmyZg8AKA5yJta6di6LwppYa//+/apUqZJq1KihHj166NixY4XfANy0iIgIzZo1S/PmzdNff/2l/v3751qh4NqJ1rZs2aKlS5fq0KFD+vnnn/XUU0/lWqFAupq8z5kzR6GhoSpRolj93g4AsKK9e/fq5MmTWr16tTIyMtSmTRtlZmaa1Tl06JAeeeQRxcbGqnPnzlqyZInWrFmjwYMHKz4+XvXq1dO+ffvM9klPT9djjz2mcePG5fvaQ4YM0TfffKMlS5bop59+0okTJwq04tWdyKr/8v777786cOCA6fnhw4eVkJCg8uXL6/7779ewYcOUmJioTz75RJLUr18/TZs2TW+88YZefPFF/fDDD/r888+1YkXR/qIEAEXBUhNr+fv7a+7cuapdu7ZOnjypkSNH6vHHH9euXbtUpkwZi7YJ12eJFQokae3atTp27BhLAQIAboqbm5vKlSsnDw8PDR48WO3atdOePXvUoEEDSVJqaqqCg4PVvXt3jRw50mzSsQYNGqhLly6aNWuWWrZsqR07dui+++6TJL3wwguSpCNHjuT5uqmpqZo9e7YWLlyoJ554QtLVEVt16tTR5s2b9eijj1qw1YXPqkn3b7/9ZjZsIWcYeGhoqObOnauTJ0+a9b5Ur15dK1as0JAhQzRlyhRVqVJFH3/8McuFAcD/N2XKFPXu3Vve3t6ysbFRzZo1FRYWpri4OFOda5dqbNCggfz9/VWtWjV9/vnneumll6wRNq5R2CsUSFLLli1lGEZhhAcAuAelpqZq0aJFkq7eDpVj7Nix8vPz06hRo3T+/HkNHDhQ8fHxqlGjhrp166bvvvtO3333ndavX6/Jkydr5MiRBXq9bdu26fLly2aj/by9vXX//fdr06ZNJN03o3nz5tf9I2Du3Ll57vPfCYGAu0VR3wd0O27lHiLcnNuZWOvSpUs6c+aMKlWqpKFDh+Y5sVaOcuXKqVatWmYjjwAAwN3r22+/lbOzs1lZXstNVqlSRdLV4eCS1K5dO7MVT+bPn29aJeXVV1/V4cOH9dVXXyklJUV9+vRR7dq1JV29j/ztt98ucNKdlJQke3v7XCO33N3dlZSUVLBG3kGK1T3dAHAvuXZirRw5E2tdby4L6X8Ta125ckVffvml2rdvn2/df//9VwcPHpSnp2ehxQ4AAO5cLVq0UEJCgtnj448/zlXv559/1rZt2zR37lzVqlVLM2bMMG07e/as/vnnH9WvX1+S9M0332jChAny9/dX27ZtzUZteXp66ty5c5Zv2B2KpBsA7mCWmFjrtdde008//aQjR45o48aN6tixo+zs7NS9e/fbjrew1xSfPn26GjRoYJoIMyAgQN99991txwkAwL2sdOnSeuCBB8welStXzlWvevXqql27tkJDQ9WrVy917drVtO3KlStmk7dmZmaqdOnSpufX9qRv375dDzzwQIHj8/DwUGZmps6fP29Wfr3Rfncykm4AuIN17dpVEyZMUGRkpHx9fZWQkJBrYq2TJ0+a6udMrFW3bl117NhRlStX1oYNG8yGZ/3999/q3r27ateurS5duqhChQravHmzKlaseFuxWmJN8SpVqmjs2LHatm2bfvvtNz3xxBNq3769/vzzz9uKFQAA3JyBAwdq165dWrZsmaSrt8FlZmaaboN77LHH9P777+vixYtKTEzUrFmzJEkbN27U22+/bbaM8434+fmpZMmSZqP99u7dq2PHjt1wtN+dyMa4x2ZWSUtLk4uLi1JTU1k+DHcc7ulGcebv769GjRpp2rRpkq4Oha9atapefvnlPNcUr1Spkt5++20NHDjQVNapUyc5OTmZrSn+X+XLl9f48eOZ9A0AYDWXLl3S4cOHVb16dbPe3rttne5z586Z/XD/5ptv6rvvvtPvv/8uGxsbhYaGysvLSyNHjtTBgwfVtm1b7d27Vy4uLgoNDdXkyZNVu3ZtjR07Vh06dDAd5+zZszp27JhOnDihNm3aaNGiRapdu7Y8PDxMPdn9+/fXypUrNXfuXJUtW1Yvv/yypKtJfFHK71xLBc8t6ekGANw2S64pniMrK0uLFi1Senp6sfyVGwCA4i48PFx//fWXlixZIkmKjIzUtGnT9N1336lmzZravXu3EhMTdfLkSY0ZM0anTp3Snj17zBJuSfr666/18MMPq02bqz8adOvWTQ8//LDZPeOTJk3S008/rU6dOqlZs2by8PDQ0qVLi6ythYmebuAOQk83iqsTJ06ocuXK2rhxo1lC/MYbb+inn37Sli1bcu3z3HPP6ffff9fy5ctNa4q3b99eWVlZysjIMNXbuXOnAgICdOnSJTk7O2vhwoVq3bp1kbSrsBWn77jE9xwA8nO93s97zZo1a9StWzc9//zz6t27t+rVqyfp6r/fEyZMUMWKFRUTE2PlKG8dPd0AgGJrypQpevDBB+Xt7S17e3uFh4crLCxMtrbm/zTVrl1bCQkJ2rJli/r376/Q0NACrU0NAAAsr2XLltq2bZv++ecfPf7447K3t5e9vb1atWqlKlWqaMSIEdYO0eqsuk43AODuYMk1xe3t7U0znvr5+enXX3/VlClT9NFHH1mmMQAA4KZUr15dc+bM0ezZs5WcnCxbW1vTpK+gpxsAUAiKak3xnONeO/wcAADcGWxtbeXp6UnC/R/0dAMACkVERIRCQ0PVsGFDNW7cWJMnT861pnjlypUVHR0t6eqa4omJifL19VViYqJGjBiRa03xYcOGqVWrVrr//vv1zz//aOHChVq3bp1Wr15tlTYCAADcLJJuAECh6Nq1q06dOqXIyEglJSXJ19c315ri196vnbOm+KFDh+Ts7KzWrVtr/vz5ZkuTpKSkKCQkRCdPnpSLi4saNGig1atX68knnyzq5gEAANwSkm4AsJK7cSbr8PBwhYeH57lt3bp1Zs8DAwNvOCHa7NmzCxwfAADAnYh7ugEAAAAAsBCSbgAAcMtiY2Pl5eUlR0dH+fv7a+vWrfnWvXz5skaNGqWaNWvK0dFRPj4+WrVq1W0dEwCAOx1JNwAAuCWLFy9WRESEoqKitH37dvn4+Cg4OFgpKSl51h8+fLg++ugjTZ06Vbt371a/fv3UsWNH7dix45aPCQDAnY6kGwAA3JKYmBj17t1bYWFhqlu3rmbMmKFSpUopLi4uz/rz58/XW2+9pdatW6tGjRrq37+/WrdurYkTJ97yMQEAuNMxkRoAALhpmZmZ2rZtm4YNG2Yqs7W1VVBQkDZt2pTnPhkZGXJ0dDQrc3Jy0oYNG275mABwRxnhUsSvl3rTu/Ts2VPz5s1TdHS0hg4daipfvny5OnbsKMMwtG7dOrVo0UKSZGNjozJlyqhGjRp68sknNWTIEHl6epodMy0tTePGjdOXX36pI0eOqFy5cqpfv74GDBigjh07ysbGRpJ04MABjRkzRmvXrlVycrJcXV3l7e2tF198UV27dlWJEubp6bFjx/TBBx/o+++/V2JiosqWLatHHnlEvXr10lNPPZWrba+88op++eUX7dq1S3Xq1FFCQkKuOn/88YcGDhyoX3/9VRUrVtTLL79stlypJdDTDQAAbtrp06eVlZVlWhIuh7u7u5KSkvLcJzg4WDExMdq/f7+ys7P1/fffa+nSpTp58uQtHxMAcPMcHR01btw4nTt37rr19u7dqxMnTujXX3/Vm2++qbVr16p+/frauXOnqc758+fVpEkTffLJJxo2bJi2b9+u9evXq2vXrnrjjTeUmnr1h4GtW7fqkUce0V9//aXY2Fjt2rVL69atU69evTR9+nT9+eefZq89f/581a9fX4mJiRoxYoTi4+P12Wef6dFHH1WfPn0UEhKirKysXDHnJPB5SUtLU8uWLVWtWjVt27ZN48eP14gRIzRz5sybfQtvCj3dAICCKepf72/XLfz6D8uaMmWKevfuLW9vb9nY2KhmzZoKCwtj6DgAFLGgoCAdOHBA0dHRev/99/Ot5+bmpnLlysnDw0O1atVS+/bt9fDDD6t///6mUUpvvfWWjhw5on379qlSpUqmfWvVqqXu3bvL0dFRhmGoZ8+eqlWrln755RfZ2v6v7/fBBx9U9+7dZRiGqeybb77R66+/rjVr1ujRRx81i8nf31/9+/dXp06dNHjwYE2dOtW07YMPPpAknTp1Sn/88Ueu9ixYsECZmZmKi4uTvb296tWrp4SEBMXExKhPnz43+S4WHD3dAADgprm6usrOzk7Jyclm5cnJyfLw8Mhzn4oVK2r58uVKT0/X0aNHtWfPHjk7O6tGjRq3fEwAwM2zs7PTmDFjNHXqVP39998F3s/JyUn9+vXTL7/8opSUFGVnZ2vRokXq0aOHWcKdw9nZWSVKlFBCQoL++usvvfbaa2YJ97VyhqBnZmYqPDxcc+fO1aOPPqoNGzaoYcOGcnd3V79+/RQSEqLly5drwYIFWrhwoQ4ePFjg+Ddt2qRmzZrJ3t7eVBYcHKy9e/fesNf/dpB0AwCAm2Zvby8/Pz/Fx8ebyrKzsxUfH6+AgIDr7uvo6KjKlSvrypUr+vLLL9W+ffvbPiYA4OZ07NhRvr6+ioqKuqn9vL29JUlHjhzR6dOnde7cOVNZfvbt2ydJql27tqksJSVFzs7OpseHH34oSfrpp59UsWJFPfXUUzp//rzat2+vNm3aaPXq1XJ1ddXChQt1+fJlVahQQa1bt9b3339f4NiTkpLyvIUpZ5ulMLwcAADckoiICIWGhqphw4Zq3LixJk+erPT0dIWFhUmSQkJCVLlyZUVHR0uStmzZosTERPn6+pru0cvOzjabwOZGxwQAFJ5x48bpiSee0GuvvVbgfXKGgdvY2JgNCb9ZFSpUME101rx5c2VmZkqSdu7cqSZNmkiSNm7cqAoVKmjkyJGSJF9fXy1evNh0DE9PT4v2UBcWkm4AAHBLunbtqlOnTikyMlJJSUny9fXVqlWrTL0Gx44dMxtGeOnSJQ0fPlyHDh2Ss7OzWrdurfnz56tcuXIFPiYAoPA0a9ZMwcHBGjZsmHr27Fmgff766y9JkpeXlypUqKBy5cppz549193nwQcflHR1YraHH35Y0tUh7g888IAkmc1afuXKFTk5OUm6OtS8dOnSZsdydnY2/f/27dvVt2/fAsUtSR4eHnnewpSzzVIYXg4AAG5ZeHi4jh49qoyMDG3ZskX+/v6mbevWrdPcuXNNzwMDA7V7925dunRJp0+f1ieffJLnPYDXOyYAoHCNHTtW33zzTYGWZrx48aJmzpypZs2aqWLFirK1tVW3bt20YMECnThxIlf9f//9V1euXNHDDz8sb29vTZgwQdnZ2dd9jQceeMA0O3qjRo20Z88effXVV8rOztZXX32l33//XRcvXtT48eN1/PhxtWvXrsBtDQgI0Pr163X58mVT2ffff6/atWvrvvvuK/BxbhZJNwAAAADcox566CH16NHDNPP3tVJSUpSUlKT9+/dr0aJFatq0qU6fPq3p06eb6rz33nuqWrWq/P399cknn2j37t3av3+/4uLi9PDDD+vff/+VjY2N5syZo71796pp06b6+uuvtX//fu3evVszZszQqVOnZGdnJ+nqzOpbtmzRvn37VLlyZcXGxqp79+6yt7fX2LFjFRwcrEGDBmnDhg2Kj4+Xg4ODKZYDBw4oISFBSUlJunjxohISEpSQkGAauv7cc8/J3t5eL730kv78808tXrxYU6ZMUUREhEXfY4aXAwAAAMA9bNSoUWb3SueoXbu2bGxsTCtNtGzZUhEREWZDscuXL6/Nmzdr7Nixevfdd3X06FHdd999euihhzR+/Hi5uFxdcvTRRx/Vtm3bNGbMGA0cOFBJSUkqXbq0fHx8NGnSJL344ouSpLJly+rNN99Uly5dFB8frxdffFHPP/+8zpw5I09PT505c0alSpUyDUG/Vq9evfTTTz+ZnucMZT98+LC8vLzk4uKiNWvWaODAgfLz85Orq6siIyMtulyYJNkYt3P3ezGUlpYmFxcXpaamqmzZstYOBzDjNXSFtUMosCNj21g7hGKvOJ1vSTri+Jy1Q7g5d+A63cXunPM9B4A8Xbp0SYcPH1b16tXl6Oho7XDuOoZhaMCAAfr2228VGRmpDh06qGLFikpPT9eqVas0evRoffzxx2rYsKHFY7neuS5obklPNwAAAADgjmFjY6Pp06erVatWev/999WvXz+VKFFCV65cUcOGDTV8+PAiSbgLC0k3AAAAAOCO065dO7Vr104XL17U6dOnVa5cOZUpU8baYd00km4AAAAAwB3LyclJVatWtXYYt4zZywEAAAAAsBB6ugEAQN5GuFg7goK7AyfOAwBAoqcbAAAAAG5Jdna2tUOAhRXGOaanGwAAAABugr29vWxtbXXixAlVrFhR9vb2srGxsXZYKESGYSgzM1OnTp2Sra2t7O3tb/lYJN0AAAAAcBNsbW1VvXp1nTx5UidOnLB2OLCgUqVK6f7775et7a0PEifpBgAAAICbZG9vr/vvv19XrlxRVlaWtcOBBdjZ2alEiRK3PYqBpBsAAAAAboGNjY1KliypkiVLWjsU3MGYSA0AAAAAAAsh6QYAAAAAwEJIugEAAAAAsBCSbgAAAAAALISkGwAAAAAACyHpBgAAAADAQki6AQAAAACwEJJuAAAAAAAshKQbAAAAAAALIekGAAAAAMBCSLoBAAAAALAQkm4AAAAAACyEpBsAAAAAAAsh6QYAAAAAwEJIugEAAAAAsBCSbgAAAAAALISkGwAAAAAACyHpBgAAAIpQbGysvLy85OjoKH9/f23dujXfus2bN5eNjU2uR5s2bUx1/v33X4WHh6tKlSpycnJS3bp1NWPGjKJoCoACIOkGAAAAisjixYsVERGhqKgobd++XT4+PgoODlZKSkqe9ZcuXaqTJ0+aHrt27ZKdnZ2effZZU52IiAitWrVKn376qf766y8NHjxY4eHh+vrrr4uqWQCug6QbAAAAKCIxMTHq3bu3wsLCTD3SpUqVUlxcXJ71y5cvLw8PD9Pj+++/V6lSpcyS7o0bNyo0NFTNmzeXl5eX+vTpIx8fn+v2oAMoOiTdAAAAQBHIzMzUtm3bFBQUZCqztbVVUFCQNm3aVKBjzJ49W926dVPp0qVNZU2aNNHXX3+txMREGYahH3/8Ufv27VPLli0LvQ0Abl4JawcAAAAA3AtOnz6trKwsubu7m5W7u7trz549N9x/69at2rVrl2bPnm1WPnXqVPXp00dVqlRRiRIlZGtrq1mzZqlZs2aFGj+AW0PSDQAAABQDs2fP1kMPPaTGjRublU+dOlWbN2/W119/rWrVqmn9+vUaOHCgKlWqZNarDsA6SLoBAACAIuDq6io7OzslJyeblScnJ8vDw+O6+6anp2vRokUaNWqUWfnFixf11ltvadmyZaYZzRs0aKCEhARNmDCBpBu4A1j9nu6bWTJBkiZPnqzatWvLyclJVatW1ZAhQ3Tp0qUiihYAAAC4Nfb29vLz81N8fLypLDs7W/Hx8QoICLjuvkuWLFFGRoaef/55s/LLly/r8uXLsrU1/7Pezs5O2dnZhRc8gFtm1Z7unCUTZsyYIX9/f02ePFnBwcHau3ev3NzcctVfuHChhg4dqri4ODVp0kT79u1Tz549ZWNjo5iYGCu0AAAAACi4iIgIhYaGqmHDhmrcuLEmT56s9PR0hYWFSZJCQkJUuXJlRUdHm+03e/ZsdejQQRUqVDArL1u2rAIDA/X666/LyclJ1apV008//aRPPvmEv4+BO4RVk+5rl0yQpBkzZmjFihWKi4vT0KFDc9XfuHGjmjZtqueee06S5OXlpe7du2vLli1FGjcAAABwK7p27apTp04pMjJSSUlJ8vX11apVq0yTqx07dixXr/XevXu1YcMGrVmzJs9jLlq0SMOGDVOPHj109uxZVatWTe+995769etn8fYAuDGrJd05SyYMGzbMVHajJROaNGmiTz/9VFu3blXjxo116NAhrVy5Ui+88EK+r5ORkaGMjAzT87S0tMJrBAAAAHCTwsPDFR4enue2devW5SqrXbu2DMPI93geHh6aM2dOYYUHoJBZLem+lSUTnnvuOZ0+fVqPPfaYDMPQlStX1K9fP7311lv5vk50dLRGjhxZqLEDAAAAAFAQVp9I7WasW7dOY8aM0Ycffqjt27dr6dKlWrFihUaPHp3vPsOGDVNqaqrpcfz48SKMGAAAAABwL7NaT/etLJnwzjvv6IUXXlCvXr0kSQ899JDS09PVp08fvf3227nuf5EkBwcHOTg4FH4DAAAAAAC4Aav1dN/KkgkXLlzIczkESde9zwUAAAAAAGuw6uzlN7tkQtu2bRUTE6OHH35Y/v7+OnDggN555x21bdvWlHwDAAAAAHCnsGrSfbNLJgwfPlw2NjYaPny4EhMTVbFiRbVt21bvvfeetZoAAAAAAEC+rJp0Sze3ZEKJEiUUFRWlqKioIogMAAAAAIDbY/WkGwAAALgbeA1dYe0QbsqRsW2sHQJwTyhWS4YBKH5iY2Pl5eUlR0dH+fv7a+vWrfnWbd68uWxsbHI92rT53x8FI0aMkLe3t0qXLq377rtPQUFB2rJlS1E0BQAAALhpJN0ALGbx4sWKiIhQVFSUtm/fLh8fHwUHByslJSXP+kuXLtXJkydNj127dsnOzk7PPvusqU6tWrU0bdo07dy5Uxs2bJCXl5datmypU6dOFVWzAAAAgAIj6QZgMTExMerdu7fCwsJUt25dzZgxQ6VKlVJcXFye9cuXLy8PDw/T4/vvv1epUqXMku7nnntOQUFBqlGjhurVq6eYmBilpaXpjz/+KKpmAQAAAAVG0g3AIjIzM7Vt2zYFBQWZymxtbRUUFKRNmzYV6BizZ89Wt27dVLp06XxfY+bMmXJxcZGPj0+hxA0AAAAUJpJuABZx+vRpZWVlmZYAzOHu7q6kpKQb7r9161bt2rVLvXr1yrXt22+/lbOzsxwdHTVp0iR9//33cnV1LbTYAQAAgMJC0g3gjjR79mw99NBDaty4ca5tLVq0UEJCgjZu3KinnnpKXbp0yfc+cQAAAMCaSLoBWISrq6vs7OyUnJxsVp6cnCwPD4/r7puenq5FixbppZdeynN76dKl9cADD+jRRx/V7NmzVaJECc2ePbvQYgcAAAAKC0k3AIuwt7eXn5+f4uPjTWXZ2dmKj49XQEDAdfddsmSJMjIy9PzzzxfotbKzs5WRkXFb8QIAAACWUMLaAQC4e0VERCg0NFQNGzZU48aNNXnyZKWnpyssLEySFBISosqVKys6Otpsv9mzZ6tDhw6qUKGCWXl6erree+89tWvXTp6enjp9+rRiY2OVmJhoNsM5AAAAcKcg6QZgMV27dtWpU6cUGRmppKQk+fr6atWqVabJ1Y4dOyZbW/MBN3v37tWGDRu0Zs2aXMezs7PTnj17NG/ePJ0+fVoVKlRQo0aN9PPPP6tevXpF0iYAAADgZpB0A7Co8PBwhYeH57lt3bp1ucpq164twzDyrO/o6KilS5cWZngAAACARXFPNwAAAAAAFkLSDQAAAACAhZB0AwAAAABgISTdAAAAAABYCEk3AAAAAAAWQtINAAAAAICFsGQYgFszwsXaEdycEanWjgAA8hQbG6vx48crKSlJPj4+mjp1qho3bpxv/fPnz+vtt9/W0qVLdfbsWVWrVk2TJ09W69atJUlZWVkaMWKEPv30UyUlJalSpUrq2bOnhg8fLhsbm6JqFgDg/yPpBgAAsJLFixcrIiJCM2bMkL+/vyZPnqzg4GDt3btXbm5uuepnZmbqySeflJubm7744gtVrlxZR48eVbly5Ux1xo0bp+nTp2vevHmqV6+efvvtN4WFhcnFxUWvvPJKEbYOACCRdAMAAFhNTEyMevfurbCwMEnSjBkztGLFCsXFxWno0KG56sfFxens2bPauHGjSpYsKUny8vIyq7Nx40a1b99ebdq0MW3/7LPPtHXrVss2BgCQJ+7pBgAAsILMzExt27ZNQUFBpjJbW1sFBQVp06ZNee7z9ddfKyAgQAMHDpS7u7vq16+vMWPGKCsry1SnSZMmio+P1759+yRJv//+uzZs2KBWrVpZtkEAgDzR0w0AAGAFp0+fVlZWltzd3c3K3d3dtWfPnjz3OXTokH744Qf16NFDK1eu1IEDBzRgwABdvnxZUVFRkqShQ4cqLS1N3t7esrOzU1ZWlt577z316NHD4m0CAORG0g0AAFBMZGdny83NTTNnzpSdnZ38/PyUmJio8ePHm5Luzz//XAsWLNDChQtVr149JSQkaPDgwapUqZJCQ0Ot3AIAuPeQdAMAAFiBq6ur7OzslJycbFaenJwsDw+PPPfx9PRUyZIlZWdnZyqrU6eOkpKSlJmZKXt7e73++usaOnSounXrJkl66KGHdPToUUVHR5N0A4AVcE83AACAFdjb28vPz0/x8fGmsuzsbMXHxysgICDPfZo2baoDBw4oOzvbVLZv3z55enrK3t5eknThwgXZ2pr/iWdnZ2e2DwCg6JB0AwAAWElERIRmzZqlefPm6a+//lL//v2Vnp5ums08JCREw4YNM9Xv37+/zp49q0GDBmnfvn1asWKFxowZo4EDB5rqtG3bVu+9955WrFihI0eOaNmyZYqJiVHHjh2LvH0AAIaXAwAAWE3Xrl116tQpRUZGKikpSb6+vlq1apVpcrVjx46Z9VpXrVpVq1ev1pAhQ9SgQQNVrlxZgwYN0ptvvmmqM3XqVL3zzjsaMGCAUlJSVKlSJfXt21eRkZFF3j4AAEk3AACAVYWHhys8PDzPbevWrctVFhAQoM2bN+d7vDJlymjy5MmaPHlyIUUIALgdDC8HAAAAAMBCSLoBAAAAALAQkm4AAAAAACyEpBsAAAAAAAsh6QYAAAAAwEJIugEAAFBgsbGx8vLykqOjo/z9/bV169br1j9//rwGDhwoT09POTg4qFatWlq5cmWedceOHSsbGxsNHjzYApEDgHWwZBgAAAAKZPHixYqIiNCMGTPk7++vyZMnKzg4WHv37pWbm1uu+pmZmXryySfl5uamL774QpUrV9bRo0dVrly5XHV//fVXffTRR2rQoEERtAQAig5JNwAAgIV4DV1h7RAK7MjYNjesExMTo969eyssLEySNGPGDK1YsUJxcXEaOnRorvpxcXE6e/asNm7cqJIlS0qSvLy8ctX7999/1aNHD82aNUvvvvvu7TUEAO4wDC8HAADADWVmZmrbtm0KCgoyldna2iooKEibNm3Kc5+vv/5aAQEBGjhwoNzd3VW/fn2NGTNGWVlZZvUGDhyoNm3amB0bAO4W9HQDAADghk6fPq2srCy5u7ublbu7u2vPnj157nPo0CH98MMP6tGjh1auXKkDBw5owIABunz5sqKioiRJixYt0vbt2/Xrr79avA0AYA0k3QAAALCI7Oxsubm5aebMmbKzs5Ofn58SExM1fvx4RUVF6fjx4xo0aJC+//57OTo6WjtcALAIkm4AAADckKurq+zs7JScnGxWnpycLA8Pjzz38fT0VMmSJWVnZ2cqq1OnjpKSkkzD1VNSUvTII4+YtmdlZWn9+vWaNm2aMjIyzPYFgOKIe7oBAABwQ/b29vLz81N8fLypLDs7W/Hx8QoICMhzn6ZNm+rAgQPKzs42le3bt0+enp6yt7fX//3f/2nnzp1KSEgwPRo2bKgePXooISGBhBvAXYGebgAAABRIRESEQkND1bBhQzVu3FiTJ09Wenq6aTbzkJAQVa5cWdHR0ZKk/v37a9q0aRo0aJBefvll7d+/X2PGjNErr7wiSSpTpozq169v9hqlS5dWhQoVcpUDQHFF0g0AAIAC6dq1q06dOqXIyEglJSXJ19dXq1atMk2uduzYMdna/m8gZdWqVbV69WoNGTJEDRo0UOXKlTVo0CC9+eab1moCABQ5hpcXc7GxsfLy8pKjo6P8/f21devW69Y/f/68Bg4cKE9PTzk4OKhWrVpauXKlafv69evVtm1bVapUSTY2Nlq+fLmFWwAAAIqT8PBwHT16VBkZGdqyZYv8/f1N29atW6e5c+ea1Q8ICNDmzZt16dIlHTx4UG+99dZ1h42vW7dOkydPtlD0AFD0SLqLscWLFysiIkJRUVHavn27fHx8FBwcrJSUlDzrZ2Zm6sknn9SRI0f0xRdfaO/evZo1a5YqV65sqpOeni4fHx/FxsYWVTMAAAAA4K7F8PJiLCYmRr179zbdRzVjxgytWLFCcXFxGjp0aK76cXFxOnv2rDZu3KiSJUtKkry8vMzqtGrVSq1atbJ47AAAAABwL6Cnu5jKWWYjKCjIVGZra6ugoCBt2rQpz32+/vprBQQEaODAgXJ3d1f9+vU1ZswYZWVlFVXYAAAAAHBPoae7mDp9+rSysrJME5fkcHd31549e/Lc59ChQ/rhhx/Uo0cPrVy5UgcOHNCAAQN0+fJlRUVFFUXYAAAAAHBPIem+h2RnZ8vNzU0zZ86UnZ2d/Pz8lJiYqPHjx5N0AwAAAIAFkHQXU66urrKzs1NycrJZeXJysjw8PPLcx9PTUyVLljSbMbROnTpKSkpSZmam7O3tLRozAAAAANxrSLqLKXt7e/n5+Sk+Pl4dOnSQdLUnOz4+XuHh4Xnu07RpUy1cuFDZ2dmmNTT37dsnT09PEm4AAO51I1ysHcHNGZFq7QgAoECYSK0Yi4iI0KxZszRv3jz99ddf6t+/v9LT002zmYeEhGjYsGGm+v3799fZs2c1aNAg7du3TytWrNCYMWM0cOBAU51///1XCQkJSkhIkCQdPnxYCQkJOnbsWJG2DQAAAADuBvR0F2Ndu3bVqVOnFBkZqaSkJPn6+mrVqlWmydWOHTtm6tGWpKpVq2r16tUaMmSIGjRooMqVK2vQoEF68803TXV+++03tWjRwvQ8IiJCkhQaGqq5c+cWTcMAAAAA4C5B0l3MhYeH5zucfN26dbnKAgICtHnz5nyP17x5cxmGUVjhAQAAAMA9jeHlAAAAAABYCEk3AAAAAAAWQtINAAAAAICFkHQDAAAAyFNsbKy8vLzk6Ogof39/bd26Nd+6c+fOlY2NjdnD0dHRrM5/t+c8xo8fb+mmAFZD0g0AAAAgl8WLFysiIkJRUVHavn27fHx8FBwcrJSUlHz3KVu2rE6ePGl6HD161Gz7tdtOnjypuLg42djYqFOnTpZuDmA1JN0AAAAAcomJiVHv3r0VFhamunXrasaMGSpVqpTi4uLy3cfGxkYeHh6mR85Stjmu3ebh4aGvvvpKLVq0UI0aNSzdHMBqrL5kWGxsrMaPH6+kpCT5+Pho6tSpaty4cb71z58/r7fffltLly7V2bNnVa1aNU2ePFmtW7cuwqiLhtfQFdYO4aYcGdvG2iEAAACgEGRmZmrbtm0aNmyYqczW1lZBQUHatGlTvvv9+++/qlatmrKzs/XII49ozJgxqlevXp51k5OTtWLFCs2bN6/Q4wfuJFbt6b7ZISuZmZl68skndeTIEX3xxRfau3evZs2apcqVKxdx5AAAAMDd6/Tp08rKysrVU+3u7q6kpKQ896ldu7bi4uL01Vdf6dNPP1V2draaNGmiv//+O8/68+bNU5kyZfTMM88UevzAncSqPd3XDlmRpBkzZmjFihWKi4vT0KFDc9WPi4vT2bNntXHjRpUsWVKS5OXlVZQhAwAAAMhDQECAAgICTM+bNGmiOnXq6KOPPtLo0aNz1Y+Li1OPHj1yTbYG3G2s1tOdM2QlKCjof8HcYMjK119/rYCAAA0cOFDu7u6qX7++xowZo6ysrHxfJyMjQ2lpaWYPAAAAAPlzdXWVnZ2dkpOTzcqTk5Pl4eFRoGOULFlSDz/8sA4cOJBr288//6y9e/eqV69ehRIvcCezWtJ9K0NWDh06pC+++EJZWVlauXKl3nnnHU2cOFHvvvtuvq8THR0tFxcX06Nq1aqF2g4AAADgbmNvby8/Pz/Fx8ebyrKzsxUfH2/Wm309WVlZ2rlzpzw9PXNtmz17tvz8/OTj41NoMQN3qmI1e3l2drbc3Nw0c+ZM+fn5qWvXrnr77bc1Y8aMfPcZNmyYUlNTTY/jx48XYcQAAABA8RQREaFZs2Zp3rx5+uuvv9S/f3+lp6ebbg0NCQkxm2ht1KhRWrNmjQ4dOqTt27fr+eef19GjR3P1ZqelpWnJkiX0cuOeYbV7um9lyIqnp6dKliwpOzs7U1mdOnWUlJSkzMxM2dvb59rHwcFBDg4OhRs8AAAAcJfr2rWrTp06pcjISCUlJcnX11erVq0yjVQ9duyYbG3/14d37tw59e7dW0lJSbrvvvvk5+enjRs3qm7dumbHXbRokQzDUPfu3Yu0PYC1WK2n+1aGrDRt2lQHDhxQdna2qWzfvn3y9PTMM+EGAAAAcOvCw8N19OhRZWRkaMuWLfL39zdtW7dunebOnWt6PmnSJFPdpKQkrVixQg8//HCuY/bp00cXLlyQi4tLUTQBsDqrDi+/2SEr/fv319mzZzVo0CDt27dPK1as0JgxYzRw4EBrNQEAAAAAgHxZdcmwmx2yUrVqVa1evVpDhgxRgwYNVLlyZQ0aNEhvvvmmtZoAAAAAAEC+rJp0S1eHrISHh+e5bd26dbnKAgICtHnzZgtHBQAAAADA7StWs5cDAAAAAFCckHQDAAAAAGAhhZJ0p6Wlafny5frrr78K43AAAAAAANwVbume7i5duqhZs2YKDw/XxYsX1bBhQx05ckSGYWjRokXq1KlTYccJAAAAoDCNKGZLdo1ItXYEwC25pZ7u9evX6/HHH5ckLVu2TIZh6Pz58/rggw/07rvvFmqAAAAAAAAUV7eUdKempqp8+fKSpFWrVqlTp04qVaqU2rRpo/379xdqgAAAAAAAFFe3lHRXrVpVmzZtUnp6ulatWqWWLVtKks6dOydHR8dCDRAAAAAAgOLqlu7pHjx4sHr06CFnZ2fdf//9at68uaSrw84feuihwowPAAAAAIBi65aS7gEDBqhx48Y6fvy4nnzySdnaXu0wr1GjBvd0AwAAAADw/91S0i1JDRs2VIMGDXT48GHVrFlTJUqUUJs2bQozNgAAAAAAirVbuqf7woULeumll1SqVCnVq1dPx44dkyS9/PLLGjt2bKEGCAAAAABAcXVLSfewYcP0+++/a926dWYTpwUFBWnx4sWFFhwAAAAAAMXZLQ0vX758uRYvXqxHH31UNjY2pvJ69erp4MGDhRYcAAAAAADF2S31dJ86dUpubm65ytPT082ScAAAAAAA7mW3lHQ3bNhQK1asMD3PSbQ//vhjBQQEFE5kAAAAAAAUc7c0vHzMmDFq1aqVdu/erStXrmjKlCnavXu3Nm7cqJ9++qmwYwQAAAAAoFi6pZ7uxx57TL///ruuXLmihx56SGvWrJGbm5s2bdokPz+/wo4RAAAAAIBi6aZ7ui9fvqy+ffvqnXfe0axZsywREwAAAAAAd4Wb7ukuWbKkvvzyS0vEAgAAAADAXeWWhpd36NBBy5cvL+RQAAAAAAC4u9zSRGoPPvigRo0apV9++UV+fn4qXbq02fZXXnmlUIIDAAAAAKA4u6Wke/bs2SpXrpy2bdumbdu2mW2zsbEh6QYAAAAAQLeYdB8+fLiw4wAAAAAA4K5zS/d0X8swDBmGURixAAAAAABwV7nlpPuTTz7RQw89JCcnJzk5OalBgwaaP39+YcYGAAAAAECxdkvDy2NiYvTOO+8oPDxcTZs2lSRt2LBB/fr10+nTpzVkyJBCDRIAAAAAgOLolpLuqVOnavr06QoJCTGVtWvXTvXq1dOIESNIugEAAAAA0C0OLz958qSaNGmSq7xJkyY6efLkbQcFAAAAAMDd4JaS7gceeECff/55rvLFixfrwQcfvO2gAAAAAAC4G9zS8PKRI0eqa9euWr9+veme7l9++UXx8fF5JuMAAAAAANyLbqmnu1OnTtqyZYtcXV21fPlyLV++XK6urtq6das6duxY2DECAAAAAFAs3VJPtyT5+fnp008/LcxYAAAAAAC4q9xST/fKlSu1evXqXOWrV6/Wd999d9tBAQAAAABwN7ilpHvo0KHKysrKVW4YhoYOHXrbQQEAAAAAcDe4paR7//79qlu3bq5yb29vHThw4LaDAgAAAADgbnBLSbeLi4sOHTqUq/zAgQMqXbr0bQcFAAAAAMDd4JaS7vbt22vw4ME6ePCgqezAgQN69dVX1a5du0ILDgAAAACA4uyWku73339fpUuXlre3t6pXr67q1avL29tbFSpU0IQJEwo7RgAAAAAAiqVbWjLMxcVFGzdu1Pfff6/ff/9dTk5O8vHx0eOPP17Y8QEAAAAAUGzdVE/3pk2b9O2330qSbGxs1LJlS7m5uWnChAnq1KmT+vTpo4yMDIsECgAAAABAcXNTSfeoUaP0559/mp7v3LlTvXv31pNPPqmhQ4fqm2++UXR0dKEHCQAAAABAcXRTSXdCQoL+7//+z/R80aJFaty4sWbNmqWIiAh98MEH+vzzzws9SAAAAAAAiqObSrrPnTsnd3d30/OffvpJrVq1Mj1v1KiRjh8/XnjRAQAAAABQjN1U0u3u7q7Dhw9LkjIzM7V9+3Y9+uijpu3//POPSpYsWbgRAgAAAABQTN1U0t26dWsNHTpUP//8s4YNG6ZSpUqZzVj+xx9/qGbNmoUeJAAAAAAAxdFNLRk2evRoPfPMMwoMDJSzs7PmzZsne3t70/a4uDi1bNmy0IMEAAAAAKA4uqmk29XVVevXr1dqaqqcnZ1lZ2dntn3JkiVydnYu1AABAAAAACiubirpzuHi4pJnefny5W8rGAAAAAAA7iY3dU83AAAAAAAoOJJuAAAAAAAshKQbAAAAAAALIelGkYqNjZWXl5ccHR3l7++vrVu35lt36dKlatiwocqVK6fSpUvL19dX8+fPN6uTnJysnj17qlKlSipVqpSeeuop7d+/39LNAAAAAIACIelGkVm8eLEiIiIUFRWl7du3y8fHR8HBwUpJScmzfvny5fX2229r06ZN+uOPPxQWFqawsDCtXr1akmQYhjp06KBDhw7pq6++0o4dO1StWjUFBQUpPT29KJsGAAAAAHki6UaRiYmJUe/evRUWFqa6detqxowZKlWqlOLi4vKs37x5c3Xs2FF16tRRzZo1NWjQIDVo0EAbNmyQJO3fv1+bN2/W9OnT1ahRI9WuXVvTp0/XxYsX9dlnnxVl0wAAAAAgTyTdKBKZmZnatm2bgoKCTGW2trYKCgrSpk2bbri/YRiKj4/X3r171axZM0lSRkaGJMnR0dHsmA4ODqbEHAAAAACsiaQbReL06dPKysqSu7u7Wbm7u7uSkpLy3S81NVXOzs6yt7dXmzZtNHXqVD355JOSJG9vb91///0aNmyYzp07p8zMTI0bN05///23Tp48adH2AAAAAEBBlLB2AMD1lClTRgkJCfr3338VHx+viIgI1ahRQ82bN1fJkiW1dOlSvfTSSypfvrzs7OwUFBSkVq1ayTAMa4cOAAAAACTdKBqurq6ys7NTcnKyWXlycrI8PDzy3c/W1lYPPPCAJMnX11d//fWXoqOj1bx5c0mSn5+fEhISlJqaqszMTFWsWFH+/v5q2LChxdoCAAAAAAXF8HIUCXt7e/n5+Sk+Pt5Ulp2drfj4eAUEBBT4ONnZ2aZ7ua/l4uKiihUrav/+/frtt9/Uvn37QokbAAAAAG4HPd0oMhEREQoNDVXDhg3VuHFjTZ48Wenp6QoLC5MkhYSEqHLlyoqOjpYkRUdHq2HDhqpZs6YyMjK0cuVKzZ8/X9OnTzcdc8mSJapYsaLuv/9+7dy5U4MGDVKHDh3UsmVLq7QRAAAAAK5F0o0i07VrV506dUqRkZFKSkqSr6+vVq1aZZpc7dixY7K1/d/gi/T0dA0YMEB///23nJyc5O3trU8//VRdu3Y11Tl58qQiIiKUnJwsT09PhYSE6J133inytgEAAABAXu6I4eWxsbHy8vKSo6Oj/P39tXXr1gLtt2jRItnY2KhDhw6WDRCFJjw8XEePHlVGRoa2bNkif39/07Z169Zp7ty5pufvvvuu9u/fr4sXL+rs2bPauHGjWcItSa+88oqOHz+uzMxMHT16VKNHj5a9vX1RNQcAAAAArsvqSffixYsVERGhqKgobd++XT4+PgoODlZKSsp19zty5Ihee+01Pf7440UUKQAAAAAAN8fqSXdMTIx69+6tsLAw1a1bVzNmzFCpUqUUFxeX7z5ZWVnq0aOHRo4cqRo1ahRhtAAAAAAAFJxVk+7MzExt27ZNQUFBpjJbW1sFBQVp06ZN+e43atQoubm56aWXXrrha2RkZCgtLc3sAQAAAABAUbBq0n369GllZWWZJtLK4e7urqSkpDz32bBhg2bPnq1Zs2YV6DWio6Pl4uJielStWvW24wYAAAAAoCCsPrz8Zvzzzz964YUXNGvWLLm6uhZon2HDhik1NdX0OH78uIWjBAAAAADgKqsuGebq6io7OzslJyeblScnJ8vDwyNX/YMHD+rIkSNq27atqSw7O1uSVKJECe3du1c1a9Y028fBwUEODg4WiB4AAAAAgOuzatJtb28vPz8/xcfHm5b9ys7OVnx8vMLDw3PV9/b21s6dO83Khg8frn/++UdTpkxh6Li1jXCxdgQFNyLV2hEAAAAAuAdYNemWpIiICIWGhqphw4Zq3LixJk+erPT0dIWFhUmSQkJCVLlyZUVHR8vR0VH169c3279cuXKSlKscAAAAAABrs3rS3bVrV506dUqRkZFKSkqSr6+vVq1aZZpc7dixY7K1LVa3ngMAAAAAIOkOSLolKTw8PM/h5JK0bt266+47d+7cwg8IAAAAAIBCQBcyAAAAAAAWQtINAAAAAICFkHQDAAAAAGAhJN0AAAAAAFgISTcAAAAAABZC0g0AAAAAgIWQdAMAAAAAYCEk3QAAAAAAWAhJNwAAAAAAFkLSDQAAAACAhZB0AwAAAABgISTdAAAAAABYCEk3AAAAAAAWQtINAAAAAICFkHQDAAAAAGAhJN0AAAAAAFgISTcAAAAAABZC0g0AAAAAgIWQdAMAAAAAYCEk3QAAAAAAWAhJNwAAAAAAFkLSDQAAAACAhZB0AwAAAABgISTdAAAAAABYCEk3AAAAAAAWQtINAAAAAICFkHQDAAAAAGAhJN0AAAAAAFgISTcAAAAAABZC0g0AAAAAgIWQdAMAAAAAYCEk3QAAAAAAWAhJNwAAAAAAFkLSDQAAAACAhZB0AwAAAABgISTdAAAAAABYCEk3AAAAAAAWQtINAAAAAICFkHQDAAAAAGAhJN0AAAAAYCGxsbHy8vKSo6Oj/P39tXXr1nzrzpo1S48//rjuu+8+3XfffQoKCspVf8SIEfL29lbp0qVNdbZs2WLpZuA2kHQDAAAAgAUsXrxYERERioqK0vbt2+Xj46Pg4GClpKTkWX/dunXq3r27fvzxR23atElVq1ZVy5YtlZiYaKpTq1YtTZs2TTt37tSGDRvk5eWlli1b6tSpU0XVLNwkkm4AAAAAsICYmBj17t1bYWFhqlu3rmbMmKFSpUopLi4uz/oLFizQgAED5OvrK29vb3388cfKzs5WfHy8qc5zzz2noKAg1ahRQ/Xq1VNMTIzS0tL0xx9/FFWzcJNIugEAAACgkGVmZmrbtm0KCgoyldna2iooKEibNm0q0DEuXLigy5cvq3z58vm+xsyZM+Xi4iIfH59CiRuFj6QbAAAAAArZ6dOnlZWVJXd3d7Nyd3d3JSUlFegYb775pipVqmSWuEvSt99+K2dnZzk6OmrSpEn6/vvv5erqWmixo3CRdAMAAADAHWbs2LFatGiRli1bJkdHR7NtLVq0UEJCgjZu3KinnnpKXbp0yfc+cVgfSTcAAAAAFDJXV1fZ2dkpOTnZrDw5OVkeHh7X3XfChAkaO3as1qxZowYNGuTaXrp0aT3wwAN69NFHNXv2bJUoUUKzZ88u1PhReEi6AQAAAKCQ2dvby8/Pz2wStJxJ0QICAvLd7/3339fo0aO1atUqNWzYsECvlZ2drYyMjNuOGZZRwtoBAAAAAMDdKCIiQqGhoWrYsKEaN26syZMnKz09XWFhYZKkkJAQVa5cWdHR0ZKkcePGKTIyUgsXLpSXl5fp3m9nZ2c5OzsrPT1d7733ntq1aydPT0+dPn1asbGxSkxM1LPPPmu1duL6SLoBAAAAwAK6du2qU6dOKTIyUklJSfL19dWqVatMk6sdO3ZMtrb/G3w8ffp0ZWZmqnPnzmbHiYqK0ogRI2RnZ6c9e/Zo3rx5On36tCpUqKBGjRrp559/Vr169Yq0bSg4km4AAAAAsJDw8HCFh4fnuW3dunVmz48cOXLdYzk6Omrp0qWFFBmKCvd0AwAAAABgISTdAAAAAABYCEk3AAAAAAAWQtINAAAAAICFkHQDAAAAAGAhJN0AAAAAAFgIS4YBAAAAwC3wGrrC2iHclCNj21g7hHsSPd0AAAAAAFgISTcAAAAAABZC0g0AAAAAgIXcEUl3bGysvLy85OjoKH9/f23dujXfurNmzdLjjz+u++67T/fdd5+CgoKuWx8AAAAAAGuxetK9ePFiRUREKCoqStu3b5ePj4+Cg4OVkpKSZ/1169ape/fu+vHHH7Vp0yZVrVpVLVu2VGJiYhFHDgAAAADA9Vk96Y6JiVHv3r0VFhamunXrasaMGSpVqpTi4uLyrL9gwQINGDBAvr6+8vb21scff6zs7GzFx8cXceQAAAAAAFyfVZPuzMxMbdu2TUFBQaYyW1tbBQUFadOmTQU6xoULF3T58mWVL1/eUmECAAAAAHBLrLpO9+nTp5WVlSV3d3ezcnd3d+3Zs6dAx3jzzTdVqVIls8T9WhkZGcrIyDA9T0tLu/WAAQAAAAC4CVYfXn47xo4dq0WLFmnZsmVydHTMs050dLRcXFxMj6pVqxZxlAAAAACAe5VVk25XV1fZ2dkpOTnZrDw5OVkeHh7X3XfChAkaO3as1qxZowYNGuRbb9iwYUpNTTU9jh8/XiixAwAAAABwI1ZNuu3t7eXn52c2CVrOpGgBAQH57vf+++9r9OjRWrVqlRo2bHjd13BwcFDZsmXNHgAAAAAAFAWr3tMtSREREQoNDVXDhg3VuHFjTZ48Wenp6QoLC5MkhYSEqHLlyoqOjpYkjRs3TpGRkVq4cKG8vLyUlJQkSXJ2dpazs7PV2gEAAAAAwH9ZPenu2rWrTp06pcjISCUlJcnX11erVq0yTa527Ngx2dr+r0N++vTpyszMVOfOnc2OExUVpREjRhRl6AAAAAAAXJfVk25JCg8PV3h4eJ7b1q1bZ/b8yJEjlg8IAAAAAIBCUKxnLwcAAAAA4E5G0g0AAAAAgIWQdAMAAAAAYCEk3QAAAAAAWAhJNwAAAAAAFkLSDQAAAACAhZB0AwAAAABgISTdAAAAAABYCEk3AAAAAAAWQtINAAAAAJAkxcbGysvLS46OjvL399fWrVvzrfvnn3+qU6dO8vLyko2NjSZPnpyrzvr169W2bVtVqlRJNjY2Wr58ueWCv0ORdAMAAAAAtHjxYkVERCgqKkrbt2+Xj4+PgoODlZKSkmf9CxcuqEaNGho7dqw8PDzyrJOeni4fHx/FxsZaMvQ7WglrBwAAAAAAsL6YmBj17t1bYWFhkqQZM2ZoxYoViouL09ChQ3PVb9SokRo1aiRJeW6XpFatWqlVq1aWC7oYoKcbAAAAAO5xmZmZ2rZtm4KCgkxltra2CgoK0qZNm6wYWfFH0g0AAAAA97jTp08rKytL7u7uZuXu7u5KSkqyUlR3B5JuAAAAAAAshKQbAAAAAO5xrq6usrOzU3Jysll5cnJyvpOkoWBIugEAAADgHmdvby8/Pz/Fx8ebyrKzsxUfH6+AgAArRlb8MXs5AAAAAEAREREKDQ1Vw4YN1bhxY02ePFnp6emm2cxDQkJUuXJlRUdHS7o6+dru3btN/5+YmKiEhAQ5OzvrgQcekCT9+++/OnDggOk1Dh8+rISEBJUvX173339/EbfQOki6AQAAAADq2rWrTp06pcjISCUlJcnX11erVq0yTa527Ngx2dr+b7D0iRMn9PDDD5ueT5gwQRMmTFBgYKDWrVsnSfrtt9/UokULU52IiAhJUmhoqObOnWv5Rt0BSLoBAAAAAJKk8PBwhYeH57ktJ5HO4eXlJcMwrnu85s2b37DO3Y57ugEAAAAAsBCSbgAAAAAALISkGwAAAAAACyHpBgAAAADAQki6AQAAAACwEJJuAAAAAAAshCXDAAAAAOBeMMLF2hEU3IhUa0dQaOjpBgAAAADAQki6AQAAAACwEJJuAAAAAAAshKQbAAAAAAALIekGAAAAAMBCSLoBAAAAALAQkm4AAAAAACyEpBsAAAAAAAsh6QYAAAAAwEJIugEAAAAAsBCSbgAAAAAALISkGwAAAAAACyHpBgAAAADAQki6AQAAAACwEJJuAAAAAAAshKQbAAAAAAALIekGAAAAAMBCSLoBAAAAALAQkm4AAAAAACyEpBsAAAAAAAsh6QYAAAAAwEJIugEAAAAAsBCSbgAAAAAALISkGwAAAAAACyHpBgAAAADAQki6AQAAAACwEJJuAAAAAAAshKQbAAAAAAALIekGAAAAAMBCSLoBAAAAALAQkm4AAAAAACyEpBsAAAAAAAsh6QYAAAAAwEJIugEAAAAAsJA7IumOjY2Vl5eXHB0d5e/vr61bt163/pIlS+Tt7S1HR0c99NBDWrlyZRFFCgAAAABAwVk96V68eLEiIiIUFRWl7du3y8fHR8HBwUpJScmz/saNG9W9e3e99NJL2rFjhzp06KAOHTpo165dRRw5AAAAAADXZ/WkOyYmRr1791ZYWJjq1q2rGTNmqFSpUoqLi8uz/pQpU/TUU0/p9ddfV506dTR69Gg98sgjmjZtWhFHDgAAAADA9Vk16c7MzNS2bdsUFBRkKrO1tVVQUJA2bdqU5z6bNm0yqy9JwcHB+dYHAAAAAMBaSljzxU+fPq2srCy5u7ublbu7u2vPnj157pOUlJRn/aSkpDzrZ2RkKCMjw/Q8NTVVkpSWlnY7oReJ7IwL1g7hpqTZGNYOoeDu0PNfnM55sTrf0h15zovT+ZY454WBc25Bd+D5lorXOS9W51u6I895cTrfEue8MHDOLegOPN//lZNTGsb131erJt1FITo6WiNHjsxVXrVqVStEc3dzsXYAN2NssYr2jlTs3kHO+W0rdu8g5/y2Fat3kPN924rdO8g5v23F7h3knN+2YvUOFqPz/c8//8jFJf94rZp0u7q6ys7OTsnJyWblycnJ8vDwyHMfDw+Pm6o/bNgwRUREmJ5nZ2fr7NmzqlChgmxsbG6zBciRlpamqlWr6vjx4ypbtqy1w4GFcb7vPZzzew/n/N7C+b73cM7vPZzzwmcYhv755x9VqlTpuvWsmnTb29vLz89P8fHx6tChg6SrSXF8fLzCw8Pz3CcgIEDx8fEaPHiwqez7779XQEBAnvUdHBzk4OBgVlauXLnCCB95KFu2LF/iewjn+97DOb/3cM7vLZzvew/n/N7DOS9c1+vhzmH14eUREREKDQ1Vw4YN1bhxY02ePFnp6ekKCwuTJIWEhKhy5cqKjo6WJA0aNEiBgYGaOHGi2rRpo0WLFum3337TzJkzrdkMAAAAAABysXrS3bVrV506dUqRkZFKSkqSr6+vVq1aZZos7dixY7K1/d8k602aNNHChQs1fPhwvfXWW3rwwQe1fPly1a9f31pNAAAAAAAgT1ZPuiUpPDw83+Hk69aty1X27LPP6tlnn7VwVLgZDg4OioqKyjWUH3cnzve9h3N+7+Gc31s43/cezvm9h3NuPTbGjeY3BwAAAAAAt8T2xlUAAAAAAMCtIOkGAAAAAMBCSLoBAAAAALAQkm4AZpjmAbi7ZWdnWzsEAADuKSTdyFdCQoJSUlKsHQaKyJ9//ilJsrGxkcQf5ne7HTt26NChQ2Zl/OBy9xo/frz++OMPnTp1ymwZTgAAYHn8y4s8xcbGqn379kpNTbV2KCgCH374oXr27Kn27dvrs88+0/nz5/nD/C72+eefKzw8XC1atNAbb7yhH374QdLVH1z4seXuk5qaqm+//VajRo1Sw4YN9dlnn2n//v3WDgsAUAj27Nmj1atX8+/3HY4lw5DLzJkzFR4eroULF6pz587WDgdFICkpSSVKlNBbb72lo0eP6ujRo5ozZ44CAgKsHRos5J9//tGOHTsUFRWlrKws1apVSx9//LGkq6Mc+NHl7nPmzBl9+OGHWrBggerWrauQkBB16NDB2mHBwgzDMP2gxvf67pZzrg8ePCjDMPTAAw9YOyRY2O+//66HH35YEydO1JAhQ6wdDq6Dqy/MzJo1SwMHDtTixYvNEu4///xTWVlZVowMlpDzq6irq6tcXV01Y8YMxcTEyN/fX0888YTmzJmjS5cuWTlKFKac31lLly6tZs2a6bPPPtOLL76o+Ph4PfHEE5IkW1tbfjG/C1WoUEHvvPOOYmNj5eTkpJEjR2r+/PnWDgsWlJOErV27VuPHj9fp06etHRIsJOdcL1u2TG3atNF3332nEydOWDssWFBCQoKaNGmit956K1fCzb/hdx6SbpgsWrRIffv21dy5c9WxY0dTeXBwsN555x1duXLFitHBEnIuyiVKlJB0NdmqV6+e5s2bpyFDhqhPnz5aunSpWV0UXxcuXFBaWpokmXq8PDw81KNHD82fP1+HDx/WU089ZdrOQKjiLTExUX///bfpnOf4v//7P7355pvy9/dXbGys1q5da6UIYUk5SdiXX36pZ599VklJSTp//ry1w4KF2NjYaM2aNerRo4fCw8PVtWtXVapUydphwUL27Nmjxo0bKzIyUu+++66pfMWKFcrKymJUyx2IMwKTX375RWXKlFFWVpYyMzMlSZ07d1ZSUpImTpwoBwcHK0eIwrRs2TL1799fXbt21ahRo0z37+ck12PGjNGQIUPUt29fHT58mCSsmFuwYIGeeeYZ+fr66qWXXtLcuXNN20qWLKmmTZvq448/1pEjRzRq1ChJ/5tUD8XP3Llz1aJFCzVr1kzVq1fXu+++q507d5q2N2jQQH369FHFihX11VdfKT09ne/3XcbGxkZbtmxRr169NHnyZE2aNMk03PjSpUumaz3nvfjLzs7W5cuXNXv2bPXp00fh4eFyc3MzbcPd5eLFi4qOjpatra1at25tKo+OjlbPnj21d+9eK0aH/JB0w2Tq1Knq3r273n33XX322Wdq37699u/fr6+++krVq1c3+4c5OTnZipHids2fP1/du3eXs7Oz/v33Xy1fvlwPPfSQduzYYTa0eMyYMWrRooVefvllXbhwgSSsmFqyZIl69eqlZs2aacCAAUpOTtb777+v8PBwUx0bGxs9+uij6tatm3799VedOnXKihHjdqxevVoDBw7Ua6+9poULF+q1117TkiVLFBkZqZ9++slU75FHHtELL7yg+fPn648//uD7fRfas2ePAgICFBoaqrS0NH355Zfq0KGDnn76acXExCgjI4PzfhewtbVVyZIldfjwYbm6ukqS6ZbAnB7PI0eOWCs8FDInJyeFhISoTZs26tmzp44cOaIPP/xQEyZM0Keffqq6detaO0TkxcA9LSUlxTh+/LiRmJhoKuvdu7dRunRpo3LlysYff/xhGIZhXLlyxbS9S5cuxjvvvFPksaJwpKWlGU2aNDGio6NNZQcOHDDat29vVKhQwfjll18Mw/jfOf/mm2+MFi1aGPv37zcMwzCys7OLPmjcssuXLxthYWHGG2+8YSpLSkoypk2bZlSqVMno06ePWf2DBw8anp6exsyZM4s6VNymnO/m8OHDjQ4dOphtW758uREUFGS0bt3a2Lp1q9m28PBw49lnnzUyMjKKLFYUjenTpxs2NjbGnDlzjMDAQKNNmzbGCy+8YISGhhq1atUy9uzZY+0QUQhyvvtNmzY12rVrZyrPysoyDMMwjh07ZkyYMME4cuSIVeJD4Th79qzx+++/GwcOHDAMwzB++ukno0OHDkalSpUMR0dH49dff7VyhLgeerrvYV9++aX69++v3r17mw1FmTlzpl588UWVKFFCW7Zs0blz52RnZyfDMNSmTRtt2bJF77zzjhUjx+3IyspSSkqKqlevbiqrWbOmFi5cqObNm6tjx45KTk6WnZ2dJOnpp59WRkaGZs2aJYkhx8WNra2tDhw4oKNHj5rK3N3d9cILLygyMlI//PCDxo8fL+nqMNMaNWronXfe0fr165lEr5jJ+W7a2dnpxIkT+vfff03b2rdvr8GDBys1NVULFizQxYsXTfN0PPHEE0pLS+O7XcwZeQwT79evn1566SVNmjRJtWrV0vDhw/XJJ59o/PjxKlGihNLT060QKW5Xzrk+e/as0tLSTKMP33jjDW3dulVDhw6V9L9e7mnTpmnBggUqXbq0dQLGbdu9e7c6d+6sl19+WQsWLFBGRoaaNWumQYMGyd/fX1WqVDHNz5PXtQDWV8LaAcA64uLi9NZbbyk6Olo1atRQYGCgJGnfvn2qVauWPvjgA124cEFjx46VJD377LN64YUXdODAAe3fv18lS5bUlStXTF9w3PmM/z+pTrly5XT//fdr/vz56tq1q6Sr93yVKlVKsbGxeuaZZ/TKK69o4cKFkq7+AT9s2DDt3r1bly9fVsmSJa3ZDBSA8Z8lgp5++mmtXLlSf/zxhxo0aCBJKlu2rJ555hn98ccfio+P18CBA+Xk5CRJqlOnjg4fPmz64QXFy4MPPqgPPvhA27ZtU2BgoLKysmRnZ6c2bdroxIkTGjRokMLDw03393bs2FGbNm3S4cOHVatWLStHj1uR853fvHmzfvnlFxmGoQYNGqhly5aaNWuWTp48KU9PT1P9SZMmydbWVlWqVLFi1LgVOef6m2++0QcffKATJ07Iw8NDXbp0Ud++fbVnzx5NnTpVO3bskLe3t5KTk7Vq1SqtW7fONPQcxcuff/6pwMBA9evXT926dVP9+vVN25o3by5J+uCDD/Tiiy9q+vTp8vf3N31OcAexUg87rOirr74yXFxcjIULF5qV9+7d2wgODjZ++OEHU1mvXr2M2rVrGzVq1DC8vb2NzMxMwzCuDllF8fXJJ58YjzzyiDFhwgTT8LOc/44dO9Zo0KCBkZaWZqp/4sQJhi0VIznfz5whhxs2bDCqVatmhIeHGydOnDCr++OPPxo2NjbGjh07zMp3795dJLHi9qWnpxvnz583K+vYsaNRqVIl0zDEa28RqlKlivHpp5+alaemphZRtChsOd/zL7/80ihXrpzRpk0bo1GjRkZAQIAxZswYs7pLly41Bg4caJQvX97Yvn27NcJFIfj2228NR0dHY9KkSUZ8fLzxxhtvGDY2NkZCQoKRmppq/Pjjj0a7du2MNm3aGC+++KLx559/Wjtk3KKUlBTDz8/P6Nevn1l5VlaW2e1+P/74o9GhQwejcePGxoYNG4o6TBQA3ZT3EMMwlJmZqU8//VQvvfSSqZdTklq1aqXff/9dnp6emjFjhmxsbNS8eXPNmjVLISEh2rdvn37++Wd6uIuhtWvXatu2bcrIyFBgYKACAwP1zDPPaMOGDfr8889VokQJDRgwwNSDXbduXdnY2OjixYsqU6aMJMnT09OslwR3rmXLlmnlypVKS0tT3bp19corr6hp06aaOHGiunTpIltbW4WHh+vBBx+UJLm5ucnn/7V353E1pv//wF9nabVlLctkF1lSlrFEGNvYGtnGEFLJ0oQhhRjDNLKUkq1EluxiyIchS8KgsgxjsoRsU/Y12k7X7w+/c386mM93xuB01+v5eHiM7vs+x3Xm7pxzve/rul63jY00yi3+/9XxevXq6fxMBdO6deuwdu1aXLp0CR06dEDLli3h5uaG1atXo1u3bmjXrh127NgBOzs7AEB6ejqKFSuG0qVLA4A0m6FkyZIAeL7lSKFQ4NixY/Dy8sKcOXMwYsQIJCUloWPHjrhx4wYyMjLw448/Ijc3F1euXMGVK1cQHx+P+vXr67vp9B6ys7Oxfv16TJs2DePGjcOff/4JV1dXjBgxAjY2NgBej35qR0C1M11InpKTk5GTkwN3d3ed7dqlA9rz265dOxgaGsLPzw9Tp07FL7/8AiMjI36eFyT6rfnpU3vy5ImwsLAQERERQojXV8ouXbokPv/8c/H06VOxf/9+0aZNG/HVV1/pjHhrr6ZxhFteVqxYIYoXLy7atWsnmjRpIhQKhRg3bpy4f/++ePbsmRg2bJj4/PPPhZubm7hz5464cOGC6Nq1q+jRowcD02RozZo1wsjISIwbN05069ZN2NraisqVK4ukpCQhhBCbN28WZcuWFU5OTiIoKEjExcWJTp06iRYtWkgzHUg+Nm/eLIyNjYW/v7+YO3eu6N69u6hbt67w9PQUQgjx8OFD0bVrV2FmZibGjh0rfvzxR9G5c2fRuHFjnZFvkhdfX18RFxens23u3LnC1dVVCCFEamqqqFGjhvjmm2/Ed999J8qXLy/mzJkjHfvmrAiSj7y8PPHy5UthbW0toqOjxf3790XlypV1AjEjIyOlQFTtY0i+Fi1aJCwsLHRmH74pMzNT/Pbbb0IIIeLj48WtW7c+VfPoH2DRXcQ8evRIlC5dWoSEhAgh/vthnJmZKR2zY8cOUaZMGWn6oRY75fJy9epVUatWLbFu3TohxOtzvXnzZmFmZiYGDRok0tLSxPPnz8W8efNEo0aNhKGhobC2thaff/65tIyA51w+/lcqfenSpcWRI0eEEELs27dPDBs2TFhYWIimTZuKjh078nzL0P9Kpa9YsaLOVMQff/xR9OzZU7Rt21YMHTpUOt8svOXJ1dVV6mBrZWZmipMnT4pXr16JNm3aCBcXFyGEEJcvXxbly5cXhoaGYvr06fpoLv0L6enpIiEhQezdu1dn+5gxY8SkSZOEpaWlGDFihPTZ/ejRIzFs2DCxdOlSfp7LWGpqqtQ/X7VqlTA0NBS///67EOLd39MzZ84UEyZM+KRtpH+ORXcRkpeXJ548eSIaNWokunbtqnMlTKPRSG/klJQU0aFDB3HgwAF9NZU+gOvXr4vq1auLw4cPCyH+e4Hll19+EeXLlxdubm5CCCGys7NFTk6OOHz4sDh//rz0e8BZDfLy+PFjUatWLbFx40ad7RkZGaJPnz6iXLly0nruV69eiYcPH4r09HTOYpEpjUYj2rRpIwYMGKCz/enTp2LZsmWiZs2aIiAgQNqenZ2tU2TzfMvfvn37xP79+3W2nT59WjRo0EAqylNSUkSvXr3EnDlzxPXr1/XQSnpf586dE40aNRK1a9cWRkZGomfPntK+kJAQoVKphIODg3j06JEQ4vV3/OTJk0XNmjXFtWvX9NVs+pcyMzNFixYthKWlpcjLyxPp6enC0tJS9O7dW6SnpwshhHThVGv06NHC39+fF1ILON4yrJDLy8uT/qtQKFCqVCn4+Phg7969WLRokXSbCaVSCaVSiefPn8PLywvGxsbSeiCSp+zsbDx+/Bj37t0DAOTm5iIvLw9dunRBZGQkVqxYgc2bN8PAwABqtRpt27ZFgwYNoFQqodFouG5fJsT/vzVI/lR6rfyp9HXq1MHYsWORm5sLY2NjlClTBubm5lLKOc93wac91/lT6dPT03Hu3DnpGG0qfZcuXXDo0CE8f/4cAKBWq6V1nUIInm8ZEe+4/Y8QApGRkejUqRMOHTokbVepVLh37x6OHDkCAIiMjIRGo4GbmxuqVav2qZpM/9LZs2fRokULdOvWDevWrUNISAh27doFHx8fAICXlxd8fHxw+vRpeHl5YfTo0RgyZAiWLVuGrVu36twSlOTF0NAQ8+bNQ8mSJdG8eXOYm5tj5MiROHDgAKZPn4779+9LGTwvXrzAtGnTsGvXLvTv359r9ws6vZb89FH9/PPPIjg4+K0rnhqNRkydOlUoFAoxbNgw8Z///Ec8efJE7NixQ3Ts2FHUr1+f000LibFjx4ry5cuL8+fPCyFej25pr4S6uLiIrl27iqysLJ7nQuLvpNIzpVq+PkQqPcmL9v374MEDceXKFZGcnCzty8nJEUOHDhVmZmbSzLQHDx6IUaNGiYoVK4o6deowpVyGrl27JpRKpU7y/L1790TlypXfmtmybNky4erqKhwcHMSECRN414lCQqPRiOPHj4vatWuLNm3aCCGE8Pb2FqVKlRJ169YVwcHB4rvvvhNff/21KFeuHN/jMsFL3YXUrVu3MHz4cNSvXx+zZ8/G+PHj0bhxY3Tp0gVKpRKzZs2ChYUFpk2bhqioKGg0GlhbW8PKygq7d+9mSrkMJSYm4smTJ3jx4gU6d+6MYsWKYfz48bh48SKGDh2KNWvWoH79+tKoSdmyZXH//n0YGhrqueX0Pt43lT4zM1NKqib5+Lep9CQ/2hkNv//+O4YMGYKsrCwkJycjNDQUY8aMgVqtxsqVKzFkyBA4OTkhOjoaX3zxBSZPnoxevXrh5s2b6NixI2rUqKHvl0J/kxACx44dg6GhIR4+fChtX7lyJf7880/89ttvmD17Nh4/fozx48djxIgR8PDwkB7LpGp5Sk9PR2pqKlq0aAHg9ezTJk2aICoqCgMGDICDgwMOHz6MFi1aYMWKFQgKCkLZsmVhb2+Po0ePwsrKSs+vgP4WPRf99JE8evRItG7dWixdulQkJCQIFxcXUatWLTF48GCxd+9e8erVKyGEEFeuXBEnTpwQ27dvFykpKVzfKVPLly8XZcqUEQ0bNhRKpVK0aNFCREREiLy8PHH8+HHRuXNnUb16dREfHy+ePHkiMjIyRMeOHcXw4cP13XR6D0ylL1qYSl/0aM/bmTNnhKmpqfDx8REHDx4UAQEBwtDQUNy4cUPn+IEDB4qSJUsyi6UQePz4sYiIiBDly5cXkydPFgsXLhRly5YVixYtErt27RKBgYGiUaNGwtraWlSuXFmsXr1a302mf+HmzZuibNmyQqFQiHbt2onJkyeLAwcOSLPSEhISRMOGDUXz5s2lx9y5c0cIwb663LDoLoS0neqff/5Z1KpVS9y+fVs8e/ZM3Lt3T3Tv3l2ULl1aNGvWTOzZs+etL24hOKVcbk6dOiXMzc3F5s2bxYMHD8Tdu3dFv379RLNmzcSMGTNEXl6e+P3334Wzs7NQq9Wibt26wtraWjRs2FBaRsBCTD6YSl+0MJW+6Prjjz+EkZGR8Pf3l7alpKSI1q1bi7i4OLFjxw5x9epVaZ+zs7NQKBRSeCbJ19OnT0V4eLioVKmSUCgU4uTJk28ds2/fPuHt7S2lWpM8paamisaNGwsrKyvRtGlTMXToUGFsbCwaN24snJ2dxaZNm8TmzZtF7dq1Rbt27XT6a+y7yQuL7kIsLS1NODk5iTVr1kjbGjRoIBwdHYWzs7OoVauWqFChgvj555/12Er6t3bu3CmqV68u7t+/L2178uSJmDBhgmjSpIkIDg6WPphjY2PFmjVrxPr166W13bxSKi9MpS9amEpfND1//lx07NhRVKxYUSeXZebMmUKhUIimTZsKhUIhbGxsRExMjBDi9cWVESNGiIsXL+qr2fSe3lU8PXr0SCxfvlyYm5uLsWPHStu1MxWp8Lhy5Yro3bu3cHR0FCdOnBA3btwQGzZsEK1btxbNmzcXpqamomHDhkKhUIjevXvru7n0nlh0FxLJycni7t27b2339fUVNjY2IiMjQ9jZ2Ql7e3vx8OFDIcTroJ2goCB2ymRKW0Tt379fVK9eXZw9e1YI8d9O9rNnz4Sbm5to2rTpX3bCeHsJ+bl06ZIwMzMTW7ZsEUK8Lq61vwu7du0SCoVCbNq06Z2P5fmWj/yd8A4dOoju3btLP2vPd3p6umjVqpXo16/fOz/HOcItL2+er8jISNGpUyfh5OQk7t+/LxYuXChKly4tfv75Z/Hs2TPpHtwDBw7ke1vGtO/1hIQEsXr1arFgwQJpBsOrV69EeHi4KFeunPD09JQew/Nd+Fy8eFF06dJFdOrUSSQkJEjbHz9+LNasWSOmTJkibG1tGZomYyy6C4ENGzYIGxsbMWHCBPH48WMhxH8/xF++fCkcHByEWq0Wbdu2fWdhLgRHQ+TswYMH4rPPPtNZn639Qn7x4oUoV66c+OGHH/TVPPoImEpftDCVvvDTns+kpCQxYsQIaXtUVJTo0KGDqF+/vihWrJg4ceKEEOK/39mjRo0STZs2FS9evPj0jaYPZsuWLcLMzEzY2tqKmjVrimLFionQ0FDx7NkzqfCuVKmSGDZsmL6bSh/R5cuXRZcuXUSXLl1EXFzcW/vZV5c33qdb5lauXAk3NzeMGjUKAwYMgJmZGQBICZZqtRqNGzdGxYoVsWvXLlSoUAHi9cUWnedhSrl8BAcHY+DAgQAAjUaDsmXLYuXKlYiKisLUqVMBQLpXY7FixdC2bVs8efJEX82lfykxMRGxsbHYvn07MjIyAADjx4+HnZ0dhg4digsXLkCtVkOpfP1xXrZsWajVahgaGkrbSD7279+POXPmYObMmTh8+DAAwMnJCU2bNsXmzZsRGhqKnJwc6dzmT6UnedKmlP/2229o3bo1TE1NpX2DBg2Cq6srihcvjsaNG6Ns2bIAIJ3/R48eoV69etJdCkh+Lly4AE9PTwQHB+Pw4cNISUnBuHHj8MMPP2DdunUwNjZG37594evri6NHj+Lu3bv6bjJ9JLVr10ZoaCgUCgVmz56NX3/9VWc/++oyp++qn95ffHy8qFKliti6detb+3JycqTR7pSUFFGiRAmxbNmyT91E+sDCwsKEWq0WGzZs0Nmel5cnIiIihFqtFt9++624e/eu0Gg0Ijs7WzRp0oQj3TLFVPqihan0RY92hPvs2bPC1NRUTJky5Z3HrVu3TrRv31707t1bXLhwQQghxLRp00SZMmWkn0ke3nyvHjx4UNSpU0ekpqbqzE6aPHmyKFmypLh9+7YQ4vWSMe1sRircLl++LHr06CFatGghjh8/ru/m0AfColvGli9fLrp16yZevnwpbTt48KCYMWOGcHBwEFOnTpXW+Y4dO1Z88cUXIi0tTV/NpX8pLCxMGBkZSet1MzMzRVZWls4X+NatW0XJkiVF8+bNhb29vbC3txf16tXjlCQZYip90cJU+qIrJSVFmJqaCm9vbyHEf89jRESEzkX1qKgo0b59ezFw4EDh6uoqjI2NpdvGkfz8+eefIjs7W8TExAhTU1Nx7949IYSQ+nSZmZmiSpUqOmG4VHQkJyeLvn37vvMuQyRPnKcgYykpKbhw4QJMTEwAAJMmTcKJEyfw7NkzVK1aFdu3b0dqaioiIyNha2uL3377Debm5npuNb2PhIQEjBw5EnPmzEH//v3x+++/Y9asWbhy5Qpyc3MxYMAADB48GH369EHTpk2xadMm3Lt3D2XLloW3tzfUajVyc3M5NUlG7ty5A1NTU7Rv316aUrp8+XLMmjULMTExMDMzg5eXF9asWYMhQ4YgLS0NarUa/fv3h0ql4vmWGaVSCY1GgypVqkjb+vXrh5IlS8LZ2RnTpk3D8uXLMXbsWIwbNw6//vorypQpA2trayiVSp5vmdFOKQeAmJgYFC9eHEZGRsjJyYGBgQH8/f2xYMECxMTESI8ZNGgQlEolfvrpJ9y6dQvHjh2DnZ2dvl4C/UOpqakIDg5GcHAwtm3bhsDAQGzbtg1ffvklrK2tMXjwYMTExMDExARCCDx//hwlSpRAqVKl9N100oO6deti3bp1MDQ01HdT6APhN7SMubi4ICoqCjVr1oRSqUR2djZ8fX3Rs2dPVKlSBfPnz0dAQADu3LmDoUOHYsiQIVAoFDpf9iQPCoUCX375JVauXInatWvDz88PdnZ2cHZ2RkpKCnbs2IGrV69i9uzZqFq1KiZNmqTzeI1Gww65TGjfn9p1nXfu3EG5cuWQm5uLUqVK4fvvv8fTp08RFRWFrl27wsrKCh07dtR5Dp5v+cnOzsbjx49x7949AEBubi5UKhW6dOmCyMhI9OzZE506dUL//v0BAG3btpUey/MtL9r3eGpqKq5evQovLy+8fPkS27dvh4GBAZRKJRYuXIi1a9eiZcuWOo8ZOHAgTExMYGNjg+rVq+v5ldDflZeXh3379uE///kPLl68iH379mHNmjUwNzeHEAJ+fn7w9/dHt27dEBERgYyMDGzZsgVPnjyBjY2NvptPesKCu5DR91A7/X1vTh3MysoSZ86cEX5+fmLGjBni0aNHOreR2LVrl2jWrJlITU0VQryersjppvIxceJEsW/fPunnc+fOiZ49ewqFQiHGjh2rM2V84cKFokqVKtJyAk4zlT+m0hc9TKUvOu7cuSPKlSsnateuLXbs2CE0Go2YOXOmqFu3rlCpVGLXrl1CCN1bQ/G8y5+Hh4dQKBSiXbt2OtszMzNFTEyMaNmypTAxMRG1a9cWNWrUEKdOndJTS4noQ+OlcZnIPzp96dIlmJiYoFKlSmjcuDEaN2781vGvXr3C0qVLUb16dVhaWgL4b6I5FXwvXrzAjRs38PLlS2lbw4YNMWPGDLRu3Rpdu3aFWq2Wfi/c3d3h4+OD8+fPw8bGhjMZZCg4OBgnT57Ehg0bdFLpu3fvDgsLC/j7+zOVvhBJTEzEkydP8OLFC3Tu3BnFihXD+PHjcfHiRQwdOhRr1qxB/fr1pTtNlC1bFvfv3+fIRyFx+fJlPHr0CNWrV8fy5cuh0Wjg5+cHtVqNDRs24OjRo+jYsSOMjIyg0WigUqn4uS5TQgip/2Vubg5nZ2dcuHABbm5uiIiIAAAYGRmhR48e6NGjB44ePYpixYrBwsICFStW1GfTiegD4id4ARcSEoJff/1V+rL18fGBo6MjGjRogLFjxyIuLk7n+BcvXuDs2bPo06cPbt26hXXr1klTykk+ihcvDhsbG0ydOlWnsLKzs4OLi4s03Uz7e3H16lVYWVmhVq1a+mgu/Uvh4eHw9vaGo6MjgP/e8u2LL77AkiVLMHfuXHh5eeHevXvIy8tDTk4Obty4Id0ikOQlIiICXbt2xYQJE9C3b1907NgRK1asgKWlJWbMmIFy5cqhZ8+eOHLkCJ49e4aXL1/i7NmzqFChgr6bTh9Iu3btMGzYMOTk5MDIyAghISHYsWMHfH190a9fP+zfvx9+fn7Izs6GSqXid7iMKRQKxMbGIikpCT/88AOWLl0KZ2dnJCUlwdXVVefY69evo1mzZrC1tWXBTVTIKIR444bNVGAcP34cAwcORNu2bTFx4kTcvn0bo0aNwpIlS3D16lVs3rxZClPq3LkzcnJyMGPGDBw6dAilS5fGzz//DAMDA+kqOcmD9qp4amoqPDw80KdPH7i6uv7lOXz+/DmcnZ2RkZGBvXv3cjREZsLDw6VAtP79+yMrKwsKhQIGBgbS6Eh0dDSGDx+OunXrSiOdDx8+xLlz57iWV2ZOnz6Nbt26ITQ0FB06dIBGo4GnpydSU1PRvXt3TJ8+HX/88QfmzJmDDRs2oFatWlAqlVCpVDh16hQMDAx0Rs6o4HszRyUrKwtGRkbYvXs3tmzZgoEDByIsLAx3797FpEmT0KNHD/j7+2P37t2wtbVFcHAwZzjIWFZWFoYNG4ZNmzbhyJEjaN26NR4/foy1a9dKQbfLli2Dv78/4uLisHPnToanERVCLLoLuOjoaMydOxd2dnYwMTFB7dq1MWrUKABAXFwcZs+eDbVajXHjxqFTp044deoU7ty5gx49ejDRVuaEEBg1ahQSExOlqab5ZWZmYv369di0aRPS0tKkDjmD8uQjISEBLVq0wJw5c+Dt7f2XqfRVq1bFjRs3mEpfCMTExGDs2LFISEhAuXLlAABPnz7FrFmzEBcXB2dnZ3h5eUGhUGD//v1MpZc57efxrVu3kJSUhN69e0v77t+/j7Zt28LT0xP9+/fHyJEjce/ePXh7e6NHjx6YOnUqTp48iY0bN3KWgwzlvzh28+ZNTJ8+HRs2bMCBAwdgb2+Px48fY9OmTQgMDER2djays7OxY8cONG/eXM8tJ6KPgUV3AZX/w3rz5s0IDAzExYsXMXHiREybNk06Tlt4GxgYYPTo0ejWrZu0j8WXfGnPf15eHmxsbFCiRAls2rQJn332mc5xCxcuREpKCoKCgliAyVBiYiJmzJiBa9euYfbs2VIqva2tLVJSUpCYmIgGDRpg9uzZ77zdH2exyIf28/jAgQNwd3fH9u3bYWNjI71nnz9/ju+++w5nz55FVFQUrKys3noOnm95unXrFmxtbfHo0SN8+eWXGDp0KBo3bow6deogJiYG8+bNQ3R0NB48eAA/Pz88fvwYo0aNQp8+ffDo0SPp4gzJy8uXL2Fqaip9n9++fRuTJ0/G5s2bcfDgQbRu3RovXrzArVu38Ntvv6FFixaoVq2avptNRB8JK7ICSPsBnZubCwDo378/pkyZgipVqiA2NhYJCQnSse3atcPUqVORlpaGgwcP6jwPC2750p5/pVKJffv24f79++jduzeOHDmCV69eScd5eXkhODgYarWatw2SCW9vb8TGxgIAmjVrhoCAANSuXRtOTk7o2LEjVq5cifHjx2Px4sVwdnZGbGws0tPTAeCtdZ0swORD+3ncuHFj5ObmYuHChQAgvXdLlCiB4OBgpKamYtOmTe98Dp5vecrLy0P16tXRokULpKenIzY2Fp07d0Z4eDhevXqFUqVKISkpCfXq1cOsWbOgUqmwatUqvHz5kgW3TJ0+fRq1a9dGcnIyFAoFhBCoUqUKfvrpJ/Tq1QsdO3bE6dOnUbx4cdSrVw9ff/01C26iQo4j3QVM/tHply9fwsTERBrx3rp1KwICAmBtbQ0vLy80bdpUetyZM2eYWi1T/2v0SnsB5unTp+jevTvy8vJgb2+P8ePH64SscI2nPLx48QLDhw/HoEGDpNA04HUHLTY2Fl27doWNjY30OZCZmYkyZcogPDwcgwcP1mPL6X29mUqvUqmwf/9+dO/eHRMnToS/v7/O8X369EHVqlURFBSkpxbTx3DlyhX4+voiLy8PQ4YMgUKhQEhICMzMzKQpxfHx8TA0NMSlS5dQrFgxVKlSRd/Npn9I+x4/deoUJk6ciJs3b2L37t2wsrKSPtcPHjyIjh07AgBOnDjB6eRERQQrtAJECCEVzQEBAejWrRu6deuGUaNGIS8vD3379oWPjw+Sk5OxcOFCnDp1Snqsra0tlEolE05lSFtwL1q0CKdPn9Y5h9op5qVKlUJ8fDz69euH69evw8bGBj4+Pjh27Jh0HBV8TKUvWphKT1q1a9fGTz/9hKysLCxbtgzW1tbYtWsXfHx80L17d3h6esLQ0BBCCFhZWbHglpGLFy9i6tSpuHHjhvRd3KRJEwQFBcHKygqdOnXCxYsXpc/1SpUqoV+/fvD09ESJEiX02XQi+oQ40l1A5B+pDAoKwowZMzBhwgTcv38fv/zyC4yMjLBv3z5UrlwZGzduRHBwMMqVK4fAwMB3rv2jgi//rIZly5ZhzJgxOHPmDBo2bPhWEf3maHhMTAyysrJQoUIFtG3b9pO2m94PU+mLFqbS07tcuXIFnp6eAIDp06ejdevWem4R/Rs5OTlo3bo1kpKSUKtWLTg6OqJp06YYMGAAAODSpUsYO3Yszp8/j507d6JatWpYtGgRzpw5g3Xr1qFYsWJ6fgVE9Kmw6C5gjhw5gg0bNqBr167o1asXgNepl/369UNWVhbOnj0LAFizZg3i4+MRHh7OzrjMxcXF4cqVKyhevDgGDhyo7+bQR8ZU+sKPqfT0v1y5cgVeXl4QQsDPzw/29vb6bhL9C/PmzYNarUaDBg1w7NgxLFy4EF9++SXatWsHNzc3XL58Gf7+/oiKikK9evVw+/ZtxMfHSzObiKhoYNFdgOzZswc+Pj64f/8+oqOj0apVK6mjnZycjC5dumDatGlwd3fXeRw74/J14cIFNGzYEAAQFhb21rmlwoWp9EUDU+np/3LlyhV89913ePDgARYsWIAWLVrou0n0nuLi4uDo6IgDBw6gadOmSEtLQ3h4OAICAtCkSRMMHToU7du3x927d/HgwQPY2NgwNI2oCGKlVoDUqVMHn3/+OZ4+fYro6GgA0FkDVKpUKZ11oFosuOXjzWtctWrVwvr161G+fHnEx8f/5XFUODCVvvBiKj39E7Vr18a8efNQpUoVVKpUSd/NoX+hXbt2GDFiBIKDg5GZmYmKFSsiOTkZ1apVQ40aNRAVFQVra2ucPn0ajo6OLLiJiihWa3ryrsCzmjVrYvr06RgyZAhiY2Mxb948aV+xYsWkETKSp7y8PGktZ3Z2NnJzc2FkZISvv/4agYGB2LJlC7777jsAkG4xQvKl0WjeuV2tVkMIgYoVKyIpKQnGxsbw8fHB999/j7S0NOk4pVIJIQQLMBl48eIFbty4gZcvX0rbGjZsiBkzZmD27NlwcXGBWq2WPr/d3d3x8OFDnD9/HgAvnBZVdevWxbp162BpaanvptC/9Pnnn+PatWswNDSEm5sb4uLisHXrVqxZswbh4eGYP38+OnTooO9mEpEecXq5HuSfDn7s2DHcvXsXVapUQe3atVG6dGlcu3YNc+bMwe7du9G8eXNYWVnh4sWLOH/+PJKTkznqJUP5z/n8+fNx+vRpJCcno2/fvujZsycaNWqEdevWwdXVFWPGjEFgYKCeW0wfyqJFi9CqVSs0btxYp7jS/k7k5eUhJCQEv/76Kw4fPgwXFxf06tWLAUsy4+/vjw0bNuDo0aM66eP37t1DhQoVdI69cOECBg8ejKVLl3JaMVEh4eDggKNHj8LCwgK7d+/mmm0i0sGi+xPLn1Lu6+uL6OhoZGZmomrVqqhSpQqCgoJQqVIlpKamYs6cOdi4cSMaNWoEV1dXDBkyBADX+8nZ5MmTERERgdmzZ+Px48dYu3YtSpQogT179sDExARbtmyBu7s7Bg4ciIiICH03l94DU+mLFqbSExVt2s+A3bt3Y/z48ZgzZw6++uornf4eERG/7T8x7Qfw3LlzsWbNGkRGRuLWrVto0aIFduzYgaFDh+LWrVuoVq0afH190b9/fxgaGuLRo0dvPQfJy5kzZxATE4MdO3bAzc0Ntra2uHLlCtzc3FCyZEkYGBjgm2++QXBwMK5evcrp5TKlLaLi4uKgUqkQFRWFRo0avfN9+2Zh1rNnT/Tt25cFt4xoz2vVqlVRvXp1hIWF4eLFi28dl5mZiZUrV6Jv3764du0adu/eLc10ICL5yn9v7ry8PJw6dUpnOxERwKL7k8nfsUpPT8eePXsQGhoKe3t7/PLLLwgLC4OzszPu378Pd3d3/Pnnn6hatSomTZqEGjVqYMuWLZg9ezYArv+Tq6ysLOTk5KBVq1aIjo6Gk5MTFixYABcXF2RkZCA6OhrPnj2Du7s7Dh06xDX8MnbhwgV06NABHh4eePHihb6bQx+ZdkRryZIlyM7Ohru7O27duqVzjLGxMV68eAErKyucPn0aBgYGUqgeEcmfubk5vv/+eyxYsAAJCQn6bg4RFTD8tv8EhBBSx+rgwYMoU6YMfH190bx5c5w8eRJubm6YP38+wsPD0aZNG+zbtw/dunVDeno6atasCT8/P1SrVg2HDh3C48eP9fxq6O/IP0qt/bsQAmZmZli9ejVcXV0xZ84cjBw5EsDrWwzt2LEDd+7c0Xkedsjlgan0RRtT6YkIANq3b49mzZoxkZ6I3sI13R9Z/jU9fn5++Pnnn7F9+3bUrl0bADBt2jRcu3YNkZGRMDQ0RHBwMPbt2wdbW1vMnDlTmn56+/ZtqNVqWFhY6O210N+Tf03vm2t227Rpg2PHjiEoKAjjxo0D8HraaZ8+fWBkZIStW7ey0JaZ/Oc7OzsbSqVSKqaioqLg5uaG0aNHIygoCAC4zk/m/lemhvbcPn36FN27d0deXh7s7e0xfvx4VKxY8a3jiKjwyczMhLGxsb6bQUQFDC+zf2TajtX169fx+++/Y+HChVLBDQCPHj3ChQsXkJOTA0NDQxw5cgSdOnXC+PHjAbzu4CmVSlSpUkUv7ad/TluABQUF4fDhw6hVqxbat2+PHj16YN26dXB0dMTy5cthZGSEnJwcxMTEIC0tDWfPnpXWeLLwlof/K5V+8ODBUCgUcHV1hUKhQGBgIIstmdMW3O9KpdcuCSlVqhTi4+OlVHobGxudVHr+DhAVXiy4iehdONL9keQfyQgNDUVgYCAsLCywYcMGVK9eXeqsb926FfPnz8fTp09RrFgxZGRk4Pz589K9fNk5k6f58+dj7ty56N27N86dO4esrCyMHDkSI0aMwN27dzFq1CjcvHkTxYsXR506dbB48WJpjSennMoPU+kLP6bSExER0fti7/4jiI+PR2JiIhQKBUaOHIm+ffsiJCQECQkJuHjxIqpXry513r766isoFAqcOXMGQgj88MMP0no/3hZMPt4cnX78+DHWr1+Pjh074uLFi1i6dCkCAwMhhICHhwe2bduGhw8fokSJEjA0NAQAFtwylT+VvlWrVti/fz+mT5+OJUuWoGTJkgCAb775BhkZGVi/fj0vpsnUX6XSv8u7UumJiIio6GIP/wNbs2YN/P390a1bN9SrVw+mpqYwNTVFUlISmjZtiunTp8PS0hL169cHAKjVavTp0wd9+vSRnoMFt7zkL7j37dsHExMTJCYmokuXLgCAunXrwtPTEwqFAsHBwcjLy8OoUaNQtmxZ6TmEECy4ZerNVHoXFxedVPpffvkFnTp1gru7O9zd3QG8fZGG5EGbSg8AYWFhem4NERERyQV7fR/Q2rVrMXLkSPz0008ICAjAiBEjALy+J3dycjJOnTqFBw8ewMPDA3/88Yf0uDdvC8WCWz7yJ9NPnDgRTk5OGDhwIOLj43Hs2DHpuNq1a2PMmDH48ssvMWXKFOzYsUPneTjyKQ9MpS9amEpPREREHwKH1j6Q5ORkzJs3DwsWLNAZte7fvz+2bt2KDh06YPbs2Th79ixsbW0xcuRIhIaGwsbGhh1wGdMWyzdu3MChQ4ekWwTFxMRg1qxZKF68OL799lsArwtvd3d3VK1aFT169NBns+k9/FUqfcuWLWFoaAgXFxcEBQVh1KhRAF4n2M6bNw9GRkawsrLSW7vp/bwrld7IyAhff/01cnNz4ebmhvLlyyMoKAgKhYLLBoiIiOgvsej+QG7duoXnz5/DwcFB6qxpg3Z27dqFBQsWwM/PDzNnzsSZM2dQvnx5hIeHY/HixfpuOr2H/B3sgIAAnDp1Cs2aNZMuotSoUQPGxsbw8/MDAKnwrlevHurVqweAywjkhqn0RQdT6YmIiOhDYnr5B+Lv748FCxbgwYMH0ra0tDRoNBpUqVIFycnJcHd3R3Z2Nk6ePInHjx+jVKlSLLpkKH/BnZCQgDNnzmDs2LGwtbXF8ePHpePS09MRHh6O4OBgeHt7Y/LkyfpqMn0gTKUvWphKT0RERB8Ch10+kFq1auHVq1eIjY2VtlWsWBFVqlRBXl4e6tWrh169eqF8+fJ49uwZypQpA5VKBY1Go8dW0z+Vv+CeMmUKRo4cCQcHB4SEhCAxMREBAQHSsRYWFvDw8MDw4cNx6NAhrvuUoTfzFrSp9GFhYYiMjESbNm0QGBiIsLAwmJubY9u2bdi7dy/27duH8PBwFtwylj+V3s3NDba2trhy5Qrc3NxQsmRJGBgY4JtvvkFwcDCuXr3K9zcRERH9JfYEP5BmzZpBrVYjLCwMderUQdWqVaV9SqUSz58/x5EjR2BlZYVSpUpJ+zjSLS/agjsxMRFnzpzBkiVLULduXVSvXh2ZmZn47rvvoFarMXHiRACAubk5pkyZgtKlS3Pdp8wwlb5oYyo9ERERfSjsDX4gNWrUwLJly+Di4gIjIyN4e3ujcePGAF6HbLm7u+PevXvYvn07ALD4krG1a9di69atEELAxsYGQggYGRlJidUTJkyAUqnEd999BwAoU6YMAJ5zOXkzlX7ZsmUwMzPDgwcPcOzYMbRt2xbAf1PplUolpkyZgkqVKsHR0VF6Hp5vecj/3tT+PX8q/dixY9+ZSm9tbS3dix1gKj0RERG9G4vuD6h///7IyMjA6NGjER8fjwYNGiA3NxfPnz8H8LqjplarGaAlc+np6Th9+jSys7ORmpoqBaNpC2+VSgUvLy9YWFjgm2++kR7HAkw+mEpfdDCVnoiIiD42Bql9BGfPnkVERAQuX74MS0tL2NnZwcPDAyqVius7ZeavpouuWLECAQEBsLe3x+TJk1GnTh1pX2ZmJmJiYtC7d2+ea5l5Vyp92bJlsWTJEiiVSqSnpyMsLAxBQUH48ccfpcI7P15Uk6d3pdLfvHkTjo6OyM7Ohqen51up9Gq1mlPKiYiI6P/EovsTYmdcXvJ3pk+dOiWFajVr1gwAsHjxYkRERKB169YYN24catWq9dZz8CKLfDCVvuhiKj0RERF9TOwtfCTvWr/Lgls+8l8g8fHxwbZt2/DkyRMYGxujWbNm2LJlC8aMGQONRoPVq1dDqVRi9OjRqFu3rs7zsEMuD2+m0v/yyy9Yv349QkJCMGbMGAQEBMDX1xfAf1Ppnz17hkOHDsHX15dLB2TmzdFpbSp9x44dcfHiRSxduhSBgYEQQsDDwwPbtm3Dw4cPUaJECRgaGgLgBTUiIiL6+9hj+EjYCZen6Oho9OnTRyq4Q0JCEBERgR07dsDY2Bjp6ekYPXo0OnfujAMHDsDLywtKpRIBAQGoVq3aW0U3yQNT6YsOptITERHRp8ZeA9H/5+DggDJlyqBXr14wMDAAAJw+fRrDhw+Hvb29dFz16tXRvn17fPvttwgNDYWnp+dbqdUkP0ylL/yYSk9ERET6wKKbCMDmzZtx5coV/PHHHzAwMMCdO3dQuXJlXLt2DRkZGdJxGo0G9evXx5gxY3Do0CE8e/YMJUuWhJOTk7Sfywjkian0hR9T6YmIiEgfWHQT4XVnvEyZMrhz5w78/PyQl5eHJUuWwNnZGaGhoYiJiUHPnj2lgrp06dJ49erVW1NMWXDLw7sSp729vVGmTBkEBARg7ty5Oqn0RkZGcHNzg7m5OXr37q2PJtO/8K5U+mbNmsHGxgZKpRI1atSAsbEx/Pz8AEAqvOvVqyddfOEFNSIiInpfLLqJAHTt2hVz587FV199hRs3buDkyZMAgBYtWmDnzp1YsWIFsrKy0LdvXzx48AC7d+9GzZo1YWJioueW0z/1v1LpXV1dkZmZiYiICCxcuFAnld7Y2Bj9+vUDwBAtOXkzlb506dKIiYmBra2t9HugDcdTKBT4/vvv8eLFi7dS6VlwExER0fvizUWpyNNoNChRogSsra1x9epVNGrUCEqlErm5uWjUqBEmT54MtVoNLy8vVKtWDR06dEB6ejpWr14thWiRPGg0GqnQ8vHxwddff41u3brByckJTk5O0Gg0GDNmDFxcXHD8+HEsXLgQFy9efOt5WHDLw5up9CNHjoSDgwNCQkKQmJiIgIAA6Vht4T18+HAcOnSI72siIiL6YNhzpCJLO+KpUqmQnZ2N5s2bw9nZGePHj4enpycCAgLQsmVLtG7dGpaWlrh37x4OHz6MypUro2/fvlCpVBzxlAmm0hdNTKUnIiKigkAheDmfiqD8U4zXrl2LEiVKoE2bNihbtizS09PRoUMHlCtXDrNnz0arVq3e2fHmGk950KbSb968WUqlHzp0KCpUqIB58+ZJx124cAHt27fHgAEDEBoaCgDYtm0bHB0deZ5lTJtKn5WVhe3bt8PY2BgKhQJZWVlYtmwZJkyYgLlz50qp9FosuImIiOhD4fRyKnLy3zbIx8cH3t7eePr0KTQaDYDX00wPHTqE+/fvY8qUKThx4sQ7p5qyECv4tKn0kZGRUio9AFy7dg3Xr1+XjsufSn/+/Hk8e/YMAODk5ASVSiX9bpD8aFPpz5w5g9TUVKmQ1qbSBwcHY+LEiVi/fr3O41hwExER0YfCopuKHG1nOjg4GGvWrMF//vMfaeQTAJ4/fw5zc3McOnQIDx8+xPDhw3HhwgV9NpneU/5Uek9PT/j7+wMAnJ2dcenSJcTExAAAU+kLCW0oXn7e3t6YMWMGSpYsiblz5+Ly5cvSPm0q/aZNm9C/f/9P2VQiIiIqQlh0U5GUl5eHxMREDBkyBE2aNMH169exbds2dOnSBaNGjcKhQ4dgYWGB2NhYNGnSRLptEMlL165dYWJigq+++grh4eFwd3cH8DqVvmrVqlixYgW2bt0KAEyll7k3U+kTExORmJgIAHB1dcW4ceNw9uxZLFy4ECkpKdLjtKn0arUaubm5emk7ERERFW5c001FQv71mUIIZGVlwdnZGRqNBq1atUJsbCzUajWMjY2RnZ2NrKwsbNy4EWXKlJGeg2u45UV7voYOHYq1a9fCzs4OK1asQP369aFWq3Hs2DEsWLAAv/76KwwNDVGyZEkolUokJibCwMCAa3plJP9708fHB9u2bcOTJ09gbGyMZs2aYcuWLVCpVFi4cCFWr16N1q1bY/To0QzIIyIiok+CI91U6OXl5ekU3MDr0a3hw4fj6dOnCA0NhYODA2bMmIHo6Gi0bdsWJiYmOgU3wCnGcqGdYpw/lX7fvn3IysqCp6cnTp48iby8PLRu3RoLFixATEwMvLy8MHXqVJw6dQoGBgbIzc1lwS0D0dHRAPBWKn1kZCT27NmDpUuXIikpCZ07dwYAeHl5wcXFBdu2bcPu3bv11m4iIiIqWjjSTYVa/tHKwMBAKSTL398f9erVw9OnT5GdnY3y5ctLj+nWrRvKly+P1atX66vZ9J6YSl90MJWeiIiI5IIj3VRo5R/hnjVrFn766Seo1WrcvHkTLVu2xM6dO1GiRAmUL18eT58+xa5du9C9e3fcunULERERAPDO1HIqmJhKX3QwlZ6IiIjkhEU3FVraAiw9PR13797Frl27EBERgaSkJPTt2xeDBg2S0qvT09MRHh6O4sWL48yZM5xiLENMpS86mEpPREREcsKimwq1jRs3olKlSjhw4ACMjIyk7RERERgwYACcnZ2xY8cOWFlZYdmyZdiwYYOUYvxmB50KPqbSFw1MpSciIiI54ZpuKtRevnwJNzc3bNy4ETt37kSPHj101nl7eHhg+fLliIuLQ9u2bQHorgumgo2p9EUPU+mJiIhIblh0U6HxV8Xyy5cvMWDAACQlJSEmJgZNmzbV2R8QEICJEydyZFtm8p9v7ceYQqHAnj17MH/+fKSkpMDDwwOdOnVCs2bNMG/ePBw9ehQ7duzQZ7PpPeU/39nZ2Vi+fDmsrKwwfvx4mJmZISAgAC1btoRSqcStW7dw7949HD58GJUrV0bfvn2hUqk4g4WIiIj0gkU3FQr5O+QJCQnIyclBqVKl0KBBAwBAVlYWevfujd9++w07duxA06ZN3xrxYodcPphKX7QwlZ6IiIjkjHNoSfbyp1b7+fmhf//+GD58OOzs7DB37lzcvXsXRkZG2L59Oxo3bgwnJyf8+uuvb3XMWXDLA1Ppixam0hMREZHcsegm2dMWYP7+/li5ciVWr16NS5cuwdPTE1OnTkVQUBDu3bsHIyMjbNu2DRUrVsScOXP03Gp6X0ylL1qYSk9ERERyx6E9kq38U05TUlJw4sQJLFmyBA4ODti+fTtWrVoFZ2dnzJs3DwAwduxYVKpUCUePHuWol8xt3LgR33zzDaysrDB8+HBpu3Yk29nZGWvXroWjoyOWLVsGCwsLKJVKLiGQqXel0p85cwZhYWEoX748XF1d0b59e8TGxsLb25up9ERERFSgcE03yd6FCxdQv359REVFwcnJCWfPnsWAAQMwadIkfPvtt/D09ERERARcXFzg7+8vJVczpVy+mEpfuDGVnoiIiAoT9kBJdu7fvy/9ffz48bC1tcXz58/Ru3dvmJqaIjo6Gvb29tK9e0uVKoXWrVvj/PnzMDMzkx7LAkwe8vLy3tpmamqKiIgIdO/eHe7u7khKStKZMh4WFoaffvoJrVq1krbxfMtD/jX72mvCxsbGGD58OJ4+fYrQ0FA4ODhgxowZiI6ORtu2bWFiYqJTcANcw01EREQFB+dZkqyEhIRg//79iImJweXLl/HixQvs378fJUqUgBACGo0Gly5dQqlSpaBWqyGEwO+//44ffvgB9vb2ADjiKSf/K5Xe1NQUW7duRe/eveHo6PhWKr2vry8AptLLSf7QtDdT6b/88ku0atXqrVT6Q4cO6fxMREREVNCw8iDZCAsLw4QJE+Dq6opdu3aha9euOHXqFKysrKTRMZVKhd69e2PDhg3o2bMnbGxscPXqVbRo0QKAbqeeCjam0hctTKUnIiKiworVB8nC+vXrMWrUKOzduxdfffUVHj16BEtLS6SkpODVq1dQKpXIyckBALi6umL16tX47LPP0KVLF5w9exZqtRoajYap1TLCVPqihan0REREVFgxSI0KvJUrV8LNzQ21a9fG6tWrpVHrnTt34vvvv4ehoSG2bNkCS0tLnanE+acmc4qxfLyZSj9+/Hi4urriq6++wvbt26W/r1q1Ct7e3lIqfU5ODlQqFWcyyFj+VPp169bBzs5O2ufm5obNmzdLqfR//vknU+mJiIhIFtg7pQItPDwcI0eOhJ+fHxo1agQ/Pz/s27cPANCrVy9MmzYNxYoVw7Bhw3Dr1i2o1Wrk5uYC0A3OYodcPrTn7cKFC6hVqxYGDBiAzp0749dff4WXlxd++OEHrFy5EqNHj0ZISAhmzZqFR48ewcDAAEql8p3BayQPvXr1wtdff41Lly7hzz//BPDfKeMREREYOHAgevfujfj4eFSqVEk633x/ExERUUHGopsKrJ07d2LkyJHYsmULZs6cCQ8PD5iYmGDu3LlS4e3k5IQxY8ZAoVDAxcUFqamp7IDLFFPpixam0hMREVFRwd4KFViVKlVCfHw8HB0dAQAdO3bEuHHjYGJignnz5kmFd58+fTBmzBjcu3cPgYGB+mwyvaeQkBAMHz4cAN5KpTc1NZVS6ZVK5Vup9EePHuUIt8y8mUp/7Ngx/P777wAgpdLb2trC0dERSUlJAP474u3r66szo4WIiIiooOOabipwtLd80v43Ly8PQgjpvrsHDhxAcHAwMjMz4e3tjc6dOwMA4uLi0KZNG96fV2bCwsIwZswYbN26FWq1Gl5eXjAzM8OePXtQvnx5qThbsWIF3N3d0aVLF9y5cwd5eXlSSJ72d4UKvvznys/PD1FRUTAyMsL169fx448/YujQoTA3N0dWVhacnJxw/vx5bNy4UWd0m4iIiEhOONJNBUp2drbUIc/IyADwevqoSqWSRra++OILacQ7MDBQSjRu164dVCoVNBqNfhpP/xhT6YseptITERFRUcORbioQDhw4gPbt20ujmvPnz8fBgwdRsmRJDBo0CF27dpVuC6Rds33w4EH4+fmhefPmCA4O1mPr6X0wlb5oYSo9ERERFVXsxZDezZ8/H2PGjMGqVasAAIsWLcKPP/6Ipk2b4tKlS/jpp58wb948ZGdn66zl7NChA0JDQxEUFKTH1tP7YCp90cNUeiIiIiqqONJNepeWloaxY8ciLS0NgwcPxm+//YbevXujU6dOyM7OxoQJE5CUlITu3btj0qRJMDQ0RE5ODgwMDKTnyD+KRgXbzp07pdFNR0dH7N+/HyEhIXj16hUmTZokrdGPjo7GkiVLoFAoEBERgWrVqum34fRe7t+/j/LlywN4nUq/ePFiPHz4EEqlEsWKFcOECRPw559/IjIyEsbGxpg6dSpOnDiBrKwsxMfH831NREREssfeDOmVRqNBxYoVsWjRIpibm2P9+vU4ePAgzM3NAQCGhoaYPXs2mjVrht27d2P+/PnIysrSKbgB3jZITphKX3QwlZ6IiIiII91UAGg0GqhUKty9exfjx4/Hzp07MWXKFEyZMkU6JiMjA1OnTsWuXbswbdo0DB06VI8tpvfBVPqihan0RERERK+x6Ca9+Kvp4Hfv3oWXlxdu3rwJNzc3uLq6SvueP3+OFStW4Ntvv2UBJjPZ2dkwNDQEALx48QLFixeX9uUPQztw4ABCQkKQlZUFT09P9OzZUzpOe3GGCr7169dj8ODBiI2NxRdffIE1a9Zg5cqVOH36NM6dO4dq1arpLBFZu3Ytjhw5glKlSmH27NlSKj3PNxERERUGLLrpk8s/ehUVFYUrV66gTJkyaNOmDezs7HD37l14enoiLS0NLi4uOoW3Fjvk8sBU+qKHqfREREREulh00yeVv+D29vZGREQE6tati6ysLJw7dw5hYWFwdXVFeno6vv32W9y/fx9OTk7w8vLSc8vpn5o/fz4iIiIwadIkDB8+HIsWLYKfnx+8vLwQExMDY2Nj9OzZExMnToShoaFOoXXq1CnY2tpyrb7MhIeHw9PTE76+vkhOTsbjx491wvG2bduGRYsWAYB0z3UW2ERERFTYsUdLn5S24D5z5gwuXbqEffv24fjx44iNjcW0adPg4eGBTZs2wcLCAqGhoVCpVLh48SJ4bUh+Bg0ahEaNGiEyMhJhYWH4448/sGXLFsycORMnT55E06ZNERMTg7lz50q3g8vJyQEANGnShCFaMrNz506MHDlSOsceHh4wMTHB3LlzpXA8JycnjBkzBgqFAi4uLkhNTWXBTURERIUeR7rpk9u0aRMWLVoEjUaDPXv2oFSpUtI+b29vrF+/HseOHUO1atXw+PFjlCpVCkqlkqFKMqKd/n/v3j2MHj0a9+/fx927d7F582Y0atQIwOu13VOmTEFSUhJ69OiBCRMmwMjISM8tp/eVlJSEzMxM2NvbS9v+Khxv27ZtmDFjBhwcHBAaGqqvJhMRERF9Ehzppk/u9u3bePr0KZKTk/H06VMAr4s0AOjVqxcA4MmTJwCA0qVLSyOeLLjlQ6VSQaPRoEKFCli8eDEqV66M27dvY9euXdIxxYsXx+zZs9G8eXOsXLkSGzdu1GOL6X1pr9s2adIE9vb2yMvLk97PX3zxBcaNGwdjY2Od28E5OTlh4cKFXLNPRERERQKLbvqo3jWRYsKECRg/fjzMzc3h5eWF69evS6FolSpVgkqlkopxLa7tlYf808G159Tc3BwLFixA9+7dERMTgxUrVkjHFCtWDLNmzYKnpycGDx78ydtL/052drZ0MSwjIwPA6/eqSqVCbm4ugP8W3iYmJggMDERMTAwAoF27dtLFGSIiIqLCjNPL6aPJn0Z8+/ZtqNVqGBkZoXTp0gCAJUuWICoqCiqVCt9//z1yc3MRGhqKtLQ0JCYmMp1cZphKX3QwlZ6IiIjo72PRTR9F/oL7hx9+wN69e5GSkoLOnTvD0dER/fr1AwCEhYVh7ty5SEtLQ6dOnWBtbY3vv/8exsbGLMBkhKn0RQdT6YmIiIj+GcbG0keh7VRPnz4dS5YsQUREBExNTREcHAwfHx9kZGRg2LBh8PDwgFKpxNq1a2FmZoaRI0fC2NgYWVlZDNWSkXel0jdr1gwPHz7EokWL4OHhgeLFi2PAgAEIDQ3FoEGDpFR6rtWXl0GDBiEhIQGRkZHIycmRUuk7deoEPz8/TJgwATExMcjLy8OkSZNgaGiInJwcGBgYoEmTJgB0L8oRERERFXYc6aYPKn8RFRcXhzFjxiAiIgItW7bEwYMH0aNHDzRv3hy3b9/GjBkzpHW8oaGh2Lx5M2rUqIHvv/8eNWrU0OfLoPfAVPrCj6n0RERERP8chxrog8mfMJ6WlgYbGxv07t0bzZo1w969e/H1118jNDQUYWFhUKvVmDx5MhYvXgwA+PbbbzFkyBCcOXMGc+bMkUKYSD6YSl/4MZWeiIiI6J/jSDd9cL6+vkhLS0NYWBiEEDAyMkLfvn1hbW2NmTNnQqlUwsnJCVevXoWNjQ1WrlwprfmMjIxEhw4dULVqVT2/Cvpf/mp0OjIyEnPmzEHdunWxYMECVK9eHQBw9epVtG/fHmvXroWDg8Onbi79S381Hfzu3bvw8vLCzZs34ebmphOO9/z5c6xYsQLffvstsxmIiIioSGPRTf9a/gLs+PHjcHNzQ2RkJJo3bw7g9XTTJk2aYPDgwZg2bRqePXuGESNGwMnJCf369YNCodAJW6KCjan0RQtT6YmIiIj+HRbd9MEsWLAAN2/eRHZ2tjRtXAiBV69eYdy4cTh37hw6d+6MY8eO4cWLFzh+/Lg0xZihSvLAVPqihan0RERERP8eKx16b29erzl37hxCQkJw6tQpae2uQqGAqakphgwZgkaNGmHPnj0wMzPD0aNHWXDLUP5U+tDQUEyaNAlRUVF49OgRfHx8sGrVKgCAh4cHfH190bRp07dS6Vlwy8e7UumPHz+O2NhYTJs2DR4eHti0aRMsLCwQGhoKlUolpdITERER0Wsc6ab3Eh8fj8TERCgUCnzzzTewsLAAAEyePBlz5szBsmXL4OzsDBMTE+kxOTk50Gg0MDIy4pRymWEqfdHFVHoiIiKif4dDjPSPrVmzBu7u7rh9+zaKFy8uFdwAMHv2bHh4eGDs2LGIjo5GZmamtE+lUsHY2BgKhQJCCBbcMsFU+qKNqfRERERE/w6rHvpH1q5di5EjR2Lt2rXo0aOHdP/d4OBgVK5cGf369cPSpUshhICHhwcUCgWcnJxgYmKiM42cHXL50J63/Kn0U6dOhVKpRFhYGEaMGAEXFxcolUpYW1vj6tWrOHnyJDw8PKBWq+Hu7g61Wo0OHTrwQksB967R6QkTJqBMmTKYM2cOvLy8dFLpK1WqBJVKJRXjWlwyQkRERPRf7AHT35acnIx58+ZhwYIF6NOnj7S9f//+2Lp1K7p06QK1Wo3evXtj2bJlUCqVcHZ2Rrly5dClSxc9tpzex5up9DExMYiMjISxsTGA16n0Fy5cgK2tLZRKJZ49ewZDQ0NMnTr1rVR6FxcXfb4U+hv+Vyq9i4sLXr16haioKAwZMkQnlb5cuXKwt7fXc+uJiIiICi4W3fS33bp1C8+fP4eDg4PUQR8zZgzOnDmDXbt2YcGCBVixYgU0Gg369u2LJUuWoGbNmvjiiy/03XR6D9qCW5tK365dO+k2cEIIKJVKODg44D//+Q9ycnKkVPq+fftCoVAgLy+PI9sy8XdS6UePHg2VSoW5c+eiV69e6NSpExo1aoTo6GioVCqm0hMRERH9BQap0d/m7++PBQsW4MGDB9K2tLQ0aDQaVKlSBcnJyXB3d4cQAlFRUdIUVAAMTZORN6cYu7i4YPXq1WjevDl++eUXmJmZSfuOHj2KNWvW4MyZM7C0tMTGjRthYGDAVHqZmj59OpYsWYKIiAiYmpoiODgYFy9exPTp0zFs2DAAwPLly7F27VpUr14dM2fORNWqVZGVlSUtNSEiIiIiXewV099Wq1YtvHr1CrGxsdK2ihUrokqVKsjLy0O9evXQq1cvmJmZoUKFCjqPZcEtD/Hx8QgKCkJQUBDS09MBAJGRkfDx8UFCQgI2b96MV69eScfb29tj8eLFOHLkCLZu3QoDAwPk5uay4JaJ/Ndc4+LiEB0djZiYGHz11VdQq9WIi4uDpaUlfvzxR0RFRQEA3N3d0a9fP1y7dg3Tp0/HtWvXWHATERER/Q/sGdPf1qxZM6jVaoSFheHGjRs6+5RKJZ4/f44jR47AysoKxYoV01Mr6X0xlb5oYSo9ERER0afB6eX0j2zYsAEuLi7o06cPvL290bhxYwDAjRs34O7ujnv37iEpKQlqtZr36ZWRtWvXwsPD43+m0gOQkuvDw8OlVHqSt/yp9EIIGBkZoW/fvrC2tsbMmTOhVCrh5OSEq1evwsbGBitXrpQurERGRqJDhw6oWrWqnl8FERERUcHFISn6R/r374+MjAyMHj0a8fHxaNCgAXJzc/H8+XMAQGJiItRqNUOVZISp9EULU+mJiIiIPi0W3fSPqFQquLm5oWnTpoiIiMDly5dRtWpV2NnZwcPDAyqViqFpMsNU+qKFqfREREREnxanl9MHxRFu+WEqfdHAVHoiIiIi/WBvmd7bu9Zss+CWn/yp9J06dQLwOpUegE4q/eHDh5lKL1Px8fFITEyEQqHAN998AwsLC0RGRsLCwgJz5szB5s2b4ezsLK3Rt7e3x+effw6NRgMjIyOdKeVERERE9M9wyILeG0PSCgem0hduTKUnIiIi0i/2ooiKuBo1amDZsmVwcXGBkZHRX6bSb9++HcC7ZzhQwbR27Vopcf6vUumXLl0KIQQ8PDygUCikVPr808h5vomIiIjeH4tuImIqfSHEVHoiIiKigoHTy4lISqVPSEiAo6MjNBoNqlatiiFDhuDYsWMwMDBAbm4uC24ZeTOVHoBOKn1ubi5WrFiBrVu3AgCWLFmCefPmMZWeiIiI6ANjejkR/Z84wi0/TKUnIiIiKhg40k1EOt51HY4Ft/zkT6XXqlixIqpUqaKTSm9mZsZUeiIiIqKPiEU3EelgaFbhwFR6IiIiooKBwxlERIUQU+mJiIiICgau6SYiKqQ0Gg0iIyMxevRomJubv5VKf+TIERgYGHDNPhEREdFHxKKbiKiQO3v2LCIiInD58mVYWlrCzs4OHh4eUKlUDE0jIiIi+shYdBMRFVEc4SYiIiL6+Fh0ExEVAVyzTURERKQfTC8nIioCWHATERER6QeLbiIiIiIiIqKPhEU3ERERERER0UfCopuIiIiIiIjoI2HRTURERERERPSRsOgmIiIiIiIi+khYdBMRERERERF9JCy6iYiIiIiIiD4SFt1ERET0l+Li4qBQKPDkyZO//Zhq1aohODj4o7WJiIhITlh0ExERydiwYcOgUCgwcuTIt/aNGTMGCoUCw4YN+/QNIyIiIgAsuomIiGTvs88+w8aNG/Hq1StpW2ZmJtavXw9LS0s9toyIiIhYdBMREcmcnZ0dPvvsM2zbtk3atm3bNlhaWsLW1lbalpWVBS8vL1SoUAHGxsawt7dHYmKiznPt3r0bderUgYmJCdq3b4/U1NS3/r2jR4+iTZs2MDExwWeffQYvLy9kZGS8s21CCMyYMQOWlpYwMjJCpUqV4OXl9WFeOBERkQyw6CYiIioEhg8fjsjISOnnlStXwsXFReeYSZMmITo6GqtXr8bp06dRq1YtdOnSBY8ePQIA3Lp1C05OTujZsyfOnj0LNzc3+Pr66jzH1atX0bVrV/Tp0wfnzp3Dpk2bcPToUXh6er6zXdHR0ViwYAHCwsJw5coV/Pzzz2jYsOEHfvVEREQFF4tuIiKiQmDw4ME4evQobty4gRs3buDYsWMYPHiwtD8jIwNLly7FvHnz8OWXX8La2hrLly+HiYkJVqxYAQBYunQpatasicDAQFhZWWHQoEFvrQefPXs2Bg0ahHHjxqF27dpo1aoVFi5ciDVr1iAzM/Otdt28eRMWFhbo2LEjLC0t0bx5c7i7u3/U/xdEREQFCYtuIiKiQqB8+fLo3r07Vq1ahcjISHTv3h3lypWT9l+9ehU5OTlo3bq1tM3AwADNmzdHcnIyACA5ORmff/65zvO2bNlS5+fffvsNq1atQvHixaU/Xbp0QV5eHq5fv/5Wu/r164dXr16hRo0acHd3x/bt25Gbm/shXzoREVGBptZ3A4iIiOjDGD58uDTNe/HixR/l33jx4gU8PDzeuS77XaFtn332GS5duoT9+/cjNjYWo0ePxrx583D48GEYGBh8lDYSEREVJBzpJiIiKiS6du2K7Oxs5OTkoEuXLjr7atasCUNDQxw7dkzalpOTg8TERFhbWwMA6tWrh4SEBJ3HnThxQudnOzs7/PHHH6hVq9ZbfwwNDd/ZLhMTE/Ts2RMLFy5EXFwcjh8/jvPnz3+Il0xERFTgcaSbiIiokFCpVNJUcZVKpbOvWLFiGDVqFLy9vVGmTBlYWlpi7ty5ePnyJVxdXQEAI0eORGBgILy9veHm5oZTp05h1apVOs/j4+ODFi1awNPTE25ubihWrBj++OMPxMbGYtGiRW+1adWqVdBoNPj8889hamqKqKgomJiYoGrVqh/nfwIREVEBw5FuIiKiQqRkyZIoWbLkO/cFBASgT58+cHZ2hp2dHVJSUrB3716ULl0awOvp4dHR0fj5559hY2ODZcuW4aefftJ5jkaNGuHw4cO4fPky2rRpA1tbW0yfPh2VKlV6579pZmaG5cuXo3Xr1mjUqBH279+PmJgYlC1b9sO+cCIiogJKIYQQ+m4EERERERERUWHEkW4iIiIiIiKij4RFNxEREREREdFHwqKbiIiIiIiI6CNh0U1ERERERET0kbDoJiIiIiIiIvpIWHQTERERERERfSQsuomIiIiIiIg+EhbdRERERERERB8Ji24iIiIiIiKij4RFNxEREREREdFHwqKbiIiIiIiI6CNh0U1ERERERET0kfw/is5b58NliR4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for each model\n",
    "models = [\n",
    "    \"GraphSAGE\",\n",
    "    \"GraphTransformer (5000)\",\n",
    "    \"GraphTransformer (50000)\",\n",
    "    \"GraphTransformer (500000)\",\n",
    "    \"MFBias\",\n",
    "    \"AutoRec\",\n",
    "    \"NCF\"\n",
    "]\n",
    "\n",
    "hr_values = [\n",
    "    0.6124,  # GraphSAGE\n",
    "    0.9850,  # GraphTransformer (5000)\n",
    "    0.9530,  # GraphTransformer (50000)\n",
    "    0.9690,  # GraphTransformer (500000)\n",
    "    0.6824,  # MFBias\n",
    "    0.7830,  # AutoRec\n",
    "    0.2250   # NCF\n",
    "]\n",
    "\n",
    "ndcg_values = [\n",
    "    0.3914,  # GraphSAGE\n",
    "    0.7307,  # GraphTransformer (5000)\n",
    "    0.9318,  # GraphTransformer (50000)\n",
    "    0.8973,  # GraphTransformer (500000)\n",
    "    0.6389,  # MFBias\n",
    "    0.5736,  # AutoRec\n",
    "    0.1100   # NCF\n",
    "]\n",
    "\n",
    "# Create a bar chart\n",
    "x = np.arange(len(models))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width/2, hr_values, width, label='HR@10')\n",
    "bars2 = ax.bar(x + width/2, ndcg_values, width, label='NDCG@10')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Comparison of HR@10 and NDCG@10 across different models')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "\n",
    "# Attach a text label above each bar in *bars*, displaying its height.\n",
    "def autolabel(bars):\n",
    "    \"\"\"Attach a text label above each bar in *bars*, displaying its height.\"\"\"\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(bars1)\n",
    "autolabel(bars2)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for HR@10 and NDCG@10\n",
    "hr_at_10 = [0.588, 0.597, 0.595, 0.607, 0.608, 0.608, 0.610, 0.609, 0.611, 0.613, 0.631, 0.634, 0.642, 0.626, 0.631, 0.95]\n",
    "ndcg_at_10 = [0.333, 0.336, 0.332, 0.341, 0.341, 0.343, 0.346, 0.343, 0.375, 0.349, 0.384, 0.372, 0.376, 0.385, 0.380, 0.93]\n",
    "\n",
    "# Model names\n",
    "models = [\n",
    "    \"BiasMF\", \"DMF\", \"NCF\", \"AutoRec\", \"CDAE\", \"NADE\", \"CF-UIcA\", \"ST-GCN\",\n",
    "    \"NGCF\", \"NMTR\", \"DIPN\", \"NGCF+M\", \"MBGCN\", \"MATN\", \"GNMR\", \"Graphtransformer\"\n",
    "]\n",
    "\n",
    "# Set up the bar plot\n",
    "x = np.arange(len(models))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot bars for HR@10 and NDCG@10\n",
    "bars1 = ax.bar(x - width/2, hr_at_10, width, label='HR@10', color='#1f77b4')\n",
    "bars2 = ax.bar(x + width/2, ndcg_at_10, width, label='NDCG@10', color='#ff7f0e')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('HR@10 and NDCG@10 for Different Models')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Add a grid for better readability\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "# Add value labels on top of the bars\n",
    "def add_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "add_labels(bars1)\n",
    "add_labels(bars2)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
