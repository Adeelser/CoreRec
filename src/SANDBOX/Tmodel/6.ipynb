{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vishesh : This is device env var dont change it in below cells \n",
    "import torch\n",
    "device=torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "import operator\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scoreformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_feedforward, input_dim, num_weights=10, use_weights=True, dropout=0.1):\n",
    "        super(Scoreformer, self).__init__()\n",
    "        self.num_weights = num_weights\n",
    "        self.use_weights = use_weights\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input projection layers\n",
    "        self.input_linear = Linear(input_dim, d_model)\n",
    "        self.dng_projection = Linear(input_dim, d_model)\n",
    "        \n",
    "        self.encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=d_feedforward, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Final output layers\n",
    "        self.pre_output = Linear(d_model, d_model)\n",
    "        self.output_linear = Linear(d_model, 1)\n",
    "        \n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.layer_norm = LayerNorm(d_model)\n",
    "        \n",
    "        if self.use_weights:\n",
    "            self.weight_linears = ModuleList([Linear(input_dim, d_model) for _ in range(num_weights)])\n",
    "\n",
    "    def compute_neighborhood_similarity(self, adjacency_matrix, x):\n",
    "        try:\n",
    "            binary_adj = (adjacency_matrix > 0).float()\n",
    "            intersection = binary_adj @ binary_adj.T\n",
    "            row_sums = binary_adj.sum(dim=1, keepdim=True)\n",
    "            col_sums = binary_adj.sum(dim=0, keepdim=True)\n",
    "            union = row_sums + col_sums.T - intersection\n",
    "            similarity = intersection / (union + 1e-8)\n",
    "            return similarity @ x\n",
    "        except RuntimeError:\n",
    "            return torch.zeros_like(x)\n",
    "\n",
    "    def project_graph_metrics(self, graph_metrics, target_dim):\n",
    "        if graph_metrics.size(1) < target_dim:\n",
    "            repeats = (target_dim + graph_metrics.size(1) - 1) // graph_metrics.size(1)\n",
    "            graph_metrics = graph_metrics.repeat(1, repeats)[:, :target_dim]\n",
    "        elif graph_metrics.size(1) > target_dim:\n",
    "            graph_metrics = graph_metrics[:, :target_dim]\n",
    "        return graph_metrics\n",
    "\n",
    "    def forward(self, x, adjacency_matrix, graph_metrics, weights=None):\n",
    "        adjacency_matrix = adjacency_matrix.float()\n",
    "        graph_metrics = graph_metrics.float()\n",
    "        batch_size, input_dim = x.shape\n",
    "        \n",
    "        if adjacency_matrix.size(0) != batch_size or adjacency_matrix.size(1) != batch_size:\n",
    "            adjacency_matrix = torch.eye(batch_size, device=x.device)\n",
    "\n",
    "        try:\n",
    "            # Direct connections\n",
    "            direct_scores = adjacency_matrix @ x\n",
    "            \n",
    "            # Neighborhood similarity\n",
    "            neighborhood_similarity = self.compute_neighborhood_similarity(adjacency_matrix, x)\n",
    "            \n",
    "            # Graph structure scores\n",
    "            if graph_metrics.dim() == 2:\n",
    "                graph_metrics_projected = self.project_graph_metrics(graph_metrics, input_dim)\n",
    "                graph_structure_scores = graph_metrics_projected * x\n",
    "            else:\n",
    "                graph_structure_scores = torch.zeros_like(x)\n",
    "\n",
    "            # Combine DNG scores and project to d_model dimension\n",
    "            dng_scores = direct_scores + neighborhood_similarity + graph_structure_scores\n",
    "            dng_scores = self.dng_projection(dng_scores)  # Project to d_model dimension\n",
    "            \n",
    "            # Process input through transformer\n",
    "            if self.use_weights and weights is not None:\n",
    "                weighted_x = torch.zeros_like(x)\n",
    "                for i, weight in enumerate(weights.T):\n",
    "                    weighted_x += self.weight_linears[i](x) * weight.unsqueeze(1)\n",
    "                transformer_input = weighted_x\n",
    "            else:\n",
    "                transformer_input = self.input_linear(x)  # Project to d_model dimension\n",
    "\n",
    "            # Apply transformer\n",
    "            transformer_input = self.layer_norm(transformer_input)\n",
    "            transformer_output = self.transformer_encoder(transformer_input.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Combine transformer output with DNG scores\n",
    "            combined = transformer_output + dng_scores\n",
    "            combined = self.dropout(combined)\n",
    "            \n",
    "            # Final output processing\n",
    "            output = self.pre_output(combined)\n",
    "            output = F.relu(output)\n",
    "            output = self.output_linear(output)\n",
    "            \n",
    "            return output.squeeze(-1)  # Return [batch_size] tensor\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError during forward pass: {e}\")\n",
    "            print(f\"x shape: {x.shape}, adjacency_matrix shape: {adjacency_matrix.shape}, graph_metrics shape: {graph_metrics.shape}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBehaviorDataset:\n",
    "    def __init__(self, data_path='/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/', max_users=10000):\n",
    "        self.data_path = data_path\n",
    "        self.behaviors = ['pv', 'cart', 'buy']\n",
    "        self.trn_file = data_path + 'trn_'\n",
    "        self.tst_file = data_path + 'tst_'\n",
    "        self.max_users = max_users  # Limit number of users\n",
    "        \n",
    "        self.load_training_data()\n",
    "        self.load_test_data()\n",
    "    \n",
    "    def load_training_data(self):\n",
    "        print(\"Loading training data from:\", self.data_path)\n",
    "        self.trn_mats = []\n",
    "        for beh in self.behaviors:\n",
    "            path = self.trn_file + beh\n",
    "            print(f\"Loading behavior: {beh} from {path}\")\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"File not found: {path}\")\n",
    "            with open(path, 'rb') as fs:\n",
    "                mat = pickle.load(fs)\n",
    "                if not isinstance(mat, csr_matrix):\n",
    "                    mat = csr_matrix(mat)\n",
    "                mat = (mat != 0).astype(np.float32)\n",
    "                self.trn_mats.append(mat)\n",
    "        \n",
    "        self.trn_label = 1 * (self.trn_mats[-1] != 0)\n",
    "        self.n_users, self.n_items = self.trn_mats[0].shape\n",
    "        self.n_behaviors = len(self.behaviors)\n",
    "        print(f\"Dataset dimensions: {self.n_users} users, {self.n_items} items\")\n",
    "\n",
    "    def load_test_data(self):\n",
    "        \"\"\"Load test data\"\"\"\n",
    "        test_path = self.tst_file + 'int'\n",
    "        print(f\"Loading test data from: {test_path}\")\n",
    "        if not os.path.exists(test_path):\n",
    "            raise FileNotFoundError(f\"File not found: {test_path}\")\n",
    "        with open(test_path, 'rb') as fs:\n",
    "            self.tst_int = np.array(pickle.load(fs))\n",
    "        \n",
    "        self.tst_users = np.reshape(np.argwhere(self.tst_int != None), [-1])\n",
    "        # Limit test users for faster processing\n",
    "        self.tst_users = self.tst_users[:min(len(self.tst_users), self.max_users)]\n",
    "        print(f\"Using {len(self.tst_users)} test users\")\n",
    "\n",
    "    def create_adjacency_matrix(self, users):\n",
    "        \"\"\"Optimized adjacency matrix creation with MPS support\"\"\"\n",
    "        # Move users to CPU for numpy/scipy operations\n",
    "        users_cpu = users.cpu()\n",
    "        batch_size = len(users_cpu)\n",
    "        adj_matrix = torch.zeros(batch_size, batch_size)\n",
    "        \n",
    "        # Pre-compute user item sets\n",
    "        user_item_sets = []\n",
    "        for user in users_cpu:\n",
    "            user_items = set()\n",
    "            for mat in self.trn_mats:\n",
    "                user_items.update(mat[user.item()].indices)\n",
    "            user_item_sets.append(user_items)\n",
    "        \n",
    "        # Compute similarities in parallel\n",
    "        for i in range(batch_size):\n",
    "            if not user_item_sets[i]:\n",
    "                continue\n",
    "            for j in range(i+1, batch_size):\n",
    "                if user_item_sets[j]:\n",
    "                    jaccard = len(user_item_sets[i] & user_item_sets[j]) / len(user_item_sets[i] | user_item_sets[j])\n",
    "                    adj_matrix[i,j] = adj_matrix[j,i] = jaccard\n",
    "        \n",
    "        # Move result to the same device as input\n",
    "        return adj_matrix.to(device)\n",
    "\n",
    "    def create_graph_metrics(self, users):\n",
    "        \"\"\"Optimized graph metrics creation with MPS support\"\"\"\n",
    "        # Convert users to tensor if it's a list\n",
    "        if isinstance(users, list):\n",
    "            users = torch.tensor(users, device=device)\n",
    "        elif not isinstance(users, torch.Tensor):\n",
    "            users = torch.tensor([users], device=device)\n",
    "        \n",
    "        # Move users to CPU for numpy/scipy operations\n",
    "        users_cpu = users.cpu()\n",
    "        metrics = torch.zeros(len(users_cpu), 3)\n",
    "        \n",
    "        # Vectorized computation\n",
    "        for i, user in enumerate(users_cpu):\n",
    "            interactions = np.array([mat[user.item()].nnz for mat in self.trn_mats])\n",
    "            metrics[i,0] = sum(interactions) / 100\n",
    "            metrics[i,1] = (interactions > 0).sum() / len(self.behaviors)\n",
    "            metrics[i,2] = interactions[-1] / max(interactions[0], 1)\n",
    "        \n",
    "        # Move result to the same device as input\n",
    "        return metrics.to(device)\n",
    "\n",
    "    def prepare_train_instances(self, max_samples=5000):\n",
    "        \"\"\"Optimized training instance preparation\"\"\"\n",
    "        print(\"Preparing training instances...\")\n",
    "        train_data = []\n",
    "        \n",
    "        # Randomly select users\n",
    "        selected_users = np.random.choice(min(self.n_users, self.max_users), size=min(1000, self.max_users), replace=False)\n",
    "        \n",
    "        for user in selected_users:\n",
    "            pos_items = self.trn_label[user].indices[:2]  # Limit positive items\n",
    "            \n",
    "            if len(pos_items) > 0:\n",
    "                for item in pos_items:\n",
    "                    behaviors = [float(mat[user, item]) for mat in self.trn_mats]\n",
    "                    train_data.append([user, item, 1.0] + behaviors)\n",
    "                    \n",
    "                    # Limited negative sampling\n",
    "                    neg_items = np.random.choice(self.n_items, size=2, replace=False)\n",
    "                    for neg_item in neg_items:\n",
    "                        behaviors = [float(mat[user, neg_item]) for mat in self.trn_mats]\n",
    "                        train_data.append([user, neg_item, 0.0] + behaviors)\n",
    "                        \n",
    "                    if len(train_data) >= max_samples:\n",
    "                        break\n",
    "            \n",
    "            if len(train_data) >= max_samples:\n",
    "                break\n",
    "        \n",
    "        print(f\"Generated {len(train_data)} training instances\")\n",
    "        return np.array(train_data)\n",
    "\n",
    "    def get_test_instances(self, num_neg_samples=99):\n",
    "        \"\"\"Generate test instances with negative sampling\"\"\"\n",
    "        print(\"Preparing test instances...\")\n",
    "        test_instances = []\n",
    "        max_test_users = 1000  # Limit number of test users\n",
    "        \n",
    "        # Randomly sample test users if there are too many\n",
    "        test_users = self.tst_users\n",
    "        if len(test_users) > max_test_users:\n",
    "            test_users = np.random.choice(test_users, max_test_users, replace=False)\n",
    "        \n",
    "        for user in test_users:\n",
    "            user = int(user)\n",
    "            pos_item = self.tst_int[user]\n",
    "            if pos_item is not None:\n",
    "                pos_item = int(pos_item)\n",
    "                test_instances.append([user, pos_item, 1.0])\n",
    "                \n",
    "                try:\n",
    "                    # Get negative items\n",
    "                    all_items = set(range(self.n_items))\n",
    "                    pos_items = set(int(x) for x in self.trn_label[user].indices)\n",
    "                    pos_items.add(pos_item)\n",
    "                    neg_items_pool = list(all_items - pos_items)\n",
    "                    \n",
    "                    # Sample negative items\n",
    "                    n_neg = min(num_neg_samples, len(neg_items_pool))\n",
    "                    if n_neg > 0:\n",
    "                        neg_items = np.random.choice(neg_items_pool, size=n_neg, replace=False)\n",
    "                        for neg_item in neg_items:\n",
    "                            test_instances.append([user, int(neg_item), 0.0])\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing test user {user}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        if not test_instances:\n",
    "            raise ValueError(\"No test instances were generated!\")\n",
    "        \n",
    "        test_instances = np.array(test_instances)\n",
    "        print(f\"Generated {len(test_instances)} test instances\")\n",
    "        print(f\"Number of unique users: {len(set(test_instances[:,0]))}\")\n",
    "        return test_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "Loading training data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/\n",
      "Loading behavior: pv from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_pv\n",
      "Loading behavior: cart from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_cart\n",
      "Loading behavior: buy from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/trn_buy\n",
      "Dataset dimensions: 21716 users, 7977 items\n",
      "Loading test data from: /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/tst_int\n",
      "Using 5000 test users\n",
      "Preparing training instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/20z1hn994jd04q4kl0gpgh740000gn/T/ipykernel_98133/4112067786.py:21: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  mat = pickle.load(fs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5001 training instances\n",
      "Using 5001 training instances\n",
      "Preparing training tensors...\n",
      "Starting training...\n",
      "Training completed in 101.35s\n",
      "Training loss: 11.0816\n",
      "\n",
      "Starting evaluation...\n",
      "Preparing test instances...\n",
      "Generated 100000 test instances\n",
      "Number of unique users: 1000\n",
      "Evaluating 100000 instances in 1000 chunks...\n",
      "Processing chunk 1/1000\n",
      "Processing chunk 51/1000\n",
      "Processing chunk 101/1000\n",
      "Processing chunk 151/1000\n",
      "Processing chunk 201/1000\n",
      "Processing chunk 251/1000\n",
      "Processing chunk 301/1000\n",
      "Processing chunk 351/1000\n",
      "Processing chunk 401/1000\n",
      "Processing chunk 451/1000\n",
      "Processing chunk 501/1000\n",
      "Processing chunk 551/1000\n",
      "Processing chunk 601/1000\n",
      "Processing chunk 651/1000\n",
      "Processing chunk 701/1000\n",
      "Processing chunk 751/1000\n",
      "Processing chunk 801/1000\n",
      "Processing chunk 851/1000\n",
      "Processing chunk 901/1000\n",
      "Processing chunk 951/1000\n",
      "Evaluation completed in 15.73s\n",
      "HR@10: 0.9520\n",
      "NDCG@10: 0.9346\n"
     ]
    }
   ],
   "source": [
    "# this code is optimised only to run in mps based machines , otherwise it will shift the computation to cpus automaticaly\n",
    "MAX_U = 5000\n",
    "TOP_N = 10\n",
    "# --\n",
    "CHUNK_S=100\n",
    "TRAIN_SAMP=5000\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, Dropout, LayerNorm\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from datetime import datetime\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class Scoreformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_feedforward, input_dim, num_weights=10, use_weights=True, dropout=0.1):\n",
    "        super(Scoreformer, self).__init__()\n",
    "        self.num_weights = num_weights\n",
    "        self.use_weights = use_weights\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input projection layers\n",
    "        self.input_linear = Linear(input_dim, d_model)\n",
    "        self.dng_projection = Linear(input_dim, d_model)\n",
    "        \n",
    "        self.encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=d_feedforward, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Final output layers\n",
    "        self.pre_output = Linear(d_model, d_model)\n",
    "        self.output_linear = Linear(d_model, 1)\n",
    "        \n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.layer_norm = LayerNorm(d_model)\n",
    "        \n",
    "        if self.use_weights:\n",
    "            self.weight_linears = ModuleList([Linear(input_dim, d_model) for _ in range(num_weights)])\n",
    "\n",
    "    def compute_neighborhood_similarity(self, adjacency_matrix, x):\n",
    "        try:\n",
    "            binary_adj = (adjacency_matrix > 0).float()\n",
    "            intersection = binary_adj @ binary_adj.T\n",
    "            row_sums = binary_adj.sum(dim=1, keepdim=True)\n",
    "            col_sums = binary_adj.sum(dim=0, keepdim=True)\n",
    "            union = row_sums + col_sums.T - intersection\n",
    "            similarity = intersection / (union + 1e-8)\n",
    "            return similarity @ x\n",
    "        except RuntimeError:\n",
    "            return torch.zeros_like(x)\n",
    "\n",
    "    def project_graph_metrics(self, graph_metrics, target_dim):\n",
    "        if graph_metrics.size(1) < target_dim:\n",
    "            repeats = (target_dim + graph_metrics.size(1) - 1) // graph_metrics.size(1)\n",
    "            graph_metrics = graph_metrics.repeat(1, repeats)[:, :target_dim]\n",
    "        elif graph_metrics.size(1) > target_dim:\n",
    "            graph_metrics = graph_metrics[:, :target_dim]\n",
    "        return graph_metrics\n",
    "\n",
    "    def forward(self, x, adjacency_matrix, graph_metrics, weights=None):\n",
    "        adjacency_matrix = adjacency_matrix.float()\n",
    "        graph_metrics = graph_metrics.float()\n",
    "        batch_size, input_dim = x.shape\n",
    "        \n",
    "        if adjacency_matrix.size(0) != batch_size or adjacency_matrix.size(1) != batch_size:\n",
    "            adjacency_matrix = torch.eye(batch_size, device=x.\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            # Direct connections\n",
    "            direct_scores = adjacency_matrix @ x\n",
    "            \n",
    "            # Neighborhood similarity\n",
    "            neighborhood_similarity = self.compute_neighborhood_similarity(adjacency_matrix, x)\n",
    "            \n",
    "            # Graph structure scores\n",
    "            if graph_metrics.dim() == 2:\n",
    "                graph_metrics_projected = self.project_graph_metrics(graph_metrics, input_dim)\n",
    "                graph_structure_scores = graph_metrics_projected * x\n",
    "            else:\n",
    "                graph_structure_scores = torch.zeros_like(x)\n",
    "\n",
    "            # Combine DNG scores and project to d_model dimension\n",
    "            dng_scores = direct_scores + neighborhood_similarity + graph_structure_scores\n",
    "            dng_scores = self.dng_projection(dng_scores)  # Project to d_model dimension\n",
    "            \n",
    "            # Process input through transformer\n",
    "            if self.use_weights and weights is not None:\n",
    "                weighted_x = torch.zeros_like(x)\n",
    "                for i, weight in enumerate(weights.T):\n",
    "                    weighted_x += self.weight_linears[i](x) * weight.unsqueeze(1)\n",
    "                transformer_input = weighted_x\n",
    "            else:\n",
    "                transformer_input = self.input_linear(x)  # Project to d_model dimension\n",
    "\n",
    "            # Apply transformer\n",
    "            transformer_input = self.layer_norm(transformer_input)\n",
    "            transformer_output = self.transformer_encoder(transformer_input.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Combine transformer output with DNG scores\n",
    "            combined = transformer_output + dng_scores\n",
    "            combined = self.dropout(combined)\n",
    "            \n",
    "            # Final output processing\n",
    "            output = self.pre_output(combined)\n",
    "            output = F.relu(output)\n",
    "            output = self.output_linear(output)\n",
    "            \n",
    "            return output.squeeze(-1)  # Return [batch_size] tensor\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError during forward pass: {e}\")\n",
    "            print(f\"x shape: {x.shape}, adjacency_matrix shape: {adjacency_matrix.shape}, graph_metrics shape: {graph_metrics.shape}\")\n",
    "            raise\n",
    "\n",
    "# # cropped dataset\n",
    "# class MultiBehaviorDataset:\n",
    "#     def __init__(self, data_path='/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/beibei01/', max_users=10000):\n",
    "#         self.data_path = data_path\n",
    "#         self.behaviors = ['pv', 'cart', 'buy']\n",
    "#         self.trn_file = data_path + 'trn_'\n",
    "#         self.tst_file = data_path + 'tst_'\n",
    "#         self.max_users = max_users  # Limit number of users\n",
    "        \n",
    "#         self.load_training_data()\n",
    "#         self.load_test_data()\n",
    "    \n",
    "#     def load_training_data(self):\n",
    "#         print(\"Loading training data from:\", self.data_path)\n",
    "#         self.trn_mats = []\n",
    "#         for beh in self.behaviors:\n",
    "#             path = self.trn_file + beh\n",
    "#             print(f\"Loading behavior: {beh} from {path}\")\n",
    "#             if not os.path.exists(path):\n",
    "#                 raise FileNotFoundError(f\"File not found: {path}\")\n",
    "#             with open(path, 'rb') as fs:\n",
    "#                 mat = pickle.load(fs)\n",
    "#                 if not isinstance(mat, csr_matrix):\n",
    "#                     mat = csr_matrix(mat)\n",
    "#                 mat = (mat != 0).astype(np.float32)\n",
    "#                 self.trn_mats.append(mat)\n",
    "        \n",
    "#         self.trn_label = 1 * (self.trn_mats[-1] != 0)\n",
    "#         self.n_users, self.n_items = self.trn_mats[0].shape\n",
    "#         self.n_behaviors = len(self.behaviors)\n",
    "#         print(f\"Dataset dimensions: {self.n_users} users, {self.n_items} items\")\n",
    "\n",
    "#     # def create_adjacency_matrix(self, users):\n",
    "#     #     \"\"\"Optimized adjacency matrix creation\"\"\"\n",
    "#     #     batch_size = len(users)\n",
    "#     #     adj_matrix = torch.zeros(batch_size, batch_size, \n",
    "# device)\n",
    "        \n",
    "#     #     # Pre-compute user item sets\n",
    "#     #     user_item_sets = []\n",
    "#     #     for user in users:\n",
    "#     #         user_items = set()\n",
    "#     #         for mat in self.trn_mats:\n",
    "#     #             user_items.update(mat[user].indices)\n",
    "#     #         user_item_sets.append(user_items)\n",
    "        \n",
    "#     #     # Compute similarities in parallel\n",
    "#     #     for i in range(batch_size):\n",
    "#     #         if not user_item_sets[i]:\n",
    "#     #             continue\n",
    "#     #         for j in range(i+1, batch_size):\n",
    "#     #             if user_item_sets[j]:\n",
    "#     #                 jaccard = len(user_item_sets[i] & user_item_sets[j]) / len(user_item_sets[i] | user_item_sets[j])\n",
    "#     #                 adj_matrix[i,j] = adj_matrix[j,i] = jaccard\n",
    "        \n",
    "#     #     return adj_matrix\n",
    "\n",
    "#     # def create_graph_metrics(self, users):\n",
    "#     #     \"\"\"Optimized graph metrics creation\"\"\"\n",
    "#     #     metrics = torch.zeros(len(users), 3, device=device)\n",
    "        \n",
    "#     #     # Vectorized computation\n",
    "#     #     for i, user in enumerate(users):\n",
    "#     #         interactions = np.array([mat[user].nnz for mat in self.trn_mats])\n",
    "#     #         metrics[i,0] = sum(interactions) / 100\n",
    "#     #         metrics[i,1] = (interactions > 0).sum() / len(self.behaviors)\n",
    "#     #         metrics[i,2] = interactions[-1] / max(interactions[0], 1)\n",
    "        \n",
    "#     #     return metrics\n",
    "\n",
    "#     def create_adjacency_matrix(self, users):\n",
    "#         \"\"\"Optimized adjacency matrix creation with MPS support\"\"\"\n",
    "#         # Move users to CPU for numpy/scipy operations\n",
    "#         users_cpu = users.cpu()\n",
    "#         batch_size = len(users_cpu)\n",
    "#         adj_matrix = torch.zeros(batch_size, batch_size)\n",
    "        \n",
    "#         # Pre-compute user item sets\n",
    "#         user_item_sets = []\n",
    "#         for user in users_cpu:\n",
    "#             user_items = set()\n",
    "#             for mat in self.trn_mats:\n",
    "#                 user_items.update(mat[user.item()].indices)\n",
    "#             user_item_sets.append(user_items)\n",
    "        \n",
    "#         # Compute similarities in parallel\n",
    "#         for i in range(batch_size):\n",
    "#             if not user_item_sets[i]:\n",
    "#                 continue\n",
    "#             for j in range(i+1, batch_size):\n",
    "#                 if user_item_sets[j]:\n",
    "#                     jaccard = len(user_item_sets[i] & user_item_sets[j]) / len(user_item_sets[i] | user_item_sets[j])\n",
    "#                     adj_matrix[i,j] = adj_matrix[j,i] = jaccard\n",
    "        \n",
    "#         # Move result to the same device as input\n",
    "#         return adj_matrix.to(device)\n",
    "\n",
    "#     def create_graph_metrics(self, users):\n",
    "#         \"\"\"Optimized graph metrics creation with MPS support\"\"\"\n",
    "#         # Move users to CPU for numpy/scipy operations\n",
    "#         users_cpu = users.cpu()\n",
    "#         metrics = torch.zeros(len(users_cpu), 3)\n",
    "        \n",
    "#         # Vectorized computation\n",
    "#         for i, user in enumerate(users_cpu):\n",
    "#             interactions = np.array([mat[user.item()].nnz for mat in self.trn_mats])\n",
    "#             metrics[i,0] = sum(interactions) / 100\n",
    "#             metrics[i,1] = (interactions > 0).sum() / len(self.behaviors)\n",
    "#             metrics[i,2] = interactions[-1] / max(interactions[0], 1)\n",
    "        \n",
    "#         # Move result to the same device as input\n",
    "#         return metrics.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         def prepare_train_instances(self, max_samples=5000):\n",
    "#             \"\"\"Optimized training instance preparation\"\"\"\n",
    "#             print(\"Preparing training instances...\")\n",
    "#             train_data = []\n",
    "            \n",
    "#             # Randomly select users\n",
    "#             selected_users = np.random.choice(min(self.n_users, self.max_users), size=min(1000, self.max_users), replace=False)\n",
    "            \n",
    "#             for user in selected_users:\n",
    "#                 pos_items = self.trn_label[user].indices[:2]  # Limit positive items\n",
    "                \n",
    "#                 if len(pos_items) > 0:\n",
    "#                     for item in pos_items:\n",
    "#                         behaviors = [float(mat[user, item]) for mat in self.trn_mats]\n",
    "#                         train_data.append([user, item, 1.0] + behaviors)\n",
    "                        \n",
    "#                         # Limited negative sampling\n",
    "#                         neg_items = np.random.choice(self.n_items, size=2, replace=False)\n",
    "#                         for neg_item in neg_items:\n",
    "#                             behaviors = [float(mat[user, neg_item]) for mat in self.trn_mats]\n",
    "#                             train_data.append([user, neg_item, 0.0] + behaviors)\n",
    "                            \n",
    "#                         if len(train_data) >= max_samples:\n",
    "#                             break\n",
    "                \n",
    "#                 if len(train_data) >= max_samples:\n",
    "#                     break\n",
    "            \n",
    "#             print(f\"Generated {len(train_data)} training instances\")\n",
    "#             return np.array(train_data)\n",
    "        \n",
    "#         def load_test_data(self):\n",
    "#             \"\"\"Load test data\"\"\"\n",
    "#             test_path = self.tst_file + 'int'\n",
    "#             print(f\"Loading test data from: {test_path}\")\n",
    "#             if not os.path.exists(test_path):\n",
    "#                 raise FileNotFoundError(f\"File not found: {test_path}\")\n",
    "#             with open(test_path, 'rb') as fs:\n",
    "#                 self.tst_int = np.array(pickle.load(fs))\n",
    "            \n",
    "#             self.tst_users = np.reshape(np.argwhere(self.tst_int != None), [-1])\n",
    "#             # Limit test users for faster processing\n",
    "#             self.tst_users = self.tst_users[:min(len(self.tst_users), self.max_users)]\n",
    "#             print(f\"Using {len(self.tst_users)} test users\")\n",
    "\n",
    "#         def get_test_instances(self, num_neg_samples=99):\n",
    "#             \"\"\"Generate test instances with negative sampling\"\"\"\n",
    "#             print(\"Preparing test instances...\")\n",
    "#             test_instances = []\n",
    "            \n",
    "#             for user in self.tst_users:\n",
    "#                 pos_item = self.tst_int[user]\n",
    "#                 if pos_item is not None:\n",
    "#                     # Add positive instance\n",
    "#                     test_instances.append([user, pos_item, 1.0])\n",
    "                    \n",
    "#                     # Add negative instances\n",
    "#                     try:\n",
    "#                         # Get all items and remove positive items\n",
    "#                         all_items = set(range(self.n_items))\n",
    "#                         pos_items_train = set(self.trn_label[user].indices)\n",
    "#                         pos_items_train.add(pos_item)\n",
    "#                         neg_items_pool = list(all_items - pos_items_train)\n",
    "                        \n",
    "#                         # Sample negative items\n",
    "#                         n_neg = min(num_neg_samples, len(neg_items_pool))\n",
    "#                         if n_neg > 0:\n",
    "#                             neg_items = np.random.choice(neg_items_pool, size=n_neg, replace=False)\n",
    "#                             for neg_item in neg_items:\n",
    "#                                 test_instances.append([user, neg_item, 0.0])\n",
    "                    \n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error processing test user {user}: {str(e)}\")\n",
    "#                         continue\n",
    "                    \n",
    "#                     # Limit the number of test instances for faster processing\n",
    "#                     if len(test_instances) >= self.max_users * 100:\n",
    "#                         break\n",
    "            \n",
    "#             if not test_instances:\n",
    "#                 raise ValueError(\"No test instances were generated!\")\n",
    "                \n",
    "#             test_instances = np.array(test_instances)\n",
    "#             print(f\"Generated {len(test_instances)} test instances\")\n",
    "#             return test_instances\n",
    "\n",
    "# def evaluate_model(model, dataset, test_instances, k=10):\n",
    "#     \"\"\"Improved evaluation function with better metrics calculation\"\"\"\n",
    "#     model.eval()\n",
    "#     hits = []\n",
    "#     ndcgs = []\n",
    "    \n",
    "#     # Process test instances in smaller chunks\n",
    "#     chunk_size = CHUNK_S\n",
    "#     test_chunks = [test_instances[i:i + chunk_size] for i in range(0, len(test_instances), chunk_size)]\n",
    "    \n",
    "#     print(f\"Evaluating {len(test_instances)} instances in {len(test_chunks)} chunks...\")\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for chunk_idx, chunk in enumerate(test_chunks):\n",
    "#             if chunk_idx % 50 == 0:  # Reduced frequency of progress updates\n",
    "#                 print(f\"Processing chunk {chunk_idx + 1}/{len(test_chunks)}\")\n",
    "            \n",
    "#             # Group by user and ensure we have both positive and negative items\n",
    "#             user_items = {}\n",
    "#             for inst in chunk:\n",
    "#                 user = int(inst[0])\n",
    "#                 item = int(inst[1])\n",
    "#                 label = float(inst[2])\n",
    "                \n",
    "#                 if user not in user_items:\n",
    "#                     user_items[user] = {'pos': [], 'neg': [], 'all_items': [], 'all_scores': []}\n",
    "                \n",
    "#                 user_items[user]['all_items'].append(item)\n",
    "#                 if label > 0.5:\n",
    "#                     user_items[user]['pos'].append(item)\n",
    "#                 else:\n",
    "#                     user_items[user]['neg'].append(item)\n",
    "            \n",
    "#             # Process each user that has both positive and negative items\n",
    "#             for user, data in user_items.items():\n",
    "#                 if not data['pos'] or not data['neg']:\n",
    "#                     continue\n",
    "                \n",
    "#                 items = data['all_items']\n",
    "#                 batch_size = len(items)\n",
    "                \n",
    "#                 # Create input features\n",
    "#                 behaviors = torch.zeros(batch_size, dataset.n_behaviors, device=device)\n",
    "#                 for i, item in enumerate(items):\n",
    "#                     for j, mat in enumerate(dataset.trn_mats):\n",
    "#                         behaviors[i, j] = float(mat[user, item])\n",
    "                \n",
    "#                 x = torch.cat([\n",
    "#                     behaviors,\n",
    "#                     torch.zeros(batch_size, model.input_dim - behaviors.size(1), device=device)\n",
    "#                 ], dim=1)\n",
    "                \n",
    "#                 # Simplified adjacency matrix for evaluation\n",
    "#                 adj_matrix = torch.eye(batch_size, device=device)\n",
    "#                 graph_metrics = dataset.create_graph_metrics([user] * batch_size)\n",
    "                \n",
    "#                 try:\n",
    "#                     predictions = model(x, adj_matrix, graph_metrics)\n",
    "#                     predictions = predictions.cpu().numpy().flatten()\n",
    "                    \n",
    "#                     # Store all scores\n",
    "#                     for item, score in zip(items, predictions):\n",
    "#                         data['all_scores'].append((item, score))\n",
    "                    \n",
    "#                     # Sort items by score\n",
    "#                     sorted_items = [x[0] for x in sorted(data['all_scores'], key=lambda x: x[1], reverse=True)]\n",
    "#                     recommended_items = sorted_items[:k]\n",
    "                    \n",
    "#                     # Calculate HR\n",
    "#                     hit = False\n",
    "#                     for pos_item in data['pos']:\n",
    "#                         if pos_item in recommended_items:\n",
    "#                             hit = True\n",
    "#                             break\n",
    "#                     hits.append(hit)\n",
    "                    \n",
    "#                     # Calculate NDCG\n",
    "#                     dcg = 0\n",
    "#                     idcg = 1  # Ideal DCG for one relevant item\n",
    "#                     for i, item in enumerate(recommended_items):\n",
    "#                         if item in data['pos']:\n",
    "#                             dcg += 1 / np.log2(i + 2)\n",
    "#                     ndcg = dcg / idcg\n",
    "#                     ndcgs.append(ndcg)\n",
    "                    \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error evaluating user {user}: {str(e)}\")\n",
    "#                     continue\n",
    "    \n",
    "#     # Calculate final metrics\n",
    "#     hr = np.mean(hits) if hits else 0\n",
    "#     ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "    \n",
    "#     # Print detailed statistics\n",
    "#     print(f\"\\nEvaluation Statistics:\")\n",
    "#     print(f\"Total users evaluated: {len(hits)}\")\n",
    "#     print(f\"Number of hits: {sum(hits)}\")\n",
    "#     print(f\"Average HR@{k}: {hr:.4f}\")\n",
    "#     print(f\"Average NDCG@{k}: {ndcg:.4f}\")\n",
    "    \n",
    "#     return hr, ndcg\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset, test_instances, k=10):\n",
    "    \"\"\"Improved evaluation function with better metrics calculation\"\"\"\n",
    "    model.eval()\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    # Process test instances in smaller chunks\n",
    "    chunk_size = CHUNK_S\n",
    "    test_chunks = [test_instances[i:i + chunk_size] for i in range(0, len(test_instances), chunk_size)]\n",
    "    \n",
    "    print(f\"Evaluating {len(test_instances)} instances in {len(test_chunks)} chunks...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for chunk_idx, chunk in enumerate(test_chunks):\n",
    "            if chunk_idx % 50 == 0:  # Reduced frequency of progress updates\n",
    "                print(f\"Processing chunk {chunk_idx + 1}/{len(test_chunks)}\")\n",
    "            \n",
    "            # Group by user and ensure we have both positive and negative items\n",
    "            user_items = {}\n",
    "            for inst in chunk:\n",
    "                user = int(inst[0])\n",
    "                item = int(inst[1])\n",
    "                label = float(inst[2])\n",
    "                \n",
    "                if user not in user_items:\n",
    "                    user_items[user] = {'pos': [], 'neg': [], 'all_items': [], 'all_scores': []}\n",
    "                \n",
    "                user_items[user]['all_items'].append(item)\n",
    "                if label > 0.5:\n",
    "                    user_items[user]['pos'].append(item)\n",
    "                else:\n",
    "                    user_items[user]['neg'].append(item)\n",
    "            \n",
    "            # Process each user that has both positive and negative items\n",
    "            for user, data in user_items.items():\n",
    "                if not data['pos'] or not data['neg']:\n",
    "                    continue\n",
    "                \n",
    "                items = data['all_items']\n",
    "                batch_size = len(items)\n",
    "                \n",
    "                # Create input features\n",
    "                behaviors = torch.zeros(batch_size, dataset.n_behaviors, device=device)\n",
    "                for i, item in enumerate(items):\n",
    "                    for j, mat in enumerate(dataset.trn_mats):\n",
    "                        behaviors[i, j] = float(mat[user, item])\n",
    "                \n",
    "                x = torch.cat([\n",
    "                    behaviors,\n",
    "                    torch.zeros(batch_size, model.input_dim - behaviors.size(1), device=device)\n",
    "                ], dim=1)\n",
    "                \n",
    "                # Simplified adjacency matrix for evaluation\n",
    "                adj_matrix = torch.eye(batch_size, device=device)\n",
    "                # Convert user to tensor for graph_metrics\n",
    "                graph_metrics = dataset.create_graph_metrics(torch.tensor([user] * batch_size, device=device))\n",
    "                \n",
    "                try:\n",
    "                    predictions = model(x, adj_matrix, graph_metrics)\n",
    "                    predictions = predictions.cpu().numpy().flatten()\n",
    "                    \n",
    "                    # Store all scores\n",
    "                    for item, score in zip(items, predictions):\n",
    "                        data['all_scores'].append((item, score))\n",
    "                    \n",
    "                    # Sort items by score\n",
    "                    sorted_items = [x[0] for x in sorted(data['all_scores'], key=lambda x: x[1], reverse=True)]\n",
    "                    recommended_items = sorted_items[:k]\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    hit = any(item in recommended_items for item in data['pos'])\n",
    "                    hits.append(hit)\n",
    "                    \n",
    "                    # Calculate NDCG\n",
    "                    dcg = 0\n",
    "                    idcg = 1  # Ideal DCG for one relevant item\n",
    "                    for i, item in enumerate(recommended_items):\n",
    "                        if item in data['pos']:\n",
    "                            dcg += 1 / np.log2(i + 2)\n",
    "                    ndcg = dcg / idcg\n",
    "                    ndcgs.append(ndcg)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating user {user}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    hr = np.mean(hits) if hits else 0\n",
    "    ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "    \n",
    "    return hr, ndcg\n",
    "\n",
    "def get_test_instances(self, num_neg_samples=100):\n",
    "    \"\"\"Improved test instance generation\"\"\"\n",
    "    print(\"Preparing test instances...\")\n",
    "    test_instances = []\n",
    "    max_test_users = 10000  # Limit number of test users\n",
    "    \n",
    "    # Randomly sample test users if there are too many\n",
    "    test_users = self.tst_users\n",
    "    if len(test_users) > max_test_users:\n",
    "        test_users = np.random.choice(test_users, max_test_users, replace=False)\n",
    "    \n",
    "    for user in test_users:\n",
    "        user = int(user)\n",
    "        pos_item = self.tst_int[user]\n",
    "        if pos_item is not None:\n",
    "            pos_item = int(pos_item)\n",
    "            test_instances.append([user, pos_item, 1.0])\n",
    "            \n",
    "            try:\n",
    "                # Get negative items\n",
    "                all_items = set(range(self.n_items))\n",
    "                pos_items = set(int(x) for x in self.trn_label[user].indices)\n",
    "                pos_items.add(pos_item)\n",
    "                neg_items_pool = list(all_items - pos_items)\n",
    "                \n",
    "                # Sample negative items\n",
    "                n_neg = min(num_neg_samples, len(neg_items_pool))\n",
    "                if n_neg > 0:\n",
    "                    neg_items = np.random.choice(neg_items_pool, size=n_neg, replace=False)\n",
    "                    for neg_item in neg_items:\n",
    "                        test_instances.append([user, int(neg_item), 0.0])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing test user {user}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    if not test_instances:\n",
    "        raise ValueError(\"No test instances were generated!\")\n",
    "    \n",
    "    test_instances = np.array(test_instances)\n",
    "    print(f\"Generated {len(test_instances)} test instances\")\n",
    "    print(f\"Number of unique users: {len(set(test_instances[:,0]))}\")\n",
    "    return test_instances\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'd_model': 32,\n",
    "        'num_heads': 2,\n",
    "        'num_layers': 1,\n",
    "        'd_feedforward': 64,\n",
    "        'input_dim': 64,\n",
    "        'num_weights': 3,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-6,\n",
    "        'dropout': 0.1,\n",
    "        'gradient_clip': 1.0,\n",
    "        'max_samples': MAX_U,\n",
    "        'eval_k': TOP_N\n",
    "    }\n",
    "    \n",
    "    print(\"Initializing dataset...\")\n",
    "    dataset = MultiBehaviorDataset(max_users=MAX_U)\n",
    "    train_data = dataset.prepare_train_instances(max_samples=config['max_samples'])\n",
    "    \n",
    "    print(f\"Using {len(train_data)} training instances\")\n",
    "\n",
    "    model = Scoreformer(\n",
    "        num_layers=config['num_layers'],\n",
    "        d_model=config['d_model'],\n",
    "        num_heads=config['num_heads'],\n",
    "        d_feedforward=config['d_feedforward'],\n",
    "        input_dim=config['input_dim'],\n",
    "        num_weights=config['num_weights'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    print(\"Preparing training tensors...\")\n",
    "    users = torch.LongTensor(train_data[:, 0]).to(device)\n",
    "    items = torch.LongTensor(train_data[:, 1]).to(device)\n",
    "    labels = torch.FloatTensor(train_data[:, 2]).to(device)\n",
    "    behaviors = torch.FloatTensor(train_data[:, 3:]).to(device)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        model.train()\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Training\n",
    "        x = torch.cat([\n",
    "            behaviors,\n",
    "            torch.zeros(len(users), config['input_dim'] - behaviors.size(1), device=device)\n",
    "        ], dim=1)\n",
    "        \n",
    "        adj_matrix = dataset.create_adjacency_matrix(users)\n",
    "        graph_metrics = dataset.create_graph_metrics(users)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x, adj_matrix, graph_metrics)\n",
    "        predictions = predictions.view(-1)\n",
    "        labels = labels.view(-1)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip'])\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"Training completed in {training_time:.2f}s\")\n",
    "        print(f\"Training loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        print(\"\\nStarting evaluation...\")\n",
    "        eval_start_time = datetime.now()\n",
    "        \n",
    "        test_instances = dataset.get_test_instances()\n",
    "        hr, ndcg = evaluate_model(model, dataset, test_instances, k=config['eval_k'])\n",
    "        \n",
    "        eval_time = (datetime.now() - eval_start_time).total_seconds()\n",
    "        print(f\"Evaluation completed in {eval_time:.2f}s\")\n",
    "        print(f\"HR@{config['eval_k']}: {hr:.4f}\")\n",
    "        print(f\"NDCG@{config['eval_k']}: {ndcg:.4f}\")\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'training_time': training_time,\n",
    "            'eval_time': eval_time,\n",
    "            'training_loss': loss.item(),\n",
    "            'hr': hr,\n",
    "            'ndcg': ndcg\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        with open(f'results_{timestamp}.txt', 'w') as f:\n",
    "            for key, value in results.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/Tmall/\n",
      "An error occurred: could not find MARK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.sparse import csr_matrix\n",
    "from datetime import datetime\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class MultiBehaviorDataset:\n",
    "    def __init__(self, data_path, max_users=5000):\n",
    "        self.data_path = data_path\n",
    "        self.behaviors = ['test', 'train']\n",
    "        self.max_users = max_users\n",
    "        self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        print(f\"Loading data from {self.data_path}\")\n",
    "        self.trn_mats = []\n",
    "        for beh in self.behaviors:\n",
    "            path = os.path.join(self.data_path, f'ijcai2016_koubei_{beh}')\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"File not found: {path}\")\n",
    "            with open(path, 'rb') as fs:\n",
    "                mat = pickle.load(fs)\n",
    "                mat = csr_matrix(mat) if not isinstance(mat, csr_matrix) else mat\n",
    "                self.trn_mats.append(mat)\n",
    "        \n",
    "        self.trn_label = 1 * (self.trn_mats[-1] != 0)\n",
    "        self.n_users, self.n_items = self.trn_mats[0].shape\n",
    "        print(f\"Loaded dataset with {self.n_users} users and {self.n_items} items\")\n",
    "\n",
    "def evaluate_model(model, test_data, k=10):\n",
    "    \"\"\"\n",
    "    Simplified evaluation function that calculates HR@k and NDCG@k\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    hits, ndcgs = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Group test data by user\n",
    "        user_items = {}\n",
    "        for user, item, label in test_data:\n",
    "            if user not in user_items:\n",
    "                user_items[user] = {'pos': [], 'neg': []}\n",
    "            if label == 1:\n",
    "                user_items[user]['pos'].append(item)\n",
    "            else:\n",
    "                user_items[user]['neg'].append(item)\n",
    "        \n",
    "        # Calculate metrics for each user\n",
    "        for user, items in user_items.items():\n",
    "            if not items['pos'] or not items['neg']:\n",
    "                continue\n",
    "                \n",
    "            # Get all items for this user\n",
    "            all_items = items['pos'] + items['neg']\n",
    "            predictions = model.predict(user, all_items)\n",
    "            \n",
    "            # Sort items by prediction score\n",
    "            item_scores = list(zip(all_items, predictions))\n",
    "            sorted_items = [x[0] for x in sorted(item_scores, key=lambda x: x[1], reverse=True)]\n",
    "            \n",
    "            # Calculate HR@k\n",
    "            recommended_items = sorted_items[:k]\n",
    "            hit = any(item in recommended_items for item in items['pos'])\n",
    "            hits.append(hit)\n",
    "            \n",
    "            # Calculate NDCG@k\n",
    "            dcg = sum(1 / np.log2(i + 2) for i, item in enumerate(recommended_items) if item in items['pos'])\n",
    "            idcg = sum(1 / np.log2(i + 2) for i in range(min(len(items['pos']), k)))\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0\n",
    "            ndcgs.append(ndcg)\n",
    "    \n",
    "    hr = np.mean(hits)\n",
    "    ndcg = np.mean(ndcgs)\n",
    "    return hr, ndcg\n",
    "\n",
    "def main():\n",
    "    config = {\n",
    "        'data_path': '/Users/visheshyadav/Documents/GitHub/CoreRec/src/SANDBOX/dataset/Tmall/',\n",
    "        'max_users': 5000,\n",
    "        'k': 10\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load dataset\n",
    "        dataset = MultiBehaviorDataset(config['data_path'], max_users=config['max_users'])\n",
    "                \n",
    "        # Get test data\n",
    "        test_data = dataset.get_test_instances()\n",
    "        \n",
    "        # Evaluate\n",
    "        start_time = datetime.now()\n",
    "        hr, ndcg = evaluate_model(model, test_data, k=config['k'])\n",
    "        eval_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"\\nEvaluation Results:\")\n",
    "        print(f\"HR@{config['k']}: {hr:.4f}\")\n",
    "        print(f\"NDCG@{config['k']}: {ndcg:.4f}\")\n",
    "        print(f\"Evaluation time: {eval_time:.2f}s\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(\"Please ensure the dataset path is correct and the required files exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
