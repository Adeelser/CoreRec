{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import networkx as nx\n",
    "import os, sys\n",
    "\n",
    "# Workaround for __file__ not being defined in Jupyter notebooks\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "sys.path.append(os.path.abspath(os.path.join(notebook_path, '../..')))  # Adjusted path\n",
    "\n",
    "from engine.core_rec import *  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import custom modules\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Workaround for __file__ not being defined in Jupyter notebooks\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "sys.path.append(os.path.abspath(os.path.join(notebook_path, '../..')))\n",
    "\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert data to torch tensors\n",
    "x = data.x\n",
    "edge_index = data.edge_index\n",
    "\n",
    "# Create a PyTorch Geometric Data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Add train_mask and test_mask\n",
    "num_nodes = x.size(0)\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask[:int(0.8 * num_nodes)] = True\n",
    "test_mask = ~train_mask\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "# Add labels (y) if not already present\n",
    "num_classes = dataset.num_classes\n",
    "data.y = data.y if data.y is not None else torch.randint(0, num_classes, (num_nodes,), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = nx.to_numpy_array(nx.from_edgelist(edge_index.t().tolist()))\n",
    "train_dataset = GraphDataset(adj_matrix[train_mask])\n",
    "test_dataset = GraphDataset(adj_matrix[test_mask])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 23.88054847717285\n",
      "Epoch 2/100, Loss: 15.703673362731934\n",
      "Epoch 3/100, Loss: 18.538469314575195\n",
      "Epoch 4/100, Loss: 16.13208770751953\n",
      "Epoch 5/100, Loss: 17.79981231689453\n",
      "Epoch 6/100, Loss: 53.00846862792969\n",
      "Epoch 7/100, Loss: 11.9574556350708\n",
      "Epoch 8/100, Loss: 6.425912857055664\n",
      "Epoch 9/100, Loss: 10.688874244689941\n",
      "Epoch 10/100, Loss: 8.965605735778809\n",
      "Epoch 11/100, Loss: 12.352456092834473\n",
      "Epoch 12/100, Loss: 8.098556518554688\n",
      "Epoch 13/100, Loss: 8.64078140258789\n",
      "Epoch 14/100, Loss: 9.981680870056152\n",
      "Epoch 15/100, Loss: 8.952320098876953\n",
      "Epoch 16/100, Loss: 17.278013229370117\n",
      "Epoch 17/100, Loss: 8.168448448181152\n",
      "Epoch 18/100, Loss: 12.569095611572266\n",
      "Epoch 19/100, Loss: 10.038256645202637\n",
      "Epoch 20/100, Loss: 8.95732593536377\n",
      "Epoch 21/100, Loss: 7.703975200653076\n",
      "Epoch 22/100, Loss: 9.584030151367188\n",
      "Epoch 23/100, Loss: 10.356860160827637\n",
      "Epoch 24/100, Loss: 12.819650650024414\n",
      "Epoch 25/100, Loss: 12.636499404907227\n",
      "Epoch 26/100, Loss: 8.94835090637207\n",
      "Epoch 27/100, Loss: 13.150598526000977\n",
      "Epoch 28/100, Loss: 11.4804048538208\n",
      "Epoch 29/100, Loss: 13.192063331604004\n",
      "Epoch 30/100, Loss: 8.629289627075195\n",
      "Epoch 31/100, Loss: 15.269425392150879\n",
      "Epoch 32/100, Loss: 9.160880088806152\n",
      "Epoch 33/100, Loss: 8.109009742736816\n",
      "Epoch 34/100, Loss: 18.22524642944336\n",
      "Epoch 35/100, Loss: 10.125009536743164\n",
      "Epoch 36/100, Loss: 8.113990783691406\n",
      "Epoch 37/100, Loss: 6.070897102355957\n",
      "Epoch 38/100, Loss: 6.24423360824585\n",
      "Epoch 39/100, Loss: 13.242779731750488\n",
      "Epoch 40/100, Loss: 8.07020378112793\n",
      "Epoch 41/100, Loss: 27.079151153564453\n",
      "Epoch 42/100, Loss: 9.497632026672363\n",
      "Epoch 43/100, Loss: 12.319409370422363\n",
      "Epoch 44/100, Loss: 8.110397338867188\n",
      "Epoch 45/100, Loss: 15.817962646484375\n",
      "Epoch 46/100, Loss: 14.454575538635254\n",
      "Epoch 47/100, Loss: 6.792697429656982\n",
      "Epoch 48/100, Loss: 10.58044719696045\n",
      "Epoch 49/100, Loss: 9.95510196685791\n",
      "Epoch 50/100, Loss: 10.575725555419922\n",
      "Epoch 51/100, Loss: 5.518978595733643\n",
      "Epoch 52/100, Loss: 7.161351203918457\n",
      "Epoch 53/100, Loss: 8.315620422363281\n",
      "Epoch 54/100, Loss: 11.170177459716797\n",
      "Epoch 55/100, Loss: 11.29510498046875\n",
      "Epoch 56/100, Loss: 7.686929702758789\n",
      "Epoch 57/100, Loss: 9.033191680908203\n",
      "Epoch 58/100, Loss: 8.009799003601074\n",
      "Epoch 59/100, Loss: 7.012889385223389\n",
      "Epoch 60/100, Loss: 9.62967300415039\n",
      "Epoch 61/100, Loss: 10.820395469665527\n",
      "Epoch 62/100, Loss: 5.0566792488098145\n",
      "Epoch 63/100, Loss: 6.255673408508301\n",
      "Epoch 64/100, Loss: 9.996898651123047\n",
      "Epoch 65/100, Loss: 6.9934611320495605\n",
      "Epoch 66/100, Loss: 10.060379981994629\n",
      "Epoch 67/100, Loss: 9.470824241638184\n",
      "Epoch 68/100, Loss: 9.420683860778809\n",
      "Epoch 69/100, Loss: 11.336588859558105\n",
      "Epoch 70/100, Loss: 11.736397743225098\n",
      "Epoch 71/100, Loss: 11.086112022399902\n",
      "Epoch 72/100, Loss: 9.052929878234863\n",
      "Epoch 73/100, Loss: 9.913601875305176\n",
      "Epoch 74/100, Loss: 10.241469383239746\n",
      "Epoch 75/100, Loss: 7.382577419281006\n",
      "Epoch 76/100, Loss: 58.71467208862305\n",
      "Epoch 77/100, Loss: 9.453140258789062\n",
      "Epoch 78/100, Loss: 10.26676082611084\n",
      "Epoch 79/100, Loss: 7.8799662590026855\n",
      "Epoch 80/100, Loss: 10.938348770141602\n",
      "Epoch 81/100, Loss: 10.936591148376465\n",
      "Epoch 82/100, Loss: 12.571613311767578\n",
      "Epoch 83/100, Loss: 7.3456711769104\n",
      "Epoch 84/100, Loss: 9.23094654083252\n",
      "Epoch 85/100, Loss: 8.265352249145508\n",
      "Epoch 86/100, Loss: 11.733115196228027\n",
      "Epoch 87/100, Loss: 7.9863057136535645\n",
      "Epoch 88/100, Loss: 10.260194778442383\n",
      "Epoch 89/100, Loss: 6.917941093444824\n",
      "Epoch 90/100, Loss: 6.911297798156738\n",
      "Epoch 91/100, Loss: 7.621037006378174\n",
      "Epoch 92/100, Loss: 53.4088020324707\n",
      "Epoch 93/100, Loss: 7.413878917694092\n",
      "Epoch 94/100, Loss: 7.475442886352539\n",
      "Epoch 95/100, Loss: 14.5172700881958\n",
      "Epoch 96/100, Loss: 13.807951927185059\n",
      "Epoch 97/100, Loss: 9.016818046569824\n",
      "Epoch 98/100, Loss: 13.082552909851074\n",
      "Epoch 99/100, Loss: 11.64892578125\n",
      "Epoch 100/100, Loss: 10.222664833068848\n",
      "Accuracy: 0.5738\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model\n",
    "num_layers = 1\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "d_feedforward = 512\n",
    "input_dim = 2708 # Ensure this matches the number of features in the dataset\n",
    "model = GraphTransformerV2(num_layers, d_model, num_heads, d_feedforward, input_dim)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=100)\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        # print(f\"Input shape: {inputs.shape}\")  # Add this line to check the input shape\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        # print(f\"Preds shape: {preds.shape}, Targets shape: {targets.shape}\")  # Debugging shapes\n",
    "        targets = torch.argmax(targets, dim=1)  # Convert one-hot encoded targets to class indices\n",
    "        all_preds.extend(preds.cpu().numpy().flatten())  # Ensure preds are flattened\n",
    "        all_labels.extend(targets.cpu().numpy().flatten())  # Ensure targets are flattened\n",
    "        # print(f\"Current length of all_preds: {len(all_preds)}, all_labels: {len(all_labels)}\")  # Debugging lengths\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Debugging: Print final lengths of all_preds and all_labels\n",
    "# print(f\"Final length of all_preds: {len(all_preds)}\")\n",
    "# print(f\"Final length of all_labels: {len(all_labels)}\")\n",
    "\n",
    "# Ensure both arrays are 1-dimensional\n",
    "all_preds = all_preds.flatten()\n",
    "all_labels = all_labels.flatten()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     75\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 76\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     77\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[1;32m     78\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/CoreRec/engine/models.py:33\u001b[0m, in \u001b[0;36mGraphTransformer.forward\u001b[0;34m(self, x, weights)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_weights:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/data/data.py:559\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_store\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object was created by an older version of PyG. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf this error occurred while loading an already existing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset, remove the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory in the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot folder and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store, key)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/data/storage.py:96\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "\n",
    "# Define a simple GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Define a simple GAT model\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=8):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=0.6)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Define the GraphTransformer model (assuming the class is already defined)\n",
    "# from your previous implementation\n",
    "# class GraphTransformer(torch.nn.Module):\n",
    "#     # Your GraphTransformer implementation\n",
    "#     pass\n",
    "\n",
    "# Define a simple MLP model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = self.fc1(x).relu()\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "num_epochs=200\n",
    "# Function to train a model\n",
    "if model == 'GraphTransformer':\n",
    "    num_layers = 1\n",
    "    d_model = 128\n",
    "    num_heads = 8\n",
    "    d_feedforward = 512\n",
    "    input_dim = 2708\n",
    "    # model = GraphTransformerV2(num_layers, d_model, num_heads, d_feedforward, input_dim)\n",
    "    train_model(model, train_loader, criterion, optimizer, num_epochs=100)\n",
    "else:\n",
    "    lr=0.01\n",
    "    weight_decay=5e-4\n",
    "    def train_model123(model, data, num_epochs, criterion=criterion, optimizer=optimizer):\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    _, pred = model(data).max(dim=1)\n",
    "    correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    acc = correct / data.test_mask.sum().item()\n",
    "    return acc\n",
    "\n",
    "# Load data\n",
    "data = dataset[0]\n",
    "\n",
    "# Debugging: Print data attributes\n",
    "print(f\"Data attributes: {data.keys}\")\n",
    "\n",
    "# Debugging: Print data types\n",
    "print(f\"Data x type: {type(data.x)}, Data edge_index type: {type(data.edge_index)}\")\n",
    "\n",
    "# Ensure data is in the correct format\n",
    "data.x = data.x.float()\n",
    "data.edge_index = data.edge_index.long()\n",
    "\n",
    "# Define models\n",
    "input_dim = dataset.num_node_features\n",
    "hidden_dim = 16\n",
    "output_dim = dataset.num_classes\n",
    "\n",
    "import sys, os\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "sys.path.append(os.path.abspath(os.path.join(notebook_path, '../..')))\n",
    "from engine.core_rec import *\n",
    "\n",
    "models = {\n",
    "    'GCN': GCN(input_dim, hidden_dim, output_dim),\n",
    "    'GAT': GAT(input_dim, hidden_dim, output_dim),\n",
    "    'MLP': MLP(input_dim, hidden_dim, output_dim),\n",
    "    # Add your GraphTransformer here with the appropriate parameters\n",
    "    # 'GraphTransformer': GraphTransformer(num_layers=2, d_model=hidden_dim, num_heads=2, d_feedforward=hidden_dim, input_dim=input_dim)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "accuracies = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    if model == 'GraphTransformer':\n",
    "        train_model(model, train_loader, criterion, optimizer, num_epochs=100)\n",
    "    else:\n",
    "        train_model(model, data, num_epochs)  # Corrected function call\n",
    "    acc = evaluate_model(model, data)\n",
    "    accuracies[name] = acc\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Plot accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(accuracies.keys(), accuracies.values(), color='skyblue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracies on Cora Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Linear, TransformerEncoderLayer, TransformerEncoder, ReLU, ModuleList\n",
    "from torch_geometric.utils import degree\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class GraphTransformer(Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_feedforward, input_dim, num_weights=10, use_weights=True):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.num_weights = num_weights\n",
    "        self.use_weights = use_weights\n",
    "        self.input_linear = Linear(input_dim, d_model)\n",
    "        self.encoder_layer = TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=d_feedforward, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.output_linear = Linear(d_model, input_dim)\n",
    "        if self.use_weights:\n",
    "            self.weight_linears = ModuleList([Linear(input_dim, d_model) for _ in range(num_weights)])\n",
    "\n",
    "    def forward(self, x, weights=None):\n",
    "        x = x.float()\n",
    "        if self.use_weights:\n",
    "            if weights is not None:\n",
    "                weighted_x = torch.zeros_like(x)\n",
    "                for i, weight in enumerate(weights):\n",
    "                    weighted_x += self.weight_linears[i](x) * weight\n",
    "                x = weighted_x\n",
    "            else:\n",
    "                x = self.input_linear(x)\n",
    "        else:\n",
    "            x = self.input_linear(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.output_linear(x)\n",
    "        return x\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
