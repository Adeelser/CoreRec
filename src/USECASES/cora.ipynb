{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/yf/20z1hn994jd04q4kl0gpgh740000gn/T/ipykernel_86584/4024293910.py\", line 13, in <module>\n",
      "    from engine.core_rec import *\n",
      "  File \"/Users/visheshyadav/Documents/GitHub/CoreRec/engine/core_rec.py\", line 11, in <module>\n",
      "    from common_import import *\n",
      "  File \"/System/Volumes/Data/Users/visheshyadav/Documents/GitHub/CoreRec/engine/common_import.py\", line 2, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/arrow/array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/Users/visheshyadav/anaconda3/lib/python3.11/site-packages/bottleneck/__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import networkx as nx\n",
    "import os,sys\n",
    "\n",
    "\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "sys.path.append(os.path.abspath(os.path.join(notebook_path, '..')))\n",
    "from engine.core_rec import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import custom modules\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Workaround for __file__ not being defined in Jupyter notebooks\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "sys.path.append(os.path.abspath(os.path.join(notebook_path, '..')))\n",
    "\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert data to torch tensors\n",
    "x = data.x\n",
    "edge_index = data.edge_index\n",
    "\n",
    "# Create a PyTorch Geometric Data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Add train_mask and test_mask\n",
    "num_nodes = x.size(0)\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask[:int(0.8 * num_nodes)] = True\n",
    "test_mask = ~train_mask\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "# Add labels (y) if not already present\n",
    "num_classes = dataset.num_classes\n",
    "data.y = data.y if data.y is not None else torch.randint(0, num_classes, (num_nodes,), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = nx.to_numpy_array(nx.from_edgelist(edge_index.t().tolist()))\n",
    "train_dataset = GraphDataset(adj_matrix[train_mask])\n",
    "test_dataset = GraphDataset(adj_matrix[test_mask])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m train_model(model, train_loader, criterion, optimizer, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Documents/GitHub/CoreRec/engine/train.py:27\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/CoreRec/engine/models.py:64\u001b[0m, in \u001b[0;36mGraphTransformerV2.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m---> 64\u001b[0m     x, edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Direct Connections: Aggregate features from direct neighbors\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m edge_index\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model\n",
    "num_layers = 1\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "d_feedforward = 512\n",
    "input_dim = 2708 # Ensure this matches the number of features in the dataset\n",
    "model = GraphTransformerV2(num_layers, d_model, num_heads, d_feedforward, input_dim)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=100)\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        # print(f\"Input shape: {inputs.shape}\")  # Add this line to check the input shape\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        # print(f\"Preds shape: {preds.shape}, Targets shape: {targets.shape}\")  # Debugging shapes\n",
    "        targets = torch.argmax(targets, dim=1)  # Convert one-hot encoded targets to class indices\n",
    "        all_preds.extend(preds.cpu().numpy().flatten())  # Ensure preds are flattened\n",
    "        all_labels.extend(targets.cpu().numpy().flatten())  # Ensure targets are flattened\n",
    "        # print(f\"Current length of all_preds: {len(all_preds)}, all_labels: {len(all_labels)}\")  # Debugging lengths\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Debugging: Print final lengths of all_preds and all_labels\n",
    "# print(f\"Final length of all_preds: {len(all_preds)}\")\n",
    "# print(f\"Final length of all_labels: {len(all_labels)}\")\n",
    "\n",
    "# Ensure both arrays are 1-dimensional\n",
    "all_preds = all_preds.flatten()\n",
    "all_labels = all_labels.flatten()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN...\n",
      "GCN Accuracy: 0.7400\n",
      "Training GAT...\n",
      "GAT Accuracy: 0.7980\n",
      "Training MLP...\n",
      "MLP Accuracy: 0.4050\n",
      "Training GraphTransformer...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'float' and 'Adam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 109\u001b[0m\n\u001b[1;32m    107\u001b[0m     train_model1(model, data, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m)  \u001b[38;5;66;03m# Corrected function call\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     train_model123(model, data, criterion, optimizer)  \u001b[38;5;66;03m# Corrected function call\u001b[39;00m\n\u001b[1;32m    110\u001b[0m acc \u001b[38;5;241m=\u001b[39m evaluate_model(model, data)\n\u001b[1;32m    111\u001b[0m accuracies[name] \u001b[38;5;241m=\u001b[39m acc\n",
      "Cell \u001b[0;32mIn[45], line 62\u001b[0m, in \u001b[0;36mtrain_model123\u001b[0;34m(model, data, epochs, lr, weight_decay)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model123\u001b[39m(model, data, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m):\n\u001b[0;32m---> 62\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     63\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:28\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     16\u001b[0m              params: ParamsT,\n\u001b[1;32m     17\u001b[0m              lr: Union[\u001b[38;5;28mfloat\u001b[39m, Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m              differentiable: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m              fused: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m lr:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lr, Tensor) \u001b[38;5;129;01mand\u001b[39;00m foreach \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m capturable:\n",
      "\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'float' and 'Adam'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "\n",
    "# Define a simple GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Define a simple GAT model\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=8):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=0.6)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Define the GraphTransformer model (assuming the class is already defined)\n",
    "# from your previous implementation\n",
    "# class GraphTransformer(torch.nn.Module):\n",
    "#     # Your GraphTransformer implementation\n",
    "#     pass\n",
    "\n",
    "# Define a simple MLP model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = self.fc1(x).relu()\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Function to train a model\n",
    "def train_model123(model, data, epochs=200, lr=0.01, weight_decay=5e-4):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    _, pred = model(data).max(dim=1)\n",
    "    correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    acc = correct / data.test_mask.sum().item()\n",
    "    return acc\n",
    "\n",
    "# Load data\n",
    "data = dataset[0]\n",
    "\n",
    "# Define models\n",
    "input_dim = dataset.num_node_features\n",
    "hidden_dim = 16\n",
    "output_dim = dataset.num_classes\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "import sys,os\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "sys.path.append(os.path.abspath(os.path.join(notebook_path, '..')))\n",
    "from engine.core_rec import *\n",
    "\n",
    "models = {\n",
    "    'GCN': GCN(input_dim, hidden_dim, output_dim),\n",
    "    'GAT': GAT(input_dim, hidden_dim, output_dim),\n",
    "    'MLP': MLP(input_dim, hidden_dim, output_dim),\n",
    "    # Add your GraphTransformer here with the appropriate parameters\n",
    "    'GraphTransformer': GraphTransformer(num_layers=2, d_model=hidden_dim, num_heads=2, d_feedforward=hidden_dim, input_dim=input_dim)\n",
    "}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# Train and evaluate models\n",
    "accuracies = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    if name != 'GraphTransformer':\n",
    "        train_model1(model, data, epochs=10, lr=0.01, weight_decay=5e-4)  # Corrected function call\n",
    "    else:\n",
    "        train_model123(model, data, criterion, optimizer)  # Corrected function call\n",
    "    acc = evaluate_model(model, data)\n",
    "    accuracies[name] = acc\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Plot accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(accuracies.keys(), accuracies.values(), color='skyblue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracies on Cora Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import core_rec as cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model1(model, data, epochs=200, lr=0.01, weight_decay=5e-4):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
